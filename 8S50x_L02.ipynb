{"cells": [{"cell_type": "markdown", "id": "d6d3c5b9", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<hr style=\"height: 1px;\">\n", "<i>This notebook was authored by the 8.S50x Course Team, Copyright 2022 MIT All Rights Reserved.</i>\n", "<hr style=\"height: 1px;\">\n", "<br>\n", "\n", "<h1>Lesson 2: Binomial, Poisson, and Gaussian Distributions</h1>\n"]}, {"cell_type": "markdown", "id": "8f15f03f", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_2_0'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L2.0 Overview</h2>\n"]}, {"cell_type": "markdown", "id": "63e300db", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<h3>Navigation</h3>\n", "\n", "<table style=\"width:100%\">\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_2_1\">L2.1 Introduction to Binomial Distribution</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_2_1\">L2.1 Exercises</a></td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_2_2\">L2.2 Applications Using the Binomial Distribution</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_2_2\">L2.2 Exercises</a></td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_2_3\">L2.3 The Poisson Distribution</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_2_3\">L2.3 Exercises</a></td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_2_4\">L2.4 Poisson Distribution Continued</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_2_4\">L2.4 Exercises</a></td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_2_5\">L2.5 The Gaussian Distribution</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_2_5\">L2.5 Exercises</a></td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_2_6\">L2.6 Uncertainties in Measurement</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_2_6\">L2.6 Exercises</a></td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_2_7\">L2.7 Propagating Uncertainties</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_2_7\">L2.7 Exercises</a></td>\n", "    </tr>\n", "</table>\n", "\n"]}, {"cell_type": "markdown", "id": "a864232e", "metadata": {"tags": ["learner", "catsoop_00", "md"]}, "source": ["<h3>Learning Objectives</h3>\n", "\n", "By the end of this Lesson, you should be able to do the following:\n", "\n", "- Understand how and when to use Binomial, Poisson, and Gaussian distributions.\n", "- Understand how uncertainties propagate in measurements"]}, {"cell_type": "markdown", "id": "d39fe23b", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<h3>Importing Data (Colab Only)</h3>\n", "\n", "If you are in a Google Colab environment, run the cell below to import the data for this notebook. Otherwise, if you have downloaded the course repository, you do not have to run the cell below.\n", "\n", "See the source and attribution information below:\n", "\n", ">data: data/L02/tmpdata.txt, data/L02/tmpmc.txt<br>\n", ">source:  https://arxiv.org/pdf/1104.0699.pdf<br>\n", ">attribution: CDF Collaboration, arXiv:1104.0699v2<br>\n", ">license type: https://arxiv.org/licenses/nonexclusive-distrib/1.0/license.html"]}, {"cell_type": "code", "execution_count": null, "id": "2ea4c4c1", "metadata": {"tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN L2.0-runcell00\n", "\n", "#Importing data:\n", "\n", "!git init\n", "!git remote add -f origin https://github.com/mitx-8s50/nb_LEARNER/\n", "!git config core.sparseCheckout true\n", "!echo 'data/L02' >> .git/info/sparse-checkout\n", "!git pull origin main"]}, {"cell_type": "markdown", "id": "4f25a8db", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Importing Libraries</h3>\n", "\n", "Before beginning, run the cell below to import the relevant libraries for this notebook.\n"]}, {"cell_type": "code", "execution_count": null, "id": "fec93314", "metadata": {"tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L2.0-runcell01\n", "\n", "# The documentation to these packages is linked beside them if you have questions\n", "\n", "import numpy as np                 #https://numpy.org/doc/stable/ \n", "from scipy.special import comb     #https://docs.scipy.org/doc/scipy/reference/special.html\n", "import scipy.stats as stats        #https://docs.scipy.org/doc/scipy/reference/stats.html\n", "import matplotlib.pyplot as plt    #https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.html"]}, {"cell_type": "markdown", "id": "0286eb9f", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Setting Default Figure Parameters</h3>\n", "\n", "The following code cell sets default values for figure parameters."]}, {"cell_type": "code", "execution_count": null, "id": "109098ea", "metadata": {"tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L2.0-runcell02\n", "\n", "#set plot resolution\n", "%config InlineBackend.figure_format = 'retina'\n", "\n", "#set default figure parameters\n", "plt.rcParams['figure.figsize'] = (9,6)\n", "\n", "medium_size = 12\n", "large_size = 15\n", "\n", "plt.rc('font', size=medium_size)          # default text sizes\n", "plt.rc('xtick', labelsize=medium_size)    # xtick labels\n", "plt.rc('ytick', labelsize=medium_size)    # ytick labels\n", "plt.rc('legend', fontsize=medium_size)    # legend\n", "plt.rc('axes', titlesize=large_size)      # axes title\n", "plt.rc('axes', labelsize=large_size)      # x and y labels\n", "plt.rc('figure', titlesize=large_size)    # figure title"]}, {"cell_type": "markdown", "id": "cbea03c9", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_2_1'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L2.1 Introduction to Binomial Distribution</h2>  \n", "\n", "| [Top](#section_2_0) | [Previous Section](#section_2_0) | [Exercises](#exercises_2_1) | [Next Section](#section_2_2) |\n"]}, {"cell_type": "markdown", "id": "7d60d26b", "metadata": {"tags": ["8S50x", "learner"]}, "source": ["*The material in this section is discussed in the video **<a href=\"https://courses.mitxonline.mit.edu/learn/course/course-v1:MITxT+8.S50.1x+3T2024/block-v1:MITxT+8.S50.1x+3T2024+type@sequential+block@seq_LS2/block-v1:MITxT+8.S50.1x+3T2024+type@vertical+block@vert_LS2_vid1\" target=\"_blank\">HERE</a>.** You are encouraged to watch that video and use this notebook concurrently.*"]}, {"cell_type": "markdown", "id": "549f4d68", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Slides</h3>\n", "\n", "Run the code below to view the slides for this lesson, which are discussed in the videos. You can also open the slides in a separate window <a href=\"https://mitx-8s50.github.io/slides/L02/slides1.html\" target=\"_blank\">HERE</a>."]}, {"cell_type": "code", "execution_count": null, "id": "b9f82529", "metadata": {"tags": ["learner", "py"]}, "outputs": [], "source": ["#>>>RUN: L2.0-slides\n", "\n", "from IPython.display import IFrame\n", "IFrame(src='https://mitx-8s50.github.io/slides/L02/slides1.html', width=975, height=550)"]}, {"cell_type": "markdown", "id": "3965767e", "metadata": {"tags": ["learner", "md", "lect_01"]}, "source": ["<h3>Overview</h3>\n", "\n", "<!--<img src=\"https://external-preview.redd.it/Kt_QUdsFmAI4v9hPw-JbYA2wC9blaF8iIvnVdla2aaE.jpg?auto=webp&s=da7915bb8a27e47d88adaf6f58bab193b7d1f35f\" width=\"500\"/>-->\n", "\n", "Often we perform measurements having some probability. Let's say we perform many equal-probability measurements. What will be our distribution? Since this is not a math class, we will not go into the depth of the math behind this, but let's at least walk through a basic derivation.\n", "\n"]}, {"cell_type": "markdown", "id": "77b3064b", "metadata": {"tags": ["learner", "md", "lect_01"]}, "source": ["Let's say you flip a coin 10 times, and the probability of heads is $p$. Let's say you get heads 3 times and tails 7 times.\n", "\n", "* What are the number of different cases where there are 3 heads?\n", "\n", "In this case, all we care about is the number of heads out of the total number of flips, so we can use the formula for a *combination*: $_{10}C_{3}=\\frac{10!}{3!(10-3)!}=120$ (<a href=\"https://en.wikipedia.org/wiki/Combination\" target=\"_blank\">details</a>). As a brief reminder of how this works, there is a total of $10!$ different ordered combinations of our distinct flips 1 through 10. Let's say we identify 3 of those 10 flips as special for whatever reason (e.g. let's take the first 3 flips). Then there are $3!$ ways to order those flips (e.g. (1,2,3),(1,3,2),....) and there are $7!$ ways to order the remaining 7 flips. To find the *distinct* number of ways a group of 3 and a group of 7 can happen, we divide the total number of different ordered combinations $10!$ by these two groups. Thus, for all 10 flips, there are $_{10}C_{3}$ different ways to order a group of 3 flips and a group of 7 flips, where the group of 3 and the group of 7 are *distinct* from one another (one example being 1,2,3 is heads and 4...10 is tails). More generally, for $n$ total flips and $m$ heads/tails, the total number of distinct combinations is written as $\\frac{n!}{m!(n-m)!}$. \n", "\n", "\n", "* What is the probability of the scenario where we get 3 heads out of 10 total flips?\n", "\n", " * Each flip of the coin has equal probability of landing on heads. Let's say that this probability is $p$. With one heads and one tails in two flips, the probability would be the probability to get heads ($p$) multiplied by the probability to get tails ($1-p$), multiplied by the number of distinct combinations that would give you one head and one tail. In this case, the number of combinations is $2$ (heads first and tails second, *or* tails first and heads second). This yields a total probability of $p(1-p)\\times N_\\mathrm{combo}=2p(1-p)$.\n", "\n", " * In the case of 10 flips where there are 3 heads, the probability becomes the probability of 3 heads $p^{3}$ multiplied by the probability of 7 tails $(1-p)^{7}$ multiplied by $N_\\mathrm{combo}$. In general, for $n$ flips and $m$ heads, we have $p^{m}(1-p)^{n}\\times N_\\mathrm{combo}$. \n", "\n", "\n", "\n", "* What is the distribution?\n", " * If we combine everything for our specific case, we have $_{10}C_{3}\\times p^3(1-p)^7$ \n", " * The general formula is actually called the binomial distribution! It is given by $f(m)=p^{m}(1-p)^{n}\\frac{n!}{m!(n-m)!}$\n", "\n", "Let's actually compute this for a few cases!\n", "\n"]}, {"cell_type": "code", "execution_count": null, "id": "149413af", "metadata": {"tags": ["learner", "py", "lect_01", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L2.1-runcell01\n", "\n", "import numpy as np\n", "from scipy.special import comb\n", "print(\"Test comb:\",comb(2,1),\"True: 2\",comb(3,2),\"True: 3\",comb(10,3),\"True: 120\")\n", "\n", "#for p=0.5, what is the probability of 3 heads out of 10 draws?\n", "def prob(p=0.5,nheads=3,ntotal=10):\n", "    pheads=np.power(p,nheads)\n", "    ptails=np.power(1-p,ntotal-nheads)\n", "    combos=comb(ntotal,nheads)\n", "    return combos*ptails*pheads\n", "\n", "print(\"Probability of 3 heads in 10 draws is:\",prob(nheads=3,ntotal=10))"]}, {"cell_type": "markdown", "id": "d71379ab", "metadata": {"tags": ["learner", "md", "lect_01"]}, "source": ["Out of all of this math, we have derived the binomial distribution. This is the first empirical distribution we will need for this Lesson. In fact, all of the other distributions we will study are built upon the binomial distribution. Let's compute the expectation and variance of this distribution. First, we can define the distribution:\n", "\n", "$$\n", "\\begin{equation}\n", "f(p,n,m) = \\frac{n!}{m!(n-m)!}p^{m}(1-p)^{n-m}\\\\\n", "\\end{equation}\n", "$$\n", "\n", "Now, let's compute the expectation over $m$ for $p$ and $n$ fixed, defined as $E[m;p,n]$ the semicolon denotes fixed.\n", "\n", "$$\n", "\\begin{equation}\n", "E[m;p,n]=\\int_0^{n} p^{m}(1-p)^{n}\\frac{n!}{m!(n-m)!} \\mathrm{heads}(m) dm \\\\\n", "\\end{equation}\n", "$$\n", "\n", "where $\\mathrm{heads}(m)$ is a function that we define as the expected value given a heads or tails observation. In this case, we will define this function as $1$ for heads and $0$ for tails. This is a complicated form, but in the case of just getting one head out of one flip, we have: \n", "\n", "\n", "$$\n", "\\begin{equation}\n", "E[m;p,n=1]= p\\times1+(1-p)\\times0\\\\\n", "E[m;p,n=1]= p\n", "\\end{equation} \n", "$$\n", "\n", "Now let's introduce a new function that is defined as the sum of $n$ individual experiments, we can define the function $f(x)=\\sum_{i} \\rm{heads}(x_{i})$. The expectation for this is\n", "\n", "$$\n", "\\begin{eqnarray}\n", "E[f(x)]&=&\\sum_{0}^{n} p\\times 1+(1-p)\\times 0 \\\\\n", "E[f(x)]&=&np\\\\\n", "\\end{eqnarray}\n", "$$\n", "\n", "That means that the average value over $n$ tries $\\bar{x}=f(x)/n$, or rather simply the value $p$. \n", "\n", "In a similar way, we can define the variance as: \n", "\n", "$$\n", "\\begin{eqnarray}\n", "V[f(x)]&=&\\sum_{i=0}^{n} (x-\\mu)^2 \\\\\n", "V[f(x)]&=&\\sum_{i=0}^{n} p\\times(1-\\mu)^2 + (1-p) \\times (0-\\mu)^2 \\\\\n", "V[f(x)]&=&\\sum_{i=0}^{n} p \\times (1-p)^2 + (1-p) \\times (0-p)^2 \\\\\n", "V[f(x)]&=&\\sum_{i=0}^{n} (1-p)\\times (p^2 +p(1-p))  \\\\\n", "V[f(x)]&=&\\sum_{i=0}^{n} (1-p)\\times p   \\\\\n", "V[f(x)]&=&np(1-p)\n", "\\end{eqnarray}\n", "$$\n", "\n", "or in other words, if we consider performing $n$ independent measurements, the variance over this distribution becomes $V[f(x)/n]=V[f(x)]/n=p(1-p)$."]}, {"cell_type": "markdown", "id": "dde2d970", "metadata": {"tags": ["learner", "md", "lect_01"]}, "source": ["<h3>A Computational Example</h3>\n", "\n", "It's all fun to do math, but the point of this class is to do it with computers, so let's do the same derivations numerically. Note that we plot these as discrete lines rather than continuous points, since the binomial distribution is discrete and only defined for discrete numbers (integers in this case). \n"]}, {"cell_type": "code", "execution_count": null, "id": "9fbc58b5", "metadata": {"tags": ["learner", "py", "lect_01", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L2.1-runcell02\n", "\n", "#We are going to use scipy stats package\n", "import numpy as np\n", "import scipy.stats as stats\n", "import matplotlib.pyplot as plt\n", "n=30\n", "p=0.25\n", "\n", "#Scipy has a binomial, but since this is a discrete distribution we use pmf (probability mass function) rather than pdf\n", "k=np.arange(0,n)\n", "binomial=stats.binom.pmf(k,n,p)\n", "\n", "#let's get the integral of this guy\n", "norm=0\n", "exp=0\n", "var=0\n", "for i0 in range(n):\n", "    norm+=stats.binom.pmf(i0,n,p)\n", "    exp+=i0*stats.binom.pmf(i0,n,p)\n", "for i0 in range(n):\n", "    pVal=stats.binom.pmf(i0,n,p)\n", "    var+=(i0-exp/norm)*(i0-exp/norm)*pVal\n", "\n", "#Print it out\n", "print(\"norm:\",norm,\"expectation:\",exp/norm,\"Var:\",var/norm)\n", "\n", "#Now let's check with the expectation\n", "print(\"norm: 1.000000, expectation:\",n*p,\"Var:\",n*p*(1-p))\n", "\n", "plt.plot(k,binomial,'o')\n", "plt.vlines(k,0, binomial)\n", "plt.ylim(bottom=0)\n", "\n", "plt.xlabel(\"Number of successes\")\n", "plt.ylabel(\"Probability\")\n", "plt.show()"]}, {"cell_type": "markdown", "id": "267e36b8", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='exercises_2_1'></a>     \n", "\n", "| [Top](#section_2_0) | [Restart Section](#section_2_1) | [Next Section](#section_2_2) |\n"]}, {"cell_type": "markdown", "id": "f2695caa", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-2.1.1: Probability as a Function of Coin Fairness</span>\n", "\n", "In order to answer this question, plot the probability of flipping a coin 10 times and observing 3 heads, for varying \"coin fairness.\" In other words, as a function of probability $p$ (i.e., the probability $p$ of getting a heads is varying).\n", "\n", "\n", "<u>Hint:</u> Use the previously defined function `prob(p=0.5,nheads=3,ntotal=10)` and vary `p`, or use the built-in function `stats.binom.pmf(k,n,p)`, defined <a href=\"https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.binom.html\" target=\"_blank\">here</a>.\n", "\n", "\n", "As the probability $p$ varies from 0 to 1, which of the following correctly describes the behavior? The probability of observing 3 head out of 10 flips...\n", "\n", "- keeps increasing to a value of 1 when p=1\n", "- keeps decreasing to a value of 0 when p=1\n", "- increases from 0 at p=0 to a maximum value, then decreases to 0 at p=1\n", "- maintains a constant value for all p"]}, {"cell_type": "code", "execution_count": null, "id": "8decfd1b", "metadata": {"tags": ["py", "draft"]}, "outputs": [], "source": ["#>>>EXERCISE: L2.1.1\n", "# Use this cell for drafting your solution (if desired)\n", "\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "from scipy.special import comb\n", "\n", "#for p=0.5, what is the probability of 3 heads out of 10 draws?\n", "def prob(p=0.5,nheads=3,ntotal=10):\n", "    #YOUR CODE HERE\n", "    return\n", "\n", "\n", "def plot_prob(x):\n", "    #plotting-------------------\n", "    #plot data\n", "    ydata = prob(p=x,nheads=3,ntotal=10)\n", "    plt.plot(x, ydata, 'o')\n", "    plt.vlines(x,0, ydata)\n", "    plt.ylim(bottom=0)\n", "\n", "    #plot labels and style\n", "    plt.title('Probability of 3 Heads in 10 Draws for Varying Coin Fairness', fontsize=15)\n", "    #plt.legend(loc='lower right', fontsize = 12)\n", "    plt.xlabel('Coin Fairness (Probability of Landing Heads)', fontsize=15) #Label x\n", "\n", "    # changing the fontsize of ticks\n", "    plt.xticks(fontsize=12)\n", "    plt.yticks(fontsize=12)\n", "\n", "    # a grid\n", "    plt.grid()\n", "    plt.show()\n", "    \n", "    \n", "prob_vals = np.linspace(0,1,11)\n", "plot_prob(prob_vals)"]}, {"cell_type": "markdown", "id": "188d58b6", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-2.1.2: Rolling a Die</span>\n", "\n", "Now, instead of flipping a coin, consider rolling a die 10 times. If the die lands on 6, we consider the trial a success. If the die lands on anything other than 6, we consider the trial a failure. Using the formulae for expectation and variance that we previously defined, calculate the expectation and variance of the binomial distribution related to these criteria.\n", "\n", "Enter your answer as a list of numbers with precision 1e-2: `[expectation, variance]`"]}, {"cell_type": "markdown", "id": "d5f9a1e7", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": [">#### Follow-up 2.1.2a (ungraded)\n", ">    \n", ">Plot the binomial distribution of these trials, and calculate the norm (which should be 1), expectation, and variance. Use the starting code below."]}, {"cell_type": "code", "execution_count": null, "id": "6a003d42", "metadata": {"tags": ["draft", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>FOLLOW-UP: L2.1.2a\n", "# Use this cell for drafting your solution (if desired)\n", "\n", "#Follow example given in L2.1\n", "import scipy.stats as stats\n", "import matplotlib.pyplot as plt\n", "\n", "n=10\n", "p=1/6\n", "\n", "#use pmf rather than pdf\n", "k=np.arange(0,n)\n", "binomial=stats.binom.pmf(k,n,p)\n", "\n", "\n", "def get_binom_integral(n,p):\n", "    #get the integral\n", "    ##########\n", "    #YOUR CODE HERE\n", "    ##########\n", "    return norm, exp, pVal, var\n", "\n", "norm, exp, pVal, var = get_binom_integral(n,p)\n", "\n", "#print\n", "print(\"norm:\",norm,\"expectation:\",exp/norm,\", Var:\",var/norm)\n", "\n", "#check\n", "print(\"norm: 1.000000, expectation:\",n*p,\", Var:\",n*p*(1-p))\n", "\n", "    \n", "plt.plot(k,binomial,'o')\n", "plt.vlines(k,0, binomial)\n", "plt.ylim(bottom=0)\n", "\n", "plt.xlabel(\"Number of successes\")\n", "plt.ylabel(\"Probability\")\n", "plt.show()"]}, {"cell_type": "markdown", "id": "3ab245b0", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_2_2'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L2.2 Applications Using the Binomial Distribution</h2>  \n", "\n", "| [Top](#section_2_0) | [Previous Section](#section_2_1) | [Exercises](#exercises_2_2) | [Next Section](#section_2_3) |\n"]}, {"cell_type": "markdown", "id": "b41f5fb7", "metadata": {"tags": ["8S50x", "learner"]}, "source": ["*The material in this section is discussed in the video **<a href=\"https://courses.mitxonline.mit.edu/learn/course/course-v1:MITxT+8.S50.1x+3T2024/block-v1:MITxT+8.S50.1x+3T2024+type@sequential+block@seq_LS2/block-v1:MITxT+8.S50.1x+3T2024+type@vertical+block@vert_LS2_vid2\" target=\"_blank\">HERE</a>.** You are encouraged to watch that video and use this notebook concurrently.*"]}, {"cell_type": "markdown", "id": "a5784ae9", "metadata": {"tags": ["learner", "md", "lect_02"]}, "source": ["<h3>Overview</h3>\n", "\n", "Now, let's do some problems that are more difficult than flipping a coin. Also, let's think about this in a real life setting. Let's say that you are observing <a href=\"https://en.wikipedia.org/wiki/Fast_radio_burst\" target=\"_blank\">fast radio bursts</a> and, based on the Wikipedia page <a href=\"https://en.wikipedia.org/wiki/List_of_fast_radio_bursts\" target=\"_blank\">here</a>, you observe about 19 fast radio bursts per year. What is the probability that you observe 2 fast radio bursts (FRB) within a day of each other? \n", "\n", "The trick to this problem is think of each day as flipping a coin, where the probability of heads is instead the probability of finding a FRB. We can caculate the average probability by noting that over a period 365 days (i.e. 365 experiments), we see 19 FRBs, or in other words:\n", "\n", "$$\n", "\\begin{eqnarray}\n", "E[f(x;n=365)]&=&np \\\\\n", "             &=&19 \\\\\n", "             &=&365\\times p \\\\\n", "            p&=&\\frac{19}{365}\n", "\\end{eqnarray}\n", "$$\n", "\n", "\n", "Thus, the probability of $2$ in a row turns out to be 0.3% (see below). Moreover, the probability of 2 observations in 7 days is (see below) 4.3%.\n"]}, {"cell_type": "code", "execution_count": null, "id": "99c8c63b", "metadata": {"tags": ["learner", "py", "lect_02", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L2.2-runcell01\n", "\n", "def prob(ndays=2,nobs=2,p=19/365):\n", "    return stats.binom.pmf(nobs,ndays,p)\n", "\n", "print(\"2 observations in 2 days:\",prob())\n", "print(\"2 observations in 7 days:\",prob(7))\n", "print(\"19 observations in 365 days:\",prob(365,19))\n"]}, {"cell_type": "markdown", "id": "a9f60a5a", "metadata": {"tags": ["learner", "md", "lect_02"]}, "source": ["Now, let's ask an important physics question. Let's say you observed 2 FRBs back to back. Given the probability of this occurence is so low, is something significant happening in the universe? <a href=\"https://en.wikipedia.org/wiki/Fast_radio_burst#FRB_201124\" target=\"_blank\">Read here.</a>\n", "\n", "Secondly, why is the probability of 19 observations in 365 days so low? (only 10%). To understand this let's make a plot. \n"]}, {"cell_type": "code", "execution_count": null, "id": "78aa681e", "metadata": {"tags": ["learner", "py", "lect_02", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L2.2-runcell02\n", "\n", "p=19/365\n", "n=365\n", "k=np.arange(0,50)\n", "binomial=stats.binom.pmf(k,n,p)\n", "\n", "def plotBinomial(iX,iBinomial,label='Binomial',color='black'):\n", "    plt.plot(iX,iBinomial,'o')\n", "    plt.vlines(iX,0, iBinomial,label=label,color=color)\n", "    plt.ylim(bottom=0)\n", "    plt.xlabel(\"Number of observations per year\")\n", "    plt.ylabel(\"Probability\")\n", "\n", "plotBinomial(k,binomial)\n"]}, {"cell_type": "markdown", "id": "c7958a63", "metadata": {"tags": ["learner", "md", "lect_02"]}, "source": ["Getting exactly 19 observations is unlikely because there is variation. What we really want to do is integrate the number of observations that are either less than or equal to 19. This is the cumulative distribution function. \n", "\n", "$$\n", "\\begin{equation}\n", "\\mathrm{CDF}(\\mathrm{binomial}(x)) = \\int_{-\\infty}^{x} \\mathrm{binomial}(u;p,k) du \n", "\\end{equation}\n", "$$\n", "\n", "The nice thing is that this is all built into our statistics code. Let's plot it!\n"]}, {"cell_type": "code", "execution_count": null, "id": "8d21604c", "metadata": {"tags": ["learner", "py", "lect_02", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L2.2-runcell03\n", "\n", "p=19/365\n", "n=365\n", "k=np.arange(0,50)\n", "binomial=stats.binom.pmf(k,n,p)\n", "binomialcdf=stats.binom.cdf(k,n,p)\n", "print(\"cdf at 19:\",stats.binom.cdf(19,n,p))\n", "\n", "plt.plot(k,binomialcdf,'o', label=\"Binomial CDF\")\n", "plt.vlines(k,0, binomialcdf, color=plt.gca().lines[-1].get_color())\n", "plt.ylim(bottom=0)\n", "\n", "plt.plot(k,binomial,'o', label=\"Binomial PMF\")\n", "plt.vlines(k,0, binomial, color=plt.gca().lines[-1].get_color())\n", "plt.ylim(bottom=0)\n", "\n", "plt.xlabel(\"Number of observations per year\")\n", "plt.ylabel(\"Probability\")\n", "plt.legend()\n", "plt.show()\n", "\n", "\n", "mean = np.average(k, weights=binomial)\n", "variance = np.average((k-mean)**2, weights=binomial)\n", "print(\"mean:\",mean,\"stddev:\",np.sqrt(variance))\n", "\n"]}, {"cell_type": "markdown", "id": "671cb843", "metadata": {"tags": ["learner", "md", "lect_02"]}, "source": ["So, now we see clearly that the CDF is at approximately 50% for 19 observations. It's not exactly 50% for the simple fact that this is a discrete distribution. However, the expectation will be at exactly 19. "]}, {"cell_type": "markdown", "id": "e2af4818", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='exercises_2_2'></a>     \n", "\n", "| [Top](#section_2_0) | [Restart Section](#section_2_2) | [Next Section](#section_2_3) |\n"]}, {"cell_type": "markdown", "id": "5febd043", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-2.2.1: Rate of GW Detections</span>\n", "\n", "Let's do another related problem. With the current rate of gravitational wave (GW) detections, we observe a GW once per week. What is the probability that on 3 or more *days* gravitational waves are detected in one week? Note here that the time interval is in days. Use the starting code below to compute your answer."]}, {"cell_type": "markdown", "id": "0417592e", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": [">#### Follow-up 2.2.1a (ungraded)\n", ">  \n", ">Try plotting this distribution! Additionally, what is the probability distribution for the number of days in which GW events were observed over a whole year, and what is the mean and variance of this distribution? Use the starting code below."]}, {"cell_type": "code", "execution_count": null, "id": "3ad465dc", "metadata": {"tags": ["py", "draft", "learner_chopped"]}, "outputs": [], "source": ["#>>>FOLLOW-UP: L2.2.1a\n", "# Use this cell for drafting your solution (if desired)\n", "\n", "import scipy.stats as stats\n", "\n", "def plotBinomial(iX,iBinomial,label='Binomial',color='black'):\n", "    plt.plot(iX,iBinomial,'o')\n", "    plt.vlines(iX,0, iBinomial,label=label,color=color)\n", "    plt.ylim(bottom=0)\n", "    plt.xlabel(\"Number of observations per year\")\n", "    plt.ylabel(\"Probability\")\n", "\n", "\n", "\n", "#now what about for GWs in a year\n", "n = #YOUR CODE HERE\n", "p = #YOUR CODE HERE\n", "k = #YOUR CODE HERE\n", "\n", "binomial=stats.binom.pmf(k,n,p)\n", "\n", "plotBinomial(k,binomial)\n", "plt.show()\n", "\n", "average  = np.average(k, weights=binomial)\n", "variance = np.average((k-average)**2, weights=binomial)\n", "print(\"mean:\",average,\"stddev:\",np.sqrt(variance))\n", "\n"]}, {"cell_type": "markdown", "id": "d98afb62", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-2.2.2: Probability of Coin Flips</span>\n", "\n", "What is the probability of 2 heads in 10 coin flips, given a 50% probability for heads? What about if there is a 10% probability for heads?\n", "\n", "Enter your answer as a list of two numbers, where the numbers correspond to probabilities: `[prob with p=50%, prob with p=10%]`\n", "\n", "Use the starting code below to aid your calculation."]}, {"cell_type": "code", "execution_count": null, "id": "b1378c33", "metadata": {"tags": ["draft", "py"]}, "outputs": [], "source": ["#>>>EXERCISE: L2.2.2\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "def prob(nheads=2,nflips=10,p=0.5):\n", "    return #your code here\n", "\n", "print(\"2 heads in 10 flips:\",prob())\n", "print(\"2 heads in 10 flips:\",prob(p=1/10))"]}, {"cell_type": "markdown", "id": "910a11da", "metadata": {"tags": ["learner", "md"]}, "source": [">#### Follow-up 2.2.2a (ungraded)\n", "> \n", ">Given the situation described in the exercise above (2 heads seen in 10 coin flips), how confidently can we conclude that our coin is biased?\n"]}, {"cell_type": "markdown", "id": "d55932f2", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_2_3'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L2.3 The Poisson Distribution</h2>  \n", "\n", "| [Top](#section_2_0) | [Previous Section](#section_2_2) | [Exercises](#exercises_2_3) | [Next Section](#section_2_4) |\n"]}, {"cell_type": "markdown", "id": "3cbb938a", "metadata": {"tags": ["8S50x", "learner"]}, "source": ["*The material in this section is discussed in the video **<a href=\"https://courses.mitxonline.mit.edu/learn/course/course-v1:MITxT+8.S50.1x+3T2024/block-v1:MITxT+8.S50.1x+3T2024+type@sequential+block@seq_LS2/block-v1:MITxT+8.S50.1x+3T2024+type@vertical+block@vert_LS2_vid3\" target=\"_blank\">HERE</a>.** You are encouraged to watch that video and use this notebook concurrently.*"]}, {"cell_type": "markdown", "id": "a1b3c733", "metadata": {"tags": ["learner", "md", "lect_03"]}, "source": ["<h3>Overview</h3>\n", "\n", "The ugly thing about the binomial distribution is that it has these damn factorials. One way to get rid of the factorials is to do an approximation of the binomial distribution. We can define this by taking a limit over the number of experiments going to infinity $n\\rightarrow\\infty$. To do this, we first define $\\lambda$ as:\n", "\n", "$$\n", "\\begin{equation}\n", "\\lambda = \\lim_{n\\rightarrow\\infty} np \\rightarrow p=\\frac{\\lambda}{n} \\\\\n", "\\end{equation}\n", "$$\n", "\n", "Now, in this limiting case, we can replace the binomial distribution with an approximate form that has fewer factorials:\n", "\n", "\n", "$$\n", "\\begin{eqnarray}\n", "\\lim_{n\\rightarrow\\infty}\\frac{n!}{m!(n-m)!}p^{m}(1-p)^{n} & = & \\frac{n(n-1)...(n-m+1)}{m!}\\left(\\frac{\\lambda}{n}\\right)^{m}\\left(1-\\frac{\\lambda}{n}\\right)^{n} \\\\\n", "&\\approx&\\frac{n^m}{m!}\\frac{\\lambda^{m}}{n^{m}}\\left(1-\\frac{\\lambda}{n}\\right)^{n} \\\\\n", "&\\approx&\\frac{\\lambda^{m}}{m!}e^{-\\lambda} \\\\\n", "f(m;\\lambda=np) & = & \\frac{\\lambda^{m}}{m!}e^{-\\lambda}\n", "\\end{eqnarray}\n", "$$\n", "\n", "This form is known as the Poisson distribution, and is achieved by taking the binomial distribution to the large $n$ limit. We still have a pesky factorial, but one factorial is better than three factorials. \n", "\n", "We can treat the Poisson distribution just like the binomial distribution, using something similar to the computations done previously for the mean and variance (of $n\\rightarrow\\infty$ experiments sampling a Poisson distribution, noting that $p\\rightarrow0$ in the large $n$ limit):\n", "\n", "\n", "$$\n", "\\begin{equation}\n", "E[f(x)]=\\lambda \\\\\n", "V[f(x)]=\\lambda\n", "\\end{equation}\n", "$$\n", "\n", "What is most important from this observation is that the standard deviation of the distribution goes as the $\\sqrt{\\lambda}$ or the square root of the mean of the distribution. This will play a critical role going forward. \n", "\n", "<br>\n", "<!--end-block-->\n"]}, {"cell_type": "markdown", "id": "3a9169de", "metadata": {"tags": ["learner", "md", "lect_03"]}, "source": ["<h3>Comparison</h3>\n", "\n", "Now, let's see how a Poisson compares to a binomial in our previous plots. Let's use our FRB example $p=19/365$. Alternatively, let's also consider the probability of a sunny day in Boston $p=200/365$.\n", "\n", "<br>\n", "<!--end-block-->\n"]}, {"cell_type": "code", "execution_count": null, "id": "0f0862a0", "metadata": {"tags": ["learner", "py", "lect_03", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L2.3-runcell01\n", "\n", "#Let's make a function for plotting\n", "def plotWeekYear(p, title=''):\n", "    #Week comparison\n", "    n=7\n", "    k=np.arange(0,n+1)\n", "    binomial_week=stats.binom.pmf(k,n,p)\n", "    poisson_week=stats.poisson.pmf(k,n*p)#note we give lambda=n*p\n", "    plt.title(title)\n", "    plotBinomial(k,binomial_week,label='Binomial',color='blue')\n", "    plotBinomial(k,poisson_week,label='Poisson',color='orange')\n", "    plt.legend(loc='upper right')\n", "    plt.xlabel('number of observations per week')\n", "    plt.show()\n", "\n", "    n=365\n", "    k=np.arange(0,2*p*n)\n", "    binomial_year=stats.binom.pmf(k,n,p)\n", "    poisson_year=stats.poisson.pmf(k,n*p)#note we give lambda=n*p\n", "    plt.title(title)\n", "    plotBinomial(k,binomial_year,label='Binomial',color='blue')\n", "    plotBinomial(k,poisson_year,label='Poisson',color='orange')\n", "    plt.legend(loc='upper right')\n", "    plt.show()\n", "\n", "    average  = np.average(k, weights=binomial_year)\n", "    variance = np.average((k-average)**2, weights=binomial_year)\n", "    print(\"Yearly Binomial mean:\",average,\"stddev:\",np.sqrt(variance))\n", "    \n", "    average  = np.average(k, weights=poisson_year)\n", "    variance = np.average((k-average)**2, weights=poisson_year)\n", "    print(\"Yearly Poisson mean:\",average,\"stddev:\",np.sqrt(variance))\n", "    print()\n", "\n", "    \n", "#First FRBs\n", "p=19/365\n", "plotWeekYear(p, title='Probability of Observing FRBs')\n", "\n", "#Now let's do sunny days\n", "p=200/365\n", "plotWeekYear(p, title='Probability of Sunny Days in Boston')\n"]}, {"cell_type": "markdown", "id": "f4059087", "metadata": {"tags": ["learner", "md", "lect_03"]}, "source": ["So, we see that the Poisson approximation is really quite good for the case where the $p\\ll1$. However, when $p$ is large (like the number of sunny days in the example above) and the number of events is small, the Poisson result can be quite far off. Just look at the number of sunny days per week. The binomial distributions gives about two percent probability of having one sunny day in a week, vs. nearly eight percent probability given by the Poisson distribution. Looking at the number of observations per year, clearly there is also a dramatic difference in the width of the distributions with $p$ is large.\n", "\n", "Which one of these distributions is correct for weather? (Answer: Neither are good because the weather from yesterday gives you some information about what will happen today, it's not a random process on a day-to-day level)"]}, {"cell_type": "markdown", "id": "54e627c0", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='exercises_2_3'></a>     \n", "\n", "| [Top](#section_2_0) | [Restart Section](#section_2_3) | [Next Section](#section_2_4) |\n"]}, {"cell_type": "markdown", "id": "4e87813d", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-2.3.1: GW Detection Timescale Comparison</span>\n", "\n", "With the current rate of Gravitational wave detections, we observe a GW once per week (one way to phrase this is that the probability of a GW on a given day is 1/7). Compare the Poisson and binomial distributions for the number of days which gravitational waves have been observed over the period of a week vs. a year. **Hint: the previously defined function `plotWeekYear` may be useful.**\n", "\n", "Can GW detections be reasonably approximated by a Poisson process?"]}, {"cell_type": "code", "execution_count": null, "id": "dc870c7a", "metadata": {"tags": ["draft", "py"]}, "outputs": [], "source": ["#>>>EXERCISE: L2.3.1\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "pass"]}, {"cell_type": "markdown", "id": "6694499b", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-2.3.2: Fraction of GWs per Week</span>\n", "\n", "If more GWs are detected per week, on average, will a Poisson distribution be a BETTER fit for the distribution of GW observations or a WORSE fit?"]}, {"cell_type": "markdown", "id": "7801d58d", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_2_4'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L2.4 Poisson Distribution Continued</h2>  \n", "\n", "| [Top](#section_2_0) | [Previous Section](#section_2_3) | [Exercises](#exercises_2_4) | [Next Section](#section_2_5) |\n"]}, {"cell_type": "markdown", "id": "74337cb1", "metadata": {"tags": ["8S50x", "learner"]}, "source": ["*The material in this section is discussed in the video **<a href=\"https://courses.mitxonline.mit.edu/learn/course/course-v1:MITxT+8.S50.1x+3T2024/block-v1:MITxT+8.S50.1x+3T2024+type@sequential+block@seq_LS2/block-v1:MITxT+8.S50.1x+3T2024+type@vertical+block@vert_LS2_vid4\" target=\"_blank\">HERE</a>.** You are encouraged to watch that video and use this notebook concurrently.*"]}, {"cell_type": "markdown", "id": "4c545768", "metadata": {"tags": ["learner", "md", "lect_04"]}, "source": ["<h3>Overview</h3>\n", "\n", "Now, why are we spending so much time on Poisson distributions? Let's say I have a distribution that is flat and I sampled that distribution 10000 times, and then made a histogram with 100 bins. Let's make a distribution like that!"]}, {"cell_type": "code", "execution_count": null, "id": "12d02cbd", "metadata": {"tags": ["learner", "py", "lect_04", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L2.4-runcell01\n", "\n", "fig, ax = plt.subplots(figsize=(9,6))\n", "N=10000\n", "nbins=100\n", "sample  = np.random.uniform (0,1,N)\n", "\n", "def plotHist(iSample,iNBins):\n", "    histy, bin_edges = np.histogram(iSample, bins=iNBins)\n", "    bin_centers = 0.5*(bin_edges[1:] + bin_edges[:-1])\n", "    ax.set_ylim([0,2*N/nbins])\n", "    plt.plot(bin_centers,histy,drawstyle = 'steps-mid')\n", "    plt.xlabel(\"x\")\n", "    plt.ylabel(\"Events/bin\")\n", "    plt.show()\n", "    return bin_centers, histy\n", "\n", "_,_ = plotHist(sample,nbins)\n"]}, {"cell_type": "markdown", "id": "da42eb57", "metadata": {"tags": ["learner", "md", "lect_04"]}, "source": ["Each sampling has a 1/100 probability of being in any one of those bins. \n", "\n", "Let's look at the mean and variance over the bins. What is the distribution of the variations over these bins? \n"]}, {"cell_type": "code", "execution_count": null, "id": "e0337645", "metadata": {"tags": ["learner", "py", "lect_04", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L2.4-runcell02\n", "\n", "#copy and past above distribution\n", "fig, ax = plt.subplots(figsize=(9,6))\n", "N=10000 \n", "#N=1000000 #Try larger N value\n", "nbins=100\n", "sample  = np.random.uniform (0,1,N)\n", "histx, histy = plotHist(sample,nbins)\n", "\n", "\n", "def normhist(iVars,iNbins=30,iNormalize=True):\n", "    y0, bin_edges = np.histogram(iVars, bins=iNbins)\n", "    bin_centers = 0.5*(bin_edges[1:] + bin_edges[:-1])\n", "    norm0 = 1 \n", "    if iNormalize:\n", "        norm0=len(iVars)*(bin_edges[-1]-bin_edges[0])/iNbins\n", "    plt.errorbar(bin_centers,y0/norm0,yerr=y0**0.5/norm0,drawstyle = 'steps-mid',c='red')\n", "    return bin_centers,y0,bin_edges\n", "\n", "residx,residy,_=normhist(histy)\n", "haverage  = np.average(residx, weights=residy)\n", "hvariance = np.average((residx-haverage)**2, weights=residy)\n", "print(\"Actual mean:\",haverage,\"Variance:\",hvariance) \n", "\n", "#Now since we have 100 bins with p=1/100 and we sample 10000 times we have lamb=np= N (1/nbins)\n", "lamb=N/nbins # Number events/bin = 100\n", "k=np.arange(0.55*N/nbins,1.45*N/nbins) \n", "#k=np.arange(0.85*N/nbins,1.15*N/nbins) #adjust range if using larger N \n", "poisson=stats.poisson.pmf(k,lamb)#lambda = n * p = 10000 * (1/100)\n", "paverage  = np.average(k, weights=poisson)\n", "pvariance = np.average((k-paverage)**2, weights=poisson)\n", "print(\"Poisson mean:\",paverage,\"Variance:\",pvariance)\n", "\n", "plt.plot(k,poisson,'o')\n", "# plt.vlines(k,0, poisson)\n", "plt.ylim(bottom=0)\n", "\n", "plt.xlabel(\"Mean per bin\")\n", "plt.ylabel(\"probability\")\n", "plt.show()"]}, {"cell_type": "markdown", "id": "cc403c95", "metadata": {"tags": ["learner", "md", "lect_04"]}, "source": ["It's a Poisson distribution! Now, this brings us to a very important plot. If we have a histogram with $N$ events in a particular bin, what are the fluctuations in that bin? \n", "\n", "\n", "If it is Poisson, then the variance is going to be $N$ and the standard deviation is going to be $\\sqrt{N}$. As a consequence, we can characterize the fluctuations per bin by the standard deviation. Thus, whenever we have a plot with data and we want to plot the expected fluctuations per bin, we plot the Poisson fluctuations. The previous distribution would thus look like:"]}, {"cell_type": "code", "execution_count": null, "id": "342d96ea", "metadata": {"tags": ["learner", "py", "lect_04", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L2.4-runcell03\n", "\n", "#And so the bins are Poisson fluctuated. This is why when we plot data in a histogram we put error bars \n", "#Corresponding the Poisson uncertainty in a bin\n", "N=10000\n", "nbins=100\n", "sample  = np.random.uniform (0,1,N)\n", "histy, bin_edges = np.histogram(sample, bins=nbins)\n", "yerr=np.sqrt(histy)\n", "bin_centers = 0.5*(bin_edges[1:] + bin_edges[:-1])\n", "ax.set_ylim([0,2*N/nbins])\n", "\n", "#Here is the command\n", "plt.errorbar(bin_centers,histy,yerr=yerr,marker='.',c='black',linestyle = 'None',label='Data')\n", "print(np.mean(yerr))\n", "\n", "k=np.arange(0,1,0.01)\n", "vals=np.full((100),N/nbins)\n", "plt.plot(k,vals,'o--',label=\"Expected value\")\n", "plt.ylim(0,150)\n", "plt.xlabel(\"x\")\n", "plt.ylabel(\"Events/bin\")\n", "plt.legend(loc='lower right')\n", "plt.show()"]}, {"cell_type": "markdown", "id": "ca231f1b", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='exercises_2_4'></a>     \n", "\n", "| [Top](#section_2_0) | [Restart Section](#section_2_4) | [Next Section](#section_2_5) |\n"]}, {"cell_type": "markdown", "id": "a58d32cc", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-2.4.1: Calculating Error for a Poisson Distribution</span>\n", "\n", "For 100 bins, what is the Poisson error (standard deviation) averaged over all bins for an experiment run 100, 1000, and 10000 times?\n", "\n", "Hint: You can use the code below to help calculate the yerr for all bins and then average them yourself.\n", "\n", "Enter your answer as a list of numbers rounded to the nearest integer: `[avg(100), avg(1000), avg(10000)]`."]}, {"cell_type": "code", "execution_count": null, "id": "b4541317", "metadata": {"tags": ["draft", "py"]}, "outputs": [], "source": ["#>>>EXERCISE: L2.4.1\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "N=10000 #YOUR CODE HERE [VARY 100, 1000, 10000]\n", "nbins= 100 \n", "\n", "sample  = np.random.uniform (0,1,N)\n", "histy, bin_edges = np.histogram(sample, bins=nbins)\n", "yerr=np.sqrt(histy)\n", "\n", "#YOUR CODE HERE"]}, {"cell_type": "markdown", "id": "f0d13378", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_2_5'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L2.5 The Gaussian Distribution</h2>  \n", "\n", "| [Top](#section_2_0) | [Previous Section](#section_2_4) | [Exercises](#exercises_2_5) | [Next Section](#section_2_6) |\n"]}, {"cell_type": "markdown", "id": "57553907", "metadata": {"tags": ["8S50x", "learner"]}, "source": ["*The material in this section is discussed in the video **<a href=\"https://courses.mitxonline.mit.edu/learn/course/course-v1:MITxT+8.S50.1x+3T2024/block-v1:MITxT+8.S50.1x+3T2024+type@sequential+block@seq_LS2/block-v1:MITxT+8.S50.1x+3T2024+type@vertical+block@vert_LS2_vid5\" target=\"_blank\">HERE</a>.** You are encouraged to watch that video and use this notebook concurrently.*"]}, {"cell_type": "markdown", "id": "8c6cddf6", "metadata": {"tags": ["learner", "md", "lect_05"]}, "source": ["<h3>Overview</h3>\n", "\n", "The Poisson distribution discussed above is very powerful. However, we will often view it as a subset of the Normal or Gaussian distributions, given by the form: \n", "\n", "\n", "$$\n", "\\begin{equation}\n", "\\mathcal{N}(x,\\mu,\\sigma)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{\\frac{(x-\\mu)^2}{2\\sigma^2}}\n", "\\end{equation}\n", "$$\n", "\n", "This distribution has the very important properties that you can derive yourselves: \n", "\n", "$$\n", "\\begin{equation}\n", "E[N(x,\\mu,\\sigma]=\\mu \\\\\n", "V[N(x,\\mu,\\sigma]=\\sigma^2 \\\\\n", "\\end{equation}\n", "$$\n", "\n", "It is effectively a Poisson distribution where the variance is now not $\\sigma=\\lambda$, but instead a free parameter $\\sigma$. A Gaussian is often viewed as a generalized version of the Poisson distribution. There are many names for this distribution. Mathematicians and statistician's often call this the Normal distribution. The public frequently refers to this as the bell curve. Physicists call this the Gaussian distribution. These notes will refer to it as Gaussian, since Normal can be confusing (especially with non-native English speakers). \n", "\n", "\n", "While the Gaussian distribution looks simple, there are several things to notice about it. The most important is that the CDF\n", "\n", "$$\n", "\\begin{equation}\n", "\\mathcal{N}(x,\\mu,\\sigma)=\\int_{x}^{\\infty} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{\\frac{(u-\\mu)^2}{2\\sigma^2}} du\n", "\\end{equation}\n", "$$\n", "\n", "does not have a closed analytic form. In fact, we have to integrate this numerically. \n", "\n", "What makes the Gaussian distribution so powerful is that it appears all over the place. Let's understand the Gaussian distribution in the context of the most important theorem in all of statistics.\n"]}, {"cell_type": "markdown", "id": "6031ed00", "metadata": {"tags": ["learner", "md", "lect_05"]}, "source": ["<h3>Central Limit Theorem</h3>\n", "\n", "Recall that in Lesson 1 we derived the sum distribution of two objects. This gave us a triangle distribution. What happens when we consider the sum of more than just two numbers, in particular as we approach the sum of a very large set of numbers?\n"]}, {"cell_type": "code", "execution_count": null, "id": "d737ee3d", "metadata": {"scrolled": false, "tags": ["learner", "py", "lect_05", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L2.5-runcell01\n", "\n", "import math\n", "\n", "def plotSum(iN):\n", "    ntoys=10000\n", "    sums=np.array([])\n", "    for i0 in range(ntoys):\n", "        pToy = np.random.uniform(0,10,iN)\n", "        sums = np.append(sums,pToy.sum())\n", "    _,_,binrange=normhist(sums) #plots a Gaussian hist\n", "    k=np.linspace(binrange[0],binrange[-1], 50)\n", "    normal=stats.norm.pdf(k,sums.mean(),sums.std())\n", "    plt.plot(k,normal,'o-')\n", "    plt.xlabel(\"Number of successes\")\n", "    plt.ylabel(\"Probability\")\n", "    print(\"Summing:\",iN,\" numbers with mean:\",sums.mean(),\" and std-deviation\",sums.std(),sums.mean()/math.sqrt(3*iN))\n", "    plt.show()\n", "\n", "plotSum(1)\n", "plotSum(2)\n", "plotSum(3)\n", "plotSum(4)\n", "plotSum(50)\n", "plotSum(5000)\n"]}, {"cell_type": "markdown", "id": "ce376be6", "metadata": {"tags": ["learner", "md", "lect_05"]}, "source": ["So, the sum of a large group of random numbers drawn from a uniform distribution approaches a Gaussian. This is a very important statement. Effectively, this means that any combination of random variables is a Gaussian distribution; this sounds crazy! We will not show the full proof here, but suffice it to say, doing the integrals yields the same observation. \n", "\n", "\n", "Another interesting thing to note is that the standard deviation of this Gaussian is incidentally given by the (range of uniform distribution)$/\\sqrt{12}$. To verify this, for the last experiment with $N = 5000$ draws, we have the following:\n"]}, {"cell_type": "code", "execution_count": null, "id": "5d436079", "metadata": {"tags": ["learner", "py", "lect_05", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L2.5-runcell02\n", "\n", "N = 5000\n", "unif_range = 10\n", "print(\"stddev:\", np.sqrt(N * unif_range ** 2 / 12))\n"]}, {"cell_type": "markdown", "id": "7bf5703e", "metadata": {"tags": ["learner", "md", "lect_05"]}, "source": ["This matches the standard deviation we obtain numerically. To see this analytically, let's compute it using the variable substitution $2a'=b-a$.\n", "\n", "$$\n", "\\begin{eqnarray}\n", "V[x]&=&\\int_{a}^{b}\\frac{1}{b-a}\\left(x-\\frac{b-a}{2}\\right)^2 dx\\\\\n", "V[x]&=&\\int_{-a^\\prime}^{a^\\prime}\\frac{1}{2a^\\prime}\\left(x\\right)^2 dx\\\\\n", "V[x]&=&\\frac{1}{2a^\\prime}\\frac{1}{3}\\left(x\\right)^3|_{-a^\\prime}^{a^\\prime} \\\\\n", "V[x]&=&\\frac{2a'^3}{2a^\\prime}\\frac{1}{3} \\\\\n", "V[x]&=&\\frac{\\left(\\frac{b-a}{2}\\right)^2}{3} \\\\\n", "V[x]&=&\\frac{\\left(b-a\\right)^2}{12} \\\\\n", "\\end{eqnarray}\n", "$$\n", "\n", "So, to get the RMS of $N$ random variables summed up, we multiply this variance by $N$. We can further note that the average of $N$ summed variables gives $\\bar{x}=\\frac{b-a}{2}$. Combining all of this, we have: \n", "\n", "\n", "\n", "$$\n", "\\begin{eqnarray}\n", "V[x_1+x_2+...+x_N]&=& N \\frac{\\left(b-a\\right)^2}{12}\\\\\n", "V[x_1+x_2+...+x_N]&=& N \\frac{\\bar{x}^2}{3}\\\\\n", "\\end{eqnarray}\n", "$$\n", "\n", "This is why we can calculate the standard deviation of our sample using the above formula. In any case, we will take this demo as a proof by demo of what we call **the central limit theorem** which states that **for any distribution composed of inputs from a large number of continuous random variables, the sum tends to a Gaussian**. For fun, outside of class, go ahead and derive it."]}, {"cell_type": "markdown", "id": "e0355fe1", "metadata": {"tags": ["learner", "md", "lect_05"]}, "source": ["<h3>Comparison of Poisson and Gaussian</h3>\n", "\n", "Let's compare the Gaussian with a Poisson distribution, so we can connect all of our friends together. \n", "\n", "Compare the Poisson distribution to a Gaussian distribution for $\\lambda$=3, 15, 100. How do these distributions vary? "]}, {"cell_type": "code", "execution_count": null, "id": "51383fde", "metadata": {"tags": ["learner", "py", "lect_05", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L2.5-runcell03\n", "\n", "#solution 1\n", "##### Let's plot a Gaussian and Poisson with same mean and RMS\n", "def poisGausPlot(n):\n", "    lamb=n\n", "    k=np.arange(-2,3.0*n)\n", "    poisson=stats.poisson.pmf(k,lamb)\n", "    normal=stats.norm.pdf(k,n,math.sqrt(n))\n", "    plt.plot(k,poisson,'o',label='Poisson')\n", "    plt.vlines(k,0, poisson, color=plt.gca().lines[-1].get_color())\n", "    plt.ylim(bottom=0)\n", "    plt.plot(k,normal,'-',label='Gaussian')\n", "    plt.xlabel(\"Number of successes\")\n", "    plt.ylabel(\"Probability\")\n", "    plt.legend(loc='lower right')\n", "    plt.show()\n", "\n", "poisGausPlot(3)\n", "poisGausPlot(15)\n", "poisGausPlot(100)\n"]}, {"cell_type": "markdown", "id": "6c444db1", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='exercises_2_5'></a>     \n", "\n", "| [Top](#section_2_0) | [Restart Section](#section_2_5) | [Next Section](#section_2_6) |\n"]}, {"cell_type": "markdown", "id": "f3c3a1c7", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-2.5.1: Sum of Two Gaussians</span>\n", "\n", "Show that the sum of two Gaussian distributions is also Gaussian. To do this, plot the normalized histogram of the sum of two numbers drawn from identical Gaussian distributions. In the same figure, plot a Gaussian distribution with mean and standard deviation equal to the mean and standard deviation of the summed distribution. Write your own code, or run the code below.\n", "\n", "\n", "Based on the output of your code, how is the standard deviation of the summed distribution, $\\sigma_{\\mathrm{sum}}$, related to the standard deviation of the Gaussian distributions from which the samples are drawn (call this $\\sigma_0$)? Choose from the options below.\n", "\n", "- $\\sigma_{\\mathrm{sum}} = 2\\sigma_0$\n", "- $\\sigma_{\\mathrm{sum}} = \\sqrt{2}\\sigma_0$\n", "- $\\sigma_{\\mathrm{sum}} = \\sigma_0$\n", "- $\\sigma_{\\mathrm{sum}} = \\sigma_0/\\sqrt{2}$\n", "- $\\sigma_{\\mathrm{sum}} = \\sigma_0/2$\n", "\n", "\n", "How would this relation change if you summed more samples (here we just did 2, 100000 times). Try varying $\\sigma$ and the number of samples chosen (where are these defined in the code)?"]}, {"cell_type": "code", "execution_count": null, "id": "07e31baf", "metadata": {"tags": ["draft", "py"]}, "outputs": [], "source": ["#>>>EXERCISE: L2.5.1\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "#Generate 2 Gaussian and sum \n", "ntoys=100000\n", "istdev=1\n", "sums=np.array([])\n", "for i0 in range(ntoys):\n", "    pToy = np.random.normal(0,istdev,2)\n", "    sums = np.append(sums,pToy.sum())\n", "_,_,binrange=normhist(sums)\n", "\n", "k=np.arange(binrange[0],binrange[-1])\n", "normal=stats.norm.pdf(k,sums.mean(),sums.std())\n", "\n", "plt.plot(k,normal,'o-')\n", "plt.xlabel(\"Number of successes\")\n", "plt.ylabel(\"Probability\")\n", "print(\"Summing: 2, numbers with mean:\",sums.mean(),\" and std-deviation\",sums.std())\n", "plt.show()"]}, {"cell_type": "markdown", "id": "4b6815a7", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": [">#### Follow-up 2.5.1a (ungraded)\n", ">  \n", ">We've compared the Poisson and binomial distributions, and Poisson and Gaussian distribution. Now try comparing the binomial and Gaussian distributions. What similarities and differences do they have?"]}, {"cell_type": "markdown", "id": "9dffc6f2", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_2_6'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L2.6 Uncertainties in Measurement</h2>  \n", "\n", "| [Top](#section_2_0) | [Previous Section](#section_2_5) | [Exercises](#exercises_2_6) | [Next Section](#section_2_7) |\n"]}, {"cell_type": "markdown", "id": "57f7bc74", "metadata": {"tags": ["8S50x", "learner"]}, "source": ["*The material in this section is discussed in the video **<a href=\"https://courses.mitxonline.mit.edu/learn/course/course-v1:MITxT+8.S50.1x+3T2024/block-v1:MITxT+8.S50.1x+3T2024+type@sequential+block@seq_LS2/block-v1:MITxT+8.S50.1x+3T2024+type@vertical+block@vert_LS2_vid6\" target=\"_blank\">HERE</a>.** You are encouraged to watch that video and use this notebook concurrently.*"]}, {"cell_type": "markdown", "id": "c15f0c01", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Slides</h3>\n", "\n", "Run the code below to view the slides for this lesson, which are discussed in the videos. You can also open the slides in a separate window <a href=\"https://mitx-8s50.github.io/slides/L02/slides2.html\" target=\"_blank\">HERE</a>."]}, {"cell_type": "code", "execution_count": null, "id": "8173c533", "metadata": {"tags": ["learner", "py"]}, "outputs": [], "source": ["#>>>RUN: L2.6-slides\n", "\n", "from IPython.display import IFrame\n", "IFrame(src='https://mitx-8s50.github.io/slides/L02/slides2.html', width=975, height=550)"]}, {"cell_type": "markdown", "id": "db8fc145", "metadata": {"tags": ["learner", "md", "lect_06"]}, "source": ["<h3>Overview</h3>\n", "\n", "In the previous Lesson, we explained that the expectation is the mean of a distribution and the variance is a measure of the width. When we perform a measurement, we are just sampling from an unknown distribution, or worse yet, we are sampling from an unknown distribution and then distorting that distribution with some sort of effect. \n", "\n", "Let's say that you are sampling a distribution that is fundamentally a Gaussian. Now on top of this, we then distort this distribution by a function $f(x)$. This distortion will modify the distribution of the events, making it less Gaussian, or shrinking and stretching it. A distortion function can arise from many aspects of the measurement, such as from sending a particle through a magnetic field, or having light reflect off a mirror. There are countless examples of such distortive effects in physical measurements. If we happen to know $f(x)$ as well as the distribution of $x$, how does the shape of $x$ get distorted by $f(x)$?\n", "\n", "To answer this question, consider the case where the probability of the input distribution is $p(x)$. If we define $x^{\\prime}=f(x)$,  the probability to be in a small region $dx^\\prime$ of the modified coordinates is defined by\n", "$x^{\\prime}=f(x)$,  the probability to be in small region $dx^\\prime$ of the modified coordinates is defined by\n", "\n", "$$\n", "\\begin{eqnarray}\n", "p^{\\prime}(x^\\prime)dx^\\prime&=&f(p(x))dx^\\prime \\\\\n", "                             &=&f(p(x))\\frac{dx^\\prime}{dx}dx\\\\\n", "                             &=&f(p(x))\\frac{df}{dx}dx\\\\\n", "\\end{eqnarray}\n", "$$\n", "\n", "This follows from the fact that the spread of a function sampled from $p(x)$ would be modified by the spread of $f(x)$ defined as  $f(x+\\Delta x)-f(x)\\approx\\frac{df}{dx}\\Delta x$. \n", "\n", "As a simple example, in the case of $f(x)=x^{2}$ or $\\frac{df}{dx}=2x$. What that means is that $\\sigma_{f(x)}\\approx2x\\sigma_{x}$. Let's actually see that empirically.\n"]}, {"cell_type": "code", "execution_count": null, "id": "a893e204", "metadata": {"tags": ["learner", "py", "lect_06", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L2.6-runcell01\n", "\n", "#Now let's say we do a measurement, and the measurement takes an input variable that is varying, \n", "#and applies a function to it. What is the spread of the function\n", "ntries=1000\n", "meas = np.full(ntries,100) #The value 100, 1k times\n", "unc  = np.random.normal (0,1, ntries) #a randomly sampled value from a Gaussian with width 1 1k times\n", "meas = meas+unc # the value 100 now smeared with sigma=1\n", "\n", "def function(ix):#our function\n", "    return ix**2\n", "outmeas = function(meas)\n", "_,_,_=normhist(outmeas)\n", "\n", "print(\"Mean:\",outmeas.mean(),\"Stddeviation:\",outmeas.std())\n", "print(\"Predicted Mean:\",function(100),\"Stddeviation:\",2*100) #expect it to be 2*100*1\n"]}, {"cell_type": "markdown", "id": "5f486d95", "metadata": {"tags": ["learner", "md", "lect_06"]}, "source": ["Now, what about if we have two sources of uncertainty? This is a little bit different in the sense that these variations are independent of each other. Let's consider the very simple function $f(x)=x$. Now, let's say that $x$ can vary by a Gaussian distributed variable $\\sigma_1$ and a second Gaussian distributed variable $\\sigma_2$. If we consider these variations, we have that $f(x)$ will be modified by\n", "\n", "\n", "$$\n", "\\begin{equation}\n", " f(x) = x + \\sigma_1 + \\sigma_2\n", "\\end{equation}\n", "$$\n", "\n", "This will give us two Gaussians. If we look to see the variance of this distribution, we can treat these two fluctuations as two independent measurements, which means we can write. \n", "\n", "$$\n", "\\begin{eqnarray}\n", " V[f(x)] &=& V(x) + V(\\sigma_1) + V(\\sigma_2)\\\\\n", "                &=& \\sigma_1^2 + \\sigma_2^2 \n", "\\end{eqnarray}\n", "$$\n", "\n", "To visualize what is going on, we can imagine plotting these variations in a 2D plot. \n", "\n"]}, {"cell_type": "code", "execution_count": null, "id": "03d9f016", "metadata": {"tags": ["learner", "py", "lect_06", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L2.6-runcell02\n", "\n", "ntoys=10000\n", "err1=np.array([])\n", "err2=np.array([])\n", "for i0 in range(ntoys):\n", "    pToy = np.random.normal(0,1,2)\n", "    err1 = np.append(pToy[0],err1)\n", "    err2 = np.append(pToy[1],err2)\n", "angle = np.linspace( 0 , 2 * np.pi , 150 ) \n", "#correct circle\n", "radius = 1*np.sqrt(2)\n", "x = radius * np.cos( angle ) \n", "y = radius * np.sin( angle ) \n", "#too large circle\n", "radius = 1*2.0\n", "x2 = radius * np.cos( angle ) \n", "y2 = radius * np.sin( angle ) \n", "\n", "plt.rcParams['figure.figsize'] = (6,6)\n", "plt.plot(err1,err2,\"p\")\n", "plt.plot(x,y,c='r')\n", "plt.plot(x2,y2,c='r')\n", "plt.xlabel(\"$\\sigma_{1}$\")\n", "plt.ylabel(\"$\\sigma_{2}$\")\n", "plt.show()\n", "plt.rcParams['figure.figsize'] = (9,6)"]}, {"cell_type": "markdown", "id": "b6c0903a", "metadata": {"tags": ["learner", "md", "lect_06"]}, "source": ["Sampling two Gaussians gives us a circular distribution with a width given by the radius of the circle. This radius can be seen to be the standard deviation of $f(x)$ or the $\\sqrt{V[f(x)]}=\\sqrt{\\sigma_1^2+\\sigma_2^2}$. In other words, when sampling two independent variables, the variations add as their squares, as if they are two separate independent coordinates. This is often denoted as a \"Sum in Quadrature.\""]}, {"cell_type": "markdown", "id": "627f6f67", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='exercises_2_6'></a>     \n", "\n", "| [Top](#section_2_0) | [Restart Section](#section_2_6) | [Next Section](#section_2_7) |\n"]}, {"cell_type": "markdown", "id": "d0477604", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-2.6.1: Uncertainty in $f(x)$</span>\n", "\n", "If $f(x) = \\log(x)$, what is $\\sigma_{f(x)}$ in terms of $x$ and $\\sigma_{x}$? Express your answer in terms of `x` and `sigma_x` for $\\sigma$.\n"]}, {"cell_type": "markdown", "id": "dd412324", "metadata": {"tags": ["md", "learner"]}, "source": [">#### Follow-up 2.6.1a (ungraded)\n", ">  \n", ">Try computing this numerically and comparing to your analytic solution.\n"]}, {"cell_type": "code", "execution_count": null, "id": "895aea72", "metadata": {"tags": ["draft", "py"]}, "outputs": [], "source": ["#>>>EXERCISE: L2.6.1a\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "ntries=100000\n", "mean=100\n", "sigma=5\n", "meas = np.full(ntries,mean) #The value 100, 1k times\n", "unc  = np.random.normal (0,sigma, ntries) #a randomly sampled value from a Gaussian with width 1 1k times\n", "meas = meas+unc # the value 100 now smeared with sigma=1\n", "\n", "def function(ix):#our function\n", "    return np.log(ix)\n", "\n", "outmeas = function(meas)\n", "_,_,_=normhist(outmeas)\n", "\n", "analytic_stdev = 0. #YOUR CODE HERE\n", "\n", "print(\"Mean:\",outmeas.mean(),\"Stddeviation:\",outmeas.std())\n", "print(\"Predicted Mean:\",function(mean),\"Stddeviation:\",analytic_stdev)\n"]}, {"cell_type": "markdown", "id": "40b92fd5", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_2_7'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L2.7 Propagating Uncertainties</h2>     \n", "\n", "| [Top](#section_2_0) | [Previous Section](#section_2_6) | [Exercises](#exercises_2_7) |\n"]}, {"cell_type": "markdown", "id": "ac0ac346", "metadata": {"tags": ["8S50x", "learner"]}, "source": ["*The material in this section is discussed in the video **<a href=\"https://courses.mitxonline.mit.edu/learn/course/course-v1:MITxT+8.S50.1x+3T2024/block-v1:MITxT+8.S50.1x+3T2024+type@sequential+block@seq_LS2/block-v1:MITxT+8.S50.1x+3T2024+type@vertical+block@vert_LS2_vid7\" target=\"_blank\">HERE</a>.** You are encouraged to watch that video and use this notebook concurrently.*"]}, {"cell_type": "markdown", "id": "18427974", "metadata": {"tags": ["learner", "md", "lect_07"]}, "source": ["<h3>A realistic example</h3>\n", "\n", "Very famously, there was an excess of events found at a certain point in a distribution measured by an experiment at the Tevatron collider in Fermilab. This excess caused a lot of excitement. However many people were skeptical. <a href=\"https://www.science20.com/quantum_diaries_survivor/no_jetjet_bump_new_cdf_diboson_analysis-123327\" target=\"_blank\">HERE</a> is a full description of what was going on. In brief, the two plots shown below summarize the evidence that people thought showed a bump indicating the existence of a new particle:\n", "\n", "<img alt=\"Fig 2.7.1: excess of events at the Tevatron collider\" src=\"http://www.pd.infn.it/~dorigo/wjjcdf73fb.jpg\" width=\"700\"/>\n", "\n", ">source:  https://arxiv.org/pdf/1104.0699.pdf<br>\n", ">attribution: CDF Collaboration, arXiv:1104.0699v2\n", "\n", "The plots show the number of events as a function of the so-called \"invariant mass\" of a pairs of jets. The term \"jets\" refers to a cluster of particles all emitted in very close to the same direction. When two such jets are observed, one can calculate the mass of a hypothetical very short lifetime particle which could have decayed to produce the two observed jets. This mass needs to be corrected for relativistic effects to determine its value when that particle was at rest, hence the term \"invariant mass\".\n", "\n", "In both plots, the black points represent a histogram of the data. The filled in areas in the left plot are a histogram summing up the simulations of all the other physics processes that we know are occurring. The different fill colors represent each individual prediction. Finally, on the right, we subtract the solid distribution from the data. However, notice that the \"WW+WZ\" process shown in red in the left plot has not been subtracted. That contribution is shown as the red histogram in the right plot. The blue histogram on the right plot is a Gaussian fit to what appears to be a deviation in the data compared to the sum of all expected physics processes. \n", "\n", "The fact that this deviation corresponds to a bump makes us think this is a new particle. The problem with this bump is that it's a bump on top of a steeply falling distribution. You need to look very closely at the left plot to see this same blue \"bump\" contribution. So, what would happen to this comparison if our predicted distribution was shifted to the right by a little bit. What would the effect be on the appearance of the bump?\n", "\n", "To see this, let's open a file with this data and try to shift it ourselves. Note that the following code subtracts *all* of the expected physics processes so that the second plot below does not show the red bump seen in the right plot above."]}, {"cell_type": "code", "execution_count": null, "id": "6628f11e", "metadata": {"tags": ["learner", "py", "lect_07", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L2.7-runcell01\n", "\n", "import csv\n", "import matplotlib.pyplot as plt\n", "import numpy as np\n", "import urllib.request\n", "\n", "#load the file\n", "def load(iName):\n", "    label=iName\n", "    datax=np.array([])\n", "    datay=np.array([])\n", "    datayerr=np.array([])\n", "    with open(label,'r') as csvfile:\n", "        plots = csv.reader(csvfile, delimiter=',')\n", "        for row in plots:\n", "            datax    = np.append(datax,float(row[0]))\n", "            datay    = np.append(datay,float(row[1]))\n", "            datayerr = np.append(datayerr,np.sqrt(float(row[1])))\n", "    return datax,datay,datayerr\n", "\n", "#compute the ratio between data and simulation\n", "def histratio(iydata,iyderr,iysim):\n", "    newydata=np.array([])\n", "    newyderr=np.array([])\n", "    for i0 in range(len(iysim)):\n", "        ynew=iydata[i0]/iysim[i0]\n", "        yner=iyderr[i0]/iysim[i0]\n", "        newydata=np.append(newydata,ynew)\n", "        newyderr=np.append(newyderr,yner)\n", "    return newydata,newyderr\n", "\n", "fig = plt.figure(figsize=(10.5, 9.5))\n", "ax = fig.add_subplot(2,1,1)\n", "datax,datay,datayerr=load(\"data/L02/tmpdata.txt\")\n", "simx,simy,simyerr=load(\"data/L02/tmpmc.txt\")\n", "plt.errorbar(datax,datay,yerr=datayerr,marker='.',c='black',linestyle = 'None')\n", "plt.plot    (datax,simy,drawstyle = 'steps-mid')\n", "ax = fig.add_subplot(2,1,2)\n", "yrdata,yrderr=histratio(datay,datayerr,simy)\n", "ax.errorbar(datax,yrdata,yerr=yrderr,marker='.',c='black',linestyle = 'None')\n", "ax.axhline(1, c='red')\n", "ax.set_ylim(0.5,1.5)\n", "plt.xlabel(\"Mjj [GeV]\")\n", "plt.ylabel(\"Data/Simulation\")\n", "plt.show()\n"]}, {"cell_type": "markdown", "id": "191c9137", "metadata": {"tags": ["learner", "md", "lect_07"]}, "source": ["We can define a shift in a histogram by just shuffling events in bins. This we do by \n", "\n", "\n", "$$\n", "\\begin{align}\n", "f(x^{\\prime}) & =f(x+\\sigma)\\approx f(x)+\\frac{df}{dx}\\sigma\\\\\n", " & =f(x)+\\frac{f(x+\\Delta x)-f(x)}{\\Delta x}\\sigma\\\\\n", " & =f(x)\\left(1-\\frac{\\sigma}{\\Delta x}\\right)+f(x+\\Delta x)\\frac{\\sigma}{\\Delta x}\n", "\\end{align}\n", "$$\n", "\n", "which we can rewrite in terms of bin shifts with a fractional uncertainty of $\\textrm{f}=\\frac{\\sigma}{\\Delta x}$. This gives us\n", "\n", "\n", "$$\n", "\\begin{equation}\n", "\\rm{bin_{i}} = (1-f)\\rm{bin}_{i} + (f) \\rm{bin}_{i-1} \\\\\n", "f(x_{i}) = f(x_{i})(1-f)+f(x-\\Delta x)f\n", "\\end{equation}\n", "$$\n", "\n", "Such a shift could occur if there was an additional uncertainty in the invariant mass which had not been accounted for. Let's add this modification and see if a fractional shift can explain our deviation. "]}, {"cell_type": "code", "execution_count": null, "id": "589ad4b2", "metadata": {"tags": ["learner", "py", "lect_07", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L2.7-runcell02\n", "\n", "#Now let's shift the bins of the simulation by a fraction\n", "def shifthist(ixunc,isimy):\n", "    newsimy=np.array([])\n", "    for i0 in range(len(isimy)):\n", "        ynew = isimy[i0]*(1-ixunc)\n", "        if i0 > 1:\n", "            ynew = isimy[i0-1]*ixunc + isimy[i0]*(1-ixunc)\n", "        newsimy=np.append(newsimy,ynew)\n", "    return newsimy\n", "\n", "            \n", "fig = plt.figure(figsize=(10.5, 9.5))\n", "ax = fig.add_subplot(2,1,1)\n", "newsimy=shifthist(0.5,simy)\n", "plt.errorbar(datax,datay,yerr=datayerr,marker='.',c='black',linestyle = 'None')\n", "plt.plot    (datax,simy,drawstyle = 'steps-mid')\n", "plt.plot    (datax,newsimy,drawstyle = 'steps-mid')\n", "\n", "ax = fig.add_subplot(2,1,2)\n", "yrdata,yrderr=histratio(datay,datayerr,newsimy)\n", "ax.errorbar(datax,yrdata,yerr=yrderr,marker='.',c='black',linestyle = 'None')\n", "ax.axhline(1, c='red')\n", "ax.set_ylim(0.5,1.5)\n", "plt.xlabel(\"Mjj [GeV]\")\n", "plt.ylabel(\"Data/Simulation\")\n", "plt.show()\n"]}, {"cell_type": "markdown", "id": "32d1a831", "metadata": {"tags": ["learner", "md", "lect_07"]}, "source": ["We can see that a fractional shift of merely half the bin size is sufficient to explain this effect. Do you believe that the bump is real? \n", "\n", "As an aside, think about how unintuitive it is that simply shifting the prediction slightly in the horizontal axis results in what looks like a very clear peak when comparing to the data. This is a good example of how subtle effects can often produce what looks otherwise like a very obvious discovery.\n", "\n", "If you are a big proponent of the existence of a peak, you could still see some evidence for a small excess in the data around 150. However, given that the vertical bars represent the independent statistical uncertainties, think about the probability that a few points will together deviate upward simply by chance."]}, {"cell_type": "markdown", "id": "d4f4dd2a", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='exercises_2_7'></a>   \n", "\n", "| [Top](#section_2_0) | [Restart Section](#section_2_7) |\n"]}, {"cell_type": "markdown", "id": "b7fa6ba3", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": [">#### Follow-up 2.7.1a (ungraded)\n", ">    \n", ">Examine the CDF data further. Try other fractional shifts. Does the resulting fit look better or worse (look at the residuals)?\n"]}], "metadata": {"celltoolbar": "Tags", "kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}