{"cells": [{"cell_type": "markdown", "id": "0378bae4", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<hr style=\"height: 1px;\">\n", "<i>This notebook was authored by the 8.S50x Course Team, Copyright 2022 MIT All Rights Reserved.</i>\n", "<hr style=\"height: 1px;\">\n", "<br>\n", "\n", "<h1>Lesson 4: Introduction to Data Fitting</h1>\n"]}, {"cell_type": "markdown", "id": "cebc864d", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_4_0'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L4.0 Overview</h2>\n"]}, {"cell_type": "markdown", "id": "fea35b2c", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<h3>Navigation</h3>\n", "\n", "<table style=\"width:100%\">\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_4_1\">L4.1 An Example: Hubble Constant</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_4_1\">L4.1 Exercises</a></td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_4_2\">L4.2 Derivation of Linear Regression</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_4_2\">L4.2 Exercises</a></td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_4_3\">L4.3 Linear Regression: Coding Example</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_4_3\">L4.3 Exercises</a></td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_4_4\">L4.4 Weighted Linear Regression</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_4_4\">L4.4 Exercises</a></td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_4_5\">L4.5 Minimization without the Math</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_4_5\">L4.5 Exercises</a></td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_4_6\">L4.6 Gradient Descent</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_4_6\">L4.6 Exercises</a></td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_4_7\">L4.7 Fitting the Full Range of Hubble Data</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_4_7\">L4.7 Exercises</a></td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_4_8\">L4.8 Fitting with lmfit</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_4_8\">L4.8 Exercises</a></td>\n", "    </tr>\n", "</table>\n", "\n"]}, {"cell_type": "markdown", "id": "f66c60bb", "metadata": {"tags": ["learner", "catsoop_00", "md"]}, "source": ["<h3>Learning Objectives</h3>\n", "\n", "In this Lesson, we will explore the following objectives:\n", "\n", "- Linear Regression\n", "- Weighted Linear Regression\n", "- Minimizing without all of the math\n", "- Gradient Descent: Actually understanding how we minimize numerically\n", "- Optimized descent: The Newton Step\n", "\n", "In the last Lesson, we introduced gravitational waves. Soon, you'll work on a project looking at real signals from LIGO. As part of that, you will need to fit a model to the data. \n", "\n", "What's fitting? Suppose you have a mathematical model that represents some physics phenomenon. It will have *parameters* that can be varies to adjust the output of the model. For example, the masses of black holes in a merger affect the waveform we expect from the resulting gravitational waves. Fitting is the process of coming up with parameter values that make your model output \"best match\" your observed data.\n", "\n", "To quantify \"best match\" while fitting, we come up with a metric to measure how far the model output is from the data. Then, we work to **minimize** this error metric (which depending on context might be called \"loss function\" to minimize, \"objective function\" to maximize, etc.; I will use them interchangeably).\n", "\n", "This minimization can be done analytically, which is a good exercise. That will be the first part of this lesson. However, for most complex data science projects, minimization is done numerically using computers. How this works will be discussed in the second part of the Lesson. Throughout, we'll be fitting some data to estimate the Hubble Constant, an important value in cosmology."]}, {"cell_type": "markdown", "id": "3857dff2", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Importing Data (Colab Only)</h3>\n", "\n", "If you are in a Google Colab environment, run the cell below to import the data for this notebook. Otherwise, if you have downloaded the course repository, you do not have to run the cell below.\n", "\n", "See the source and attribution information below:\n", "\n", "<pre>\n", "data: data/L04/sn_z_mu_dmu_plow_union2.1.txt\n", "source: http://supernova.lbl.gov/Union/, https://arxiv.org/abs/1105.3470 \n", "attribution: The Supernova Cosmology Project, arXiv:1105.3470v1 \n", "license type: https://arxiv.org/licenses/nonexclusive-distrib/1.0/license.html\n", "</pre>"]}, {"cell_type": "code", "execution_count": null, "id": "d25fc1df", "metadata": {"tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L4.0-runcell01\n", "\n", "#importing data from git repository\n", "\n", "!git init\n", "!git remote add -f origin https://github.com/mitx-8s50/nb_LEARNER/\n", "!git config core.sparseCheckout true\n", "!echo 'data/L04' >> .git/info/sparse-checkout\n", "!git pull origin main"]}, {"cell_type": "markdown", "id": "1b28388b", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Importing Libraries</h3>\n", "\n", "Before beginning, run the cell below to import the relevant libraries for this notebook.\n"]}, {"cell_type": "code", "execution_count": null, "id": "eae588a6", "metadata": {"tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L4.0-runcell02\n", "\n", "!pip install lmfit"]}, {"cell_type": "code", "execution_count": null, "id": "ca7976b7", "metadata": {"tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L4.0-runcell03\n", "\n", "#import packages\n", "import math as math                   #https://docs.python.org/3/library/math.html\n", "import matplotlib.pyplot as plt       #https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.html\n", "import numpy as np                    #https://numpy.org/doc/stable/\n", "import csv                            #https://docs.python.org/3/library/csv.html \n", "from scipy import stats               #https://docs.scipy.org/doc/scipy/reference/stats.html\n", "from scipy.optimize import curve_fit  #https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.curve_fit.html\n", "import scipy.linalg as la             #https://docs.scipy.org/doc/scipy/reference/linalg.html\n", "import lmfit                          #https://lmfit.github.io/lmfit-py/ "]}, {"cell_type": "markdown", "id": "3370205e", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Setting Default Figure Parameters</h3>\n", "\n", "The following code cell sets default values for figure parameters."]}, {"cell_type": "code", "execution_count": null, "id": "600cbada", "metadata": {"tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L4.0-runcell04\n", "\n", "#set plot resolution\n", "%config InlineBackend.figure_format = 'retina'\n", "\n", "#set default figure parameters\n", "plt.rcParams['figure.figsize'] = (9,6)\n", "\n", "medium_size = 12\n", "large_size = 15\n", "\n", "plt.rc('font', size=medium_size)          # default text sizes\n", "plt.rc('xtick', labelsize=medium_size)    # xtick labels\n", "plt.rc('ytick', labelsize=medium_size)    # ytick labels\n", "plt.rc('legend', fontsize=medium_size)    # legend\n", "plt.rc('axes', titlesize=large_size)      # axes title\n", "plt.rc('axes', labelsize=large_size)      # x and y labels\n", "plt.rc('figure', titlesize=large_size)    # figure title"]}, {"cell_type": "markdown", "id": "f3e315e9", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_4_1'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L4.1 An Example: Hubble Constant</h2>  \n", "\n", "| [Top](#section_4_0) | [Previous Section](#section_4_0) | [Exercises](#exercises_4_1) | [Next Section](#section_4_2) |\n"]}, {"cell_type": "markdown", "id": "60f2ee2f", "metadata": {"tags": ["learner", "8S50x", "md"]}, "source": ["*The material in this section is discussed in the video **<a href=\"https://courses.mitxonline.mit.edu/learn/course/course-v1:MITxT+8.S50.1x+3T2022/block-v1:MITxT+8.S50.1x+3T2022+type@sequential+block@seq_LS4/block-v1:MITxT+8.S50.1x+3T2022+type@vertical+block@vert_LS4_vid1\" target=\"_blank\">HERE</a>.** You are encouraged to watch that video and use this notebook concurrently.*"]}, {"cell_type": "markdown", "id": "d16f92b9", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Slides</h3>\n", "\n", "Run the code below to view the slides for this section, which are discussed in the related video. You can also open the slides in a separate window <a href=\"https://mitx-8s50.github.io/slides/L04/slides_L04_01.html\" target=\"_blank\">HERE</a>."]}, {"cell_type": "code", "execution_count": null, "id": "50b994dd", "metadata": {"tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L4.1-slides\n", "\n", "from IPython.display import IFrame\n", "IFrame(src='https://mitx-8s50.github.io/slides/L04/slides_L04_01.html', width=975, height=550)"]}, {"cell_type": "markdown", "id": "006eb8b5", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='exercises_4_1'></a>     \n", "\n", "| [Top](#section_4_0) | [Restart Section](#section_4_1) | [Next Section](#section_4_2) |\n"]}, {"cell_type": "markdown", "id": "b0687430", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-4.1.1 Expansion and Recessional Velocity</span>\n", "\n", "True or False: If the universe is expanding, the apparent recessional velocity of galaxies further away will be larger.\n"]}, {"cell_type": "markdown", "id": "7beb47e6", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_4_2'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L4.2 Derivation of Linear Regression</h2>  \n", "\n", "| [Top](#section_4_0) | [Previous Section](#section_4_1) | [Exercises](#exercises_4_2) | [Next Section](#section_4_3) |\n"]}, {"cell_type": "markdown", "id": "b0a4dd75", "metadata": {"tags": ["learner", "8S50x", "md"]}, "source": ["*The material in this section is discussed in the video **<a href=\"https://courses.mitxonline.mit.edu/learn/course/course-v1:MITxT+8.S50.1x+3T2022/block-v1:MITxT+8.S50.1x+3T2022+type@sequential+block@seq_LS4/block-v1:MITxT+8.S50.1x+3T2022+type@vertical+block@vert_LS4_vid2\" target=\"_blank\">HERE</a>.** You are encouraged to watch that video and use this notebook concurrently.*"]}, {"cell_type": "markdown", "id": "9d496fe7", "metadata": {"tags": ["learner", "md", "lect_02"]}, "source": ["<h3>Supernova Data</h3>\n", "\n", "For this class, we will use public data provided by the supernova galactic survey. This gives a list of supernovae along with their observed distances (with uncertainties) and their redshifts. These data are obtained through galactic observations of supernovae, with the line shifts indicating their respective redshift.\n", "\n", "**Data Source**\n", "\n", "We have included the data as a `.txt` file in the git repository. If using Colab, you will have downloaded the data in the initial steps of this Lesson.\n", "\n", "The data are from https://supernova.lbl.gov/Union/. We have used the following file under \"Cosmology Tables:\"\n", "\n", "- Union2.1 Compilation Magnitude vs. Redshift Table.\n", "\n", "This table stores the name, redshift, distance modulus, and distance modulus error for a collection of supernovae. Feel free to also look around the rest of the website and see what people have done with the data!\n", "\n", "\n", "**About the Data**\n", "\n", "When loading the data, we need to know that the data is stored in terms of the distance modulus, $\\mu$ defined <a href=\"https://en.wikipedia.org/wiki/Distance_modulus\" target=\"_blank\">here</a>. We can write this as \n", "\n", "$$\n", "\\begin{equation}\n", "d=10^{\\frac{\\mu}{5} + 1}\n", "\\end{equation}\n", "$$\n", "\n", "Furthermore, the uncertainty $\\sigma_{\\mu}$ is also given in terms of the distance modulus. Given our relationship between the distance modulus and the distance, we can get the uncertainty in distance:\n", "\n", "$$\n", "\\begin{eqnarray}\n", "\\sigma_{d} & = &  \\frac{d}{d\\mu}d(\\mu) \\cdot \\sigma_{\\mu} \\\\\n", "           & = &  \\frac{\\log(10)}{5} 10^{\\frac{\\mu}{5} + 1}   \\sigma_{\\mu}\n", "\\end{eqnarray}\n", "$$\n", "\n", "With that in mind, let's go ahead and process the data. "]}, {"cell_type": "code", "execution_count": null, "id": "7468f053", "metadata": {"tags": ["learner", "py", "lect_02", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L4.2-runcell01\n", "\n", "import math as math\n", "import matplotlib.pyplot as plt\n", "import numpy as np\n", "import csv\n", "\n", "#Today we are going to start with astro data from here : \n", "#http://supernova.lbl.gov/Union/\n", "#Let's load the data\n", "\n", "label='data/L04/sn_z_mu_dmu_plow_union2.1.txt'\n", "\n", "\n", "#Table stores name, redshift, distance modulus, distance modulus error\n", "#Let's convert from distance modulus to distance\n", "#See here https://en.wikipedia.org/wiki/Distance_modulus\n", "def distanceconv(iMu):\n", "    power=iMu/5+1\n", "    return 10**power\n", "\n", "# and the same for uncertainty in distance modulus to uncertainty in distance\n", "def distanceconverr(iMu,iMuErr):\n", "    power=iMu/5+1\n", "    const=math.log(10)/5.\n", "    return const*(10**power)*iMuErr\n", "\n", "redshift=[]\n", "distance=[]\n", "distance_err=[]\n", "# read in the data\n", "with open(label,'r') as csvfile:\n", "    plots = csv.reader(csvfile, delimiter='\\t')\n", "    # name, redshift, distance modulus, distance modulus uncertainty\n", "    for row in plots:\n", "        redshift.append(float(row[1]))\n", "        distance.append(distanceconv(float(row[2])))\n", "        distance_err.append(distanceconverr(float(row[2]),float(row[3])))\n", "\n", "plt.xlabel('redshift(z)', fontsize=15) #Label x\n", "plt.ylabel('distances(parsec)', fontsize=15)#Label y\n", "# plot with errorbars\n", "plt.errorbar(redshift,distance,yerr=distance_err,marker='.',linestyle = 'None', color = 'black')\n", "plt.show()"]}, {"cell_type": "markdown", "id": "5f39e821", "metadata": {"tags": ["learner", "md", "lect_02"]}, "source": ["This plot has an interesting shape. Instead of looking at the whole plot, we can zoom in on a specific region, namely the area with small redshifts: $z \\lt 0.1$"]}, {"cell_type": "code", "execution_count": null, "id": "5b8265b8", "metadata": {"tags": ["learner", "py", "lect_02", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L4.2-runcell02\n", "\n", "#Now let's zoom in on the small redshift data.\n", "def load(iLabel,iZMax):\n", "    # loads data from file iLabel with redshift below iZMax:\n", "    redshift=[]\n", "    distance=[]\n", "    distance_err=[]\n", "    with open(label,'r') as csvfile:\n", "        plots = csv.reader(csvfile, delimiter='\\t')\n", "        for row in plots:\n", "            if float(row[1]) > iZMax:\n", "                continue\n", "            redshift.append(float(row[1]))\n", "            distance.append(distanceconv(float(row[2])))\n", "            distance_err.append(distanceconverr(float(row[2]),float(row[3])))\n", "    return np.array(redshift), np.array(distance), np.array(distance_err)\n", "\n", "redshift,distance,distance_err = load(label,0.1)\n", "plt.xlabel('redshift(z)', fontsize=15) #Label x\n", "plt.ylabel('distances(parsec)', fontsize=15)#Label y\n", "plt.errorbar(redshift,distance,yerr=distance_err,marker='.',linestyle = 'None', color = 'black')\n", "plt.show()"]}, {"cell_type": "markdown", "id": "a1a17f3c", "metadata": {"tags": ["learner", "md", "lect_02"]}, "source": ["<h3>Linear (Least-Squares) Regression</h3>\n", "\n", "The data above has what looks like a linear trend. So, we try fitting a linear model; that is, something of form\n", "\n", "$$\n", "\\begin{equation}\n", "y = Ax + b\n", "\\end{equation}\n", "$$\n", "\n", "In this case, $x$ is the redshift observed, and $y$ is the distance observed. The goal here is to extract the parameters $A$ and $b$ that make the model's output prediction match those of the data. We can do this analytically, with a somewhat involved derivation.\n", "\n", "We start by defining $\\hat{y}$, which is the model's prediction of $y$ based on $x$. Our data is a set of $x$ and $y$ values that we'll call $x_i$ and $y_i$. For each $x_i$, we'll get the corresponding prediction $\\hat{y}_i$ from the model:\n", "\n", "$$\n", "\\begin{equation}\n", "\\hat{y}_{i} = Ax_{i} + b\n", "\\end{equation}\n", "$$\n", "\n", "where now we vary $A$ and $b$ to make the model fit. As mentioned earlier, we need to define an objective\u2014some function to measure error\u2014then find parameters to minimize it.\n", "\n", "What do we use for error? There are many possibilities. A common one is the square error: we take the difference between each predicted distance $\\hat{y}_i$ and the corresponding observed distance $y_i$, and square it. Then we add those up over all our data points. We'll call this metric $Q$:\n", "\n", "$$\n", "\\begin{eqnarray} \n", "Q & = & \\sum_{i=1}^{N}\\left(y_{i}-\\hat{y}_{i}\\right)^2 \\\\\n", "Q & = & \\sum_{i=1}^{N}\\left(y_{i}-Ax_{i}-b\\right)^2 \\\\\n", "\\end{eqnarray}\n", "$$\n", "\n", "where for the second line we've replaced $\\hat{y}$ with the parameterization in our model. By the way, minimizing this particular type of $Q$ is what we call \"least-squares\" regression.\n", "\n", "Now, the minimization step. We can take partial derivatives with respect to $A$ and to $b$ and set each to zero, to find extrema.\n", "\n", "$$\n", "\\begin{eqnarray} \n", "\\frac{\\partial Q}{\\partial A} & = & \\sum_{i=1}^{N} -2 x_{i} \\left(y_{i}-Ax_{i}-b\\right) \\\\\n", "              & = & \\sum_{i=1}^{N} -2 \\left(x_{i} y_{i}-Ax^{2}_{i}-b x_{i}\\right) \\\\\n", "              & = & 0\n", "\\end{eqnarray}          \n", "$$\n", "\n", "and for $b$ we have, noting that the average value of $x$ is $\\bar{x}=\\frac{1}{N} \\sum_{i=1}^{N} x_{i}$ (and similar for $\\bar{y}$), gives us\n", "\n", "$$\n", "\\begin{eqnarray} \n", "\\frac{\\partial Q}{\\partial b} & = & \\sum_{i=1}^{N} -2  \\left(y_{i}-Ax_{i}-b\\right) \\\\\n", "              & = & 2Nb + 2A\\sum_{i=1}^{N}x_{i}-2\\sum_{i=1}^{N}y_{i} \\\\\n", "              & = & 0 \\\\\n", "{\\rm Rearranging,} \\\\\n", "           b  & = & \\frac{1}{N} \\sum_{i=1}^{N}y_{i} - \\frac{A}{N} \\sum_{i=1}^{N}x_{i} \\\\\n", "              & = & \\bar{y} - A\\bar{x}      \n", "\\end{eqnarray}          \n", "$$\n", "\n", "Since we have the data, we can easily calculate $\\bar{x}$ and $\\bar{y}$ and we have derived a formula for the optimal $b$ in terms of these two averages. (*It's good to keep in mind that the optimal $A$ and $b$ that we find here are only optimal in the sense that they minimize our choice of function $Q$, giving the least-squares fit. If we had a different measure of error, our results might be different.*)\n", "\n", "\n", "\n", "Now, we can go back and solve for $A$, by substituting in our optimal $b$. This gives us\n", "\n", "$$\n", "\\begin{eqnarray} \n", "\\frac{dQ}{dA} & = & \\sum_{i=1}^{N} -2 \\left(x_{i} y_{i}-Ax^{2}_{i}-\\left(\\bar{y} - A\\bar{x}\\right) x_{i}\\right) \\\\\n", "              & = & \\sum_{i=1}^{N} -2 \\left(x_{i} y_{i}-x_{i}\\bar{y}-Ax^{2}_{i}+ A\\bar{x}x_{i}\\right) \\\\                           & = & -2 \\sum_{i=1}^{N} x_{i}\\left( y_{i}-\\bar{y}\\right)+2A\\sum_{i=1}^{N} x_{i}\\left(x_{i}-\\bar{x}\\right) \\\\\n", "              & = & 0 \\\\\n", "{\\rm Solving} \\\\\n", "A & = & \\frac{\\sum_{i=1}^{N} x_{i}\\left( y_{i}-\\bar{y}\\right)}{\\sum_{i=1}^{N} x_{i}\\left(x_{i}-\\bar{x}\\right)} \\\\\n", "  & = & \\frac{\\sum_{i=1}^{N} x_{i}\\left( y_{i}-\\bar{y}\\right) +\\sum_{i=1}^{N}\\left(\\bar{x}\\bar{y}-y_{i}\\bar{x}\\right)}      \n", "  {\\sum_{i=1}^{N} x_{i}\\left(x_{i}-\\bar{x}\\right) + \\sum_{i=1}^{N}\\left(\\bar{x}^2-x_{i}\\bar{x}\\right)} \\\\\n", "  & = & \\frac{\\sum_{i=1}^{N} \\left( x_{i} y_{i}-x_{i}\\bar{y}+\\bar{x}\\bar{y}-y_{i}\\bar{x}\\right)}      \n", "  {\\sum_{i=1}^{N} \\left(x^2_{i}-x_{i}\\bar{x} + \\bar{x}^2-x_{i}\\bar{x}\\right)} \\\\\n", "  & = & \\frac{\\frac{1}{N}\\sum_{i=1}^{N} \\left(x_{i} - \\bar{x} \\right) \\left(y_{i}-\\bar{y}\\right)}      \n", "             {\\frac{1}{N}\\sum_{i=1}^{N} \\left(x_{i}-\\bar{x}\\right)^2}\\\\\n", "\\end{eqnarray} \n", "$$\n", "\n", "Ok, that's a lot of math, but in the end we get an explicit formula for the optimal $A$. To make things look cleaner, we can break the above into a few functions. We define the covariance and variance as: \n", "\n", "$$\n", "\\begin{eqnarray} \n", "\\rm{VAR(x)}   & = & \\frac{1}{N}\\sum_{i=1}^{N} \\left(x_{i}-\\bar{x}\\right)^2 \\\\\n", "\\rm{COV(x,y)} & = & \\frac{1}{N}\\sum_{i=1}^{N} \\left(x_{i} - \\bar{x} \\right) \\left(y_{i}-\\bar{y}\\right) \\\\\n", "            A & = & \\frac{\\rm{COV(x,y)}}{\\rm{VAR(x)}}\n", "\\end{eqnarray}\n", "$$\n", "\n", "We'll talk about the covariance later. The variance is just the variance we defined on the first Lesson, but now in discrete form. Alright, now let's code this guy up!"]}, {"cell_type": "markdown", "id": "5eee7b1b", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='exercises_4_2'></a>     \n", "\n", "| [Top](#section_4_0) | [Restart Section](#section_4_2) | [Next Section](#section_4_3) |\n"]}, {"cell_type": "markdown", "id": "250b2631", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-4.2.1 Complete the Derivation</span>\n", "\n", "In the derivation shown previously, how did we get the strange formulation in that second step? Part of the answer is that we could add a term $\\sum_{i} x_{i}\\bar{x} - \\bar{x}^2$ to the denominator because $\\sum_{i} x_{i}\\bar{x} - \\bar{x}^2 = 0$. This helped simplify the derivation. Below, we show a partial proof that $\\sum_{i} x_{i}\\bar{x} - \\bar{x}^2 = 0$:\n", "\n", "<br>\n", "\n", "$$\n", "\\begin{eqnarray} \n", "\\sum_{i} (x_{i}\\bar{x}-\\bar{x}^{2}) & = & \\sum_{i} x_{i}\\bar{x} -\\sum_{i} \\bar{x}^{2} \\\\\n", "& = & [\\mathrm{insert\\,missing\\,step}]\\\\\n", "& = & 0 \\\\\n", "\\end{eqnarray}\n", "$$  \n", "\n", "<br>\n", "\n", "In order for this to be true, what must the value of $\\sum_{i} x_{i}\\bar{x}$ be? Express your answer in terms of `N` for $N$ and `xbar` for $\\bar{x}$.\n"]}, {"cell_type": "markdown", "id": "13c0ed13", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_4_3'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L4.3 Linear Regression: Coding Example</h2>  \n", "\n", "| [Top](#section_4_0) | [Previous Section](#section_4_2) | [Exercises](#exercises_4_3) | [Next Section](#section_4_4) |\n"]}, {"cell_type": "markdown", "id": "1dee72bc", "metadata": {"tags": ["learner", "8S50x", "md"]}, "source": ["*The material in this section is discussed in the video **<a href=\"https://courses.mitxonline.mit.edu/learn/course/course-v1:MITxT+8.S50.1x+3T2022/block-v1:MITxT+8.S50.1x+3T2022+type@sequential+block@seq_LS4/block-v1:MITxT+8.S50.1x+3T2022+type@vertical+block@vert_LS4_vid3\" target=\"_blank\">HERE</a>.** You are encouraged to watch that video and use this notebook concurrently.*"]}, {"cell_type": "markdown", "id": "1c1835f2", "metadata": {"tags": ["learner", "md", "lect_03"]}, "source": ["<h3>Overview</h3>\n", "\n", "\n", "Now, it's time to run the linear regression we mathematically derived in the section above! Below is an implementation of regression based on our calculation, and the units are explained following this implementation.\n"]}, {"cell_type": "code", "execution_count": null, "id": "9a66e14d", "metadata": {"tags": ["learner", "py", "lect_03", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L4.3-runcell01\n", "\n", "#Let's run the regression again\n", "def variance(isamples):\n", "    mean=isamples.mean()\n", "    n=len(isamples)\n", "    tot=0\n", "    for pVal in isamples:\n", "        tot+=(pVal-mean)**2\n", "    return tot/n\n", "\n", "def covariance(ixs,iys):\n", "    meanx=ixs.mean()\n", "    meany=iys.mean()\n", "    n=len(ixs)\n", "    tot=0\n", "    for i0 in range(len(ixs)):\n", "        tot+=(ixs[i0]-meanx)*(iys[i0]-meany)\n", "    return tot/n\n", "\n", "def linear(ix,ia,ib):\n", "    return ia*ix+ib\n", "\n", "def regress(redshift,distance):\n", "    #Let's regress\n", "    var=variance(redshift)\n", "    cov=covariance(redshift,distance)\n", "    A=cov/var\n", "    b=distance.mean()-A*redshift.mean()\n", "    #Done!\n", "    return A,b\n", "\n", "def plotAll(redshift,distance,distance_err,A,b):\n", "    #now let's plot it\n", "    xvals = np.linspace(0,0.1,100)\n", "    yvals = []\n", "    for pX in xvals:\n", "        yvals.append(linear(pX,A,b))\n", "\n", "    #Plot the line\n", "    plt.plot(xvals,yvals)\n", "    plt.errorbar(redshift,distance,yerr=distance_err,marker='.',linestyle = 'None', color = 'black')\n", "    plt.xlabel('redshift(z)', fontsize=15) #Label x\n", "    plt.ylabel('distances(parsec)', fontsize=15)#Label y\n", "    plt.show()\n", "    #Print it out\n", "    print(\"Hubbles Constant:\",1e6*3e5/A,\"intercept\",b)#Note 1e6 is from pc to Mpc and 3e5 is c in km/s\n", "\n", "A,b=regress(redshift,distance)\n", "plotAll(redshift,distance,distance_err,A,b)"]}, {"cell_type": "markdown", "id": "bac80961", "metadata": {"tags": ["learner", "md", "lect_03"]}, "source": ["From the data we're fitting, we can figure out the units of the slope $A$ :\n", "\n", "$$A = \\frac{\\rm distance(pc) }{\\rm  Redshift~(z)}$$\n", "\n", "where the redshift $z$ is the fractional (dimensionless) change in wavelength. Hubble's constant is defined as\n", "\n", "$$h_{0} = \\frac{\\rm Recession\\, speed~(km/s)}{\\rm distance(Mpc)}$$ \n", "\n", "For nonrelativistic speeds, the magnitude of redshift roughly corresponds to the recession speed by $z\\approx v/c$. So, we do the unit conversions to obtain\n", "\n", "$$h_{0} = \\frac{1}{A}\\frac{\\rm c~(km/s)}{10^{-6}}=\\frac{1}{A}\\frac{\\rm 3\\times10^{5}~(km/s)}{10^{-6}} $$"]}, {"cell_type": "markdown", "id": "e96d1703", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='exercises_4_3'></a>     \n", "\n", "| [Top](#section_4_0) | [Restart Section](#section_4_3) | [Next Section](#section_4_4) |\n"]}, {"cell_type": "markdown", "id": "4f5b1717", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-4.3.1 Uncertainty in $h_{0}$</span>\n", "\n", "Now we wish to obtain the uncertainty on $h_{0}$, which we call $\\sigma_{h_0}$, where we will use the definition:\n", "\n", "$$h_{0} =\\frac{1}{A}\\frac{\\rm 3\\times10^{5}~(km/s)}{10^{-6}} $$\n", "\n", "\n", "Assume that we know the uncertainty on $A$, which we call $\\sigma_{A}$. What is the value of $\\sigma_{h_0}$? Express your answer in terms of `A` for $A$, `h_0` for $h_{0}$, and `sigma_A` for $\\sigma_{A}$.\n", "\n", "**Hint: To answer this question, you need to use propagation of uncertainty!**"]}, {"cell_type": "markdown", "id": "297aab3b", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": [">#### Follow-up 4.3.1a (ungraded)\n", ">\n", ">What is the uncertainty $\\sigma_{h_0}$ written only in terms of the slope $A$ and its uncertainty? \n"]}, {"cell_type": "markdown", "id": "f309d6fe", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_4_4'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L4.4 Weighted Linear Regression</h2>  \n", "\n", "| [Top](#section_4_0) | [Previous Section](#section_4_3) | [Exercises](#exercises_4_4) | [Next Section](#section_4_5) |\n"]}, {"cell_type": "markdown", "id": "5e22fc7f", "metadata": {"tags": ["learner", "8S50x", "md"]}, "source": ["*The material in this section is discussed in the video **<a href=\"https://courses.mitxonline.mit.edu/learn/course/course-v1:MITxT+8.S50.1x+3T2022/block-v1:MITxT+8.S50.1x+3T2022+type@sequential+block@seq_LS4/block-v1:MITxT+8.S50.1x+3T2022+type@vertical+block@vert_LS4_vid4\" target=\"_blank\">HERE</a>.** You are encouraged to watch that video and use this notebook concurrently.*"]}, {"cell_type": "markdown", "id": "30efa30d", "metadata": {"tags": ["learner", "md", "lect_04"]}, "source": ["<h3>Overview</h3>\n", "\n", "Each measurement of some variable $y$ carries an uncertainty $\\sigma_{y}$. \n", "\n", "*How the uncertainty is actually computed depends on the experiment. However, the uncertainty is **supposed** to correspond to the RMS of the measured value. In other words, if we perform the same measurement $N$ times, the variance of those measured values $y_{i}$ will be $V[y]=\\sigma^2_{y}$.* More precisely, the variance  should approach $\\sigma^{2}_{y}$ as we increase $N$.\n", "\n", "We can define a metric to minimize that accounts for the fact that different measurements can have larger or smaller uncertainties. \n", "\n", "$$\n", "\\begin{eqnarray} \n", "Q & = & \\sum_{i=1}^{N}\\frac{\\left(y_{i}-\\hat{y}_{i}\\right)^2}{\\sigma^{2}_{y_{i}}} \\\\\n", "Q & = & \\sum_{i=1}^{N}\\frac{\\left(y_{i}-Ax_{i}-b\\right)^2}{\\sigma^{2}_{y_{i}}} \\\\\n", "\\end{eqnarray}\n", "$$\n", "\n", "Note that if all of the uncertainties $\\sigma_{y_{i}}$ are the same, this is just a constant times the $Q$ we defined earlier and therefore setting the derivative to zero will give exactly the same answer as before.\n", "\n", "What does this do? For any $y_i$ that has a large uncertainty, the corresponding contribution to the metric $Q$ is scaled down, since we trust it less. We can now follow the same procedure as before and minimize this adjusted metric with respect to $A$ and $b$, yielding the following: \n", "\n", "$$\n", "\\begin{eqnarray} \n", "\\frac{\\partial Q}{\\partial A} & = & \\sum_{i=1}^{N} -2 \\frac{x_{i}}{\\sigma_{i}^2} \\left(y_{i}-Ax_{i}-b\\right) \\\\\n", "              & = & \\sum_{i=1}^{N} -2 \\frac{1}{\\sigma_{i}^2} \\left(x_{i} y_{i}-Ax^{2}_{i}-b x_{i}\\right) \\\\\n", "              & = & 0\n", "\\end{eqnarray}          \n", "$$\n", "\n", "and \n", "\n", "$$\n", "\\begin{eqnarray} \n", "\\frac{\\partial Q}{\\partial b} & = & \\sum_{i=1}^{N} -2  \\frac{1}{\\sigma_{i}^2} \\left(y_{i}-Ax_{i}-b\\right) \\\\\n", "              & = & 2Nb\\sum_{i=1}^{N}\\frac{1}{\\sigma_{i}^2} + 2A\\sum_{i=1}^{N} \\frac{x_{i}}{\\sigma_{i}^2} -2\\sum_{i=1}^{N}\\frac{y_{i}}{\\sigma_{i}^2} \\\\\n", "              & = & 0 \\\\\n", "\\end{eqnarray}          \n", "$$\n", "\n", "From this, we can draw an analogy to the previous derivation, and write the answer given by reparametrizing in terms of the weighted mean defined as \n", "\n", "$$\n", "\\begin{eqnarray} \n", " \\bar{y}_{w} & = & \\frac{\\sum_{i=1}^{N} \\frac{y_{i}}{\\sigma_{i}^2} }{\\sum_{i=1}^{N} \\frac{1}{\\sigma_{i}^2} }\n", "\\end{eqnarray} \n", "$$\n", "\n", "and the same for $\\bar{x}_{w}$. Given this, the values for $A$ and $b$ with weights are. \n", "\n", "$$\n", "\\begin{eqnarray} \n", "A  & = & \\frac{\\frac{1}{N}\\sum_{i=1}^{N} \\frac{1}{\\sigma_{i}^2}\\left(x_{i} - \\bar{x}_{w} \\right) \\left(y_{i}-\\bar{y}_{w}\\right)}      \n", "              {\\frac{1}{N}\\sum_{i=1}^{N} \\frac{1}{\\sigma_{i}^2}\\left(x_{i}-\\bar{x}_{w}\\right)^2}\\\\\n", "b  & = & \\bar{y}_{w} - A\\bar{x}_{w}      \n", "\\end{eqnarray}   \n", "$$\n", "\n", "Let's again update our plot with this result.  "]}, {"cell_type": "code", "execution_count": null, "id": "b339a82f", "metadata": {"tags": ["learner", "py", "lect_04", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L4.4-runcell01\n", "\n", "weights=[]\n", "for err in distance_err:\n", "    weights.append(1./err**2)\n", "    \n", "weights = np.array(weights)\n", "\n", "#Now let's do it with weights\n", "def variance_w(isamples,iweights):\n", "    mean=np.average(isamples,weights=iweights)\n", "    sumw=np.sum(iweights)\n", "    tot=0\n", "    for i0 in range(len(isamples)):\n", "        tot+=iweights[i0]*(isamples[i0]-mean)**2\n", "    return tot/sumw\n", "\n", "def covariance_w(ixs,iys,iweights):\n", "    meanx=np.average(ixs,weights=iweights)\n", "    meany=np.average(iys,weights=iweights)\n", "    sumw=np.sum(iweights)\n", "    tot=0\n", "    for i0 in range(len(ixs)):\n", "        tot+=iweights[i0]*(ixs[i0]-meanx)*(iys[i0]-meany)\n", "    return tot/sumw\n", "\n", "def regress_w(redshift,weights,distance):\n", "    varw=variance_w(redshift,weights)\n", "    covw=covariance_w(redshift,distance,weights)\n", "    Aw=covw/varw\n", "    bw=np.average(distance,weights=weights)-Aw*np.average(redshift,weights=weights)\n", "    return Aw,bw\n", "\n", "Aw,bw=regress_w(redshift,weights,distance)\n", "plotAll(redshift,distance,distance_err,Aw,bw)"]}, {"cell_type": "markdown", "id": "887de8d7", "metadata": {"tags": ["learner", "md", "lect_04"]}, "source": ["*Brief aside on general form*:\n", "\n", "One last quick technical point is that people often write the linear regression in terms of a matrix over a vector $\\vec{x}$ where each element $x_{i}$ in the vector is one of our measurements. The linear equation is then written as \n", "\n", "$$\n", "\\begin{eqnarray} \n", "  \\vec{\\hat{y}} & = & A\\vec{x}+b \\\\\n", "  \\vec{\\hat{y}} & = & \\beta\\vec{x^{\\prime}}\\\\\n", "\\end{eqnarray}\n", "$$\n", "\n", "where $\\vec{x^\\prime} = (\\vec{x},1)$ The resulting solution for best fit is written as\n", "\n", "$$\n", "\\begin{eqnarray} \n", "  \\hat{\\beta} & = & (\\vec{x}^{T}\\vec{x})^{-1}\\vec{x}^{T}\\vec{y}\n", "\\end{eqnarray} \n", "$$\n", "\n", "This is often how it is written in more advanced classes. In reality, this answer is no different than what we showed above. We haven't included weights here, but they can be added. "]}, {"cell_type": "markdown", "id": "78fcbf0e", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='exercises_4_4'></a>     \n", "\n", "| [Top](#section_4_0) | [Restart Section](#section_4_4) | [Next Section](#section_4_5) |\n"]}, {"cell_type": "markdown", "id": "39321475", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-4.4.1 Effects of Changing Uncertainty I</span>\n", "\n", "Let's imagine, for no particular reason, that our uncertainty on any given observation is multiplied by a factor of the redshift. That is, we define a new $\\sigma_{i}^{new} = z \\sigma_{i}^{old}$. How would the Hubble constant found by fitting the data with these modified uncertainties change, compared to the case when $\\sigma_{i}^{old}$ was used? Choose from the following options:\n", "\n", "- The fitted value of the Hubble constant would increase\n", "- The fitted value of the Hubble constant would decrease\n", "- The fitted value of the Hubble constant would stay the same\n", "\n", "You can modify the starting code below to find the answer."]}, {"cell_type": "code", "execution_count": null, "id": "635954f4", "metadata": {"tags": ["py", "draft", "learner_chopped"]}, "outputs": [], "source": ["#>>>EXERCISE: L4.4.1\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "#The code below scales the uncertainties being used in our weighted regression. \n", "\n", "import numpy as np\n", "\n", "redshift,distance,distance_err = load(label,0.1)\n", "\n", "# scale the errors by the redshift\n", "new_errors = #YOUR CODE HERE\n", "\n", "# translate these scaled uncertainties into new weights for regression (1/sigma^2 for each)\n", "weights = 1 / (new_errors * new_errors)\n", "\n", "Aw,bw=regress_w(redshift,weights,distance)\n", "plotAll(redshift,distance,new_errors,Aw,bw)"]}, {"cell_type": "markdown", "id": "f6895d01", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-4.4.2 Effects of Changing Uncertainty II</span>\n", "\n", "What happens if I made a mistake and all of the errors are 100 times larger than what I thought, such that $\\sigma_{i}^{new} = 100 \\sigma_{i}^{old}$? How would the Hubble constant found by fitting the data with these modified uncertainties change, compared to the case when $\\sigma_{i}^{old}$ was used?\n", "\n", "Again, choose from the following options:\n", "\n", "- The fitted value of the Hubble constant would increase\n", "- The fitted value of the Hubble constant would decrease\n", "- The fitted value of the Hubble constant would stay the same\n", "\n", "You can modify the starting code below to find the answer."]}, {"cell_type": "code", "execution_count": null, "id": "55c2f7ec", "metadata": {"tags": ["draft", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>EXERCISE: L4.4.2\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "import numpy as np\n", "\n", "redshift,distance,distance_err = load(label,0.1)\n", "\n", "# scale the errors by the redshift\n", "new_errors = #YOUR CODE HERE\n", "\n", "# translate these scaled uncertainties into new weights for regression (1/sigma^2 for each)\n", "weights = 1 / (new_errors * new_errors)\n", "\n", "Aw,bw=regress_w(redshift,weights,distance)\n", "plotAll(redshift,distance,new_errors,Aw,bw)"]}, {"cell_type": "markdown", "id": "f4c0ef0f", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_4_5'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L4.5 Minimization without the Math</h2>  \n", "\n", "| [Top](#section_4_0) | [Previous Section](#section_4_4) | [Exercises](#exercises_4_5) | [Next Section](#section_4_6) |\n"]}, {"cell_type": "markdown", "id": "ef82c547", "metadata": {"tags": ["learner", "8S50x", "md"]}, "source": ["*The material in this section is discussed in the video **<a href=\"https://courses.mitxonline.mit.edu/learn/course/course-v1:MITxT+8.S50.1x+3T2022/block-v1:MITxT+8.S50.1x+3T2022+type@sequential+block@seq_LS4/block-v1:MITxT+8.S50.1x+3T2022+type@vertical+block@vert_LS4_vid5\" target=\"_blank\">HERE</a>.** You are encouraged to watch that video and use this notebook concurrently.*"]}, {"cell_type": "markdown", "id": "4c581da1", "metadata": {"tags": ["learner", "md", "lect_05"]}, "source": ["<h3>Overview</h3>\n", "\n", "Many models have more than two parameters. Let's do some generalizing. \n", "\n", "Consider an arbitrary function $f(x|\\theta_{i})$ where $\\theta_{i}$ denotes a given set of model parameters. For the linear regression, $\\vec{\\theta}=(A,b)$. The quantity we would want to minimize in the context of least-squares is\n", "\n", "$$\n", "\\begin{equation}\n", "Q=\\sum_{i=1}^N \\left(y-f(x\\mid\\theta_j)\\right)^2,\n", "\\end{equation}\n", "$$\n", "\n", "which, in its most general form, gives us this set of partial derivatives:\n", "\n", "$$\n", "\\begin{equation}\n", "\\frac{\\partial Q}{\\partial\\theta_{i}} = \\frac{\\partial}{\\partial \\theta_{i}}\\sum_{i=1}^{N}\\left(y_{i}-f(x|\\theta_{i}\\right)^2=\\sum_{i=1}^{N} 2 \\left(y_{i}-f(x|\\theta_{i}\\right)\\frac{\\partial f(x|\\theta_{i})}{\\partial \\theta_{i}}\n", "\\end{equation}\n", "$$\n", "\n", "Using this, we can, in principle, calculate the derivatives and set them to 0, and then solve for the parameters that minimize our loss function. However, in many cases that's much easier said than done.\n", "\n", "Instead, we take a numerical approach, and fortunately there's plenty of packages to do so. Let's demonstrate by using the `scipy stats` toolkit to perform the linear regression. Note, this code comes more or less straight from examples in the `scipy` documentation."]}, {"cell_type": "code", "execution_count": null, "id": "ffe5a404", "metadata": {"tags": ["learner", "py", "lect_05", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L4.5-runcell01\n", "\n", "from scipy import stats\n", "\n", "#Now lets do the same thing with scipy\n", "slope, intercept, r_value, p_value, std_err = stats.linregress(redshift,distance)\n", "print(\"UnWeighted Fit:\",\"Hubbles Constant:\",1e6*3e5/slope,\"intercept\",intercept)\n", "\n", "#Now the weighted version; we use scikit-learn, a package dedicated to fitting\n", "from sklearn.linear_model import LinearRegression\n", "model = LinearRegression()\n", "redshifthack=np.reshape(redshift,(len(redshift),1))#line to get the fit code to work\n", "model.fit(redshifthack,distance,weights)\n", "slope=model.coef_\n", "const=model.intercept_\n", "print(\"Weighted Fit:\",\"Hubbles Constant:\",1e6*3e5/slope,\"intercept\",const)"]}, {"cell_type": "markdown", "id": "a8e7bb56", "metadata": {"tags": ["learner", "md", "lect_05"]}, "source": ["What exactly is going on under the hood? You could, of course, look at the `scipy` documentation and from there find the relevant source code. But, in short, there is an optimizer that uses numerical methods to evolve the values of the slope ($A$) and intercept ($b$) until a minimum of the objective is reached.\n", "\n", "In fact, we can access this optimizer directly! Let's see what it does."]}, {"cell_type": "code", "execution_count": null, "id": "dccfb9d0", "metadata": {"tags": ["learner", "py", "lect_05", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L4.5-runcell02\n", "\n", "#scipy provides the general tool that optimizes\n", "from scipy import optimize as opt\n", "\n", "#First an example minimizing a simple polynomial\n", "def f(x):\n", "    return x**4 + 3*(x-2)**3 - 15*(x)**2 + 1\n", "sol=opt.minimize_scalar(f, method='Brent')\n", "x = np.linspace(-8, 5, 100)\n", "\n", "plt.xlabel('x', fontsize=15) #Label x\n", "plt.ylabel('f(x)', fontsize=15)#Label y\n", "plt.plot(x, f(x));\n", "plt.axvline(sol.x, c='red')\n", "plt.show()"]}, {"cell_type": "markdown", "id": "2b15e0bb", "metadata": {"tags": ["learner", "md", "lect_05"]}, "source": ["The optimizer successfully found a minimum! Before we discuss how it works, let's go ahead and see how we can use scipy's optimizers to perform a fit. We'll also see how uncertainties fit into this."]}, {"cell_type": "code", "execution_count": null, "id": "000a886a", "metadata": {"tags": ["learner", "py", "lect_05", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L4.5-runcell03\n", "\n", "# scipy offers a tool for fitting a generic curve to data.\n", "# It's using the same sort of optimizer, to optimize its choice of error metric\n", "from scipy.optimize import curve_fit \n", "\n", "# We define a curve functional form\n", "def f(x,a,b):\n", "    return a*x+b\n", "\n", "popt, pcov = curve_fit(f, redshift,distance) # returns optimal values of a, b in popt\n", "perr = np.sqrt(np.diag(pcov))\n", "print(\"Unweighted Hubbles Constant:\",1e6*3e5/popt[0],\"+/-\",(1e6*3e5/popt[0]/popt[0])*perr[0],\"intercept\",popt[1],\"+/-\",perr[1])\n", "\n", "#Now let's do it weighted by uncertainties, by passing them in the \"sigma\" optional argument\n", "popt, pcov = curve_fit(f, redshift,distance,sigma=distance_err)\n", "perr = np.sqrt(np.diag(pcov))\n", "print(\"Weighted Hubbles Constant:\",1e6*3e5/popt[0],\"+/-\",(1e6*3e5/popt[0]/popt[0])*perr[0],\"intercept\",popt[1],\"+/-\",perr[1])"]}, {"cell_type": "markdown", "id": "d2c654c1", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='exercises_4_5'></a>     \n", "\n", "| [Top](#section_4_0) | [Restart Section](#section_4_5) | [Next Section](#section_4_6) |\n"]}, {"cell_type": "markdown", "id": "8f073d26", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-4.5.1 Finding the Minimum of a Potential</span>\n", "\n", "One of the most important distributions used in high energy physics is the Mexcian Hat Potential distribution. A generic form is given by: \n", "\n", "$$\n", "\\begin{eqnarray} \n", "  V(x,y) & = & (x^2+y^2)^2-A(x^2+y^2)\n", "\\end{eqnarray}\n", "$$\n", "\n", "Here, let's consider a 1D version where we fix $y$ and $A$, so we can write this as: \n", "\n", "$$\n", "\\begin{equation}\n", "f(x) = (x-2)^{2}(x+2)^2-24x^2\n", "\\end{equation}\n", "$$\n", "\n", "**Complete the code below to find ONE minimum of this new potential, using bounded minimization. What is the value of the minimum that you obtained? Enter your answer as a number with precision 1e-3.**\n", "\n", "*Note, in the next video we will look at a potential with slightly different values, and discuss some aspects of the minimization.*\n", "\n", "*Check out the `scipy.optimize.minimize` documentation here: https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html*\n", "\n", "*Also, for your interest, similarly shaped potentials include the Ricker potential, which you can read more about here: https://en.wikipedia.org/wiki/Ricker_wavelet*"]}, {"cell_type": "code", "execution_count": null, "id": "feed6a57", "metadata": {"tags": ["draft", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>EXERCISE: L4.5.1\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "from scipy import optimize as opt\n", "\n", "def f(x):\n", "    return #YOUR CODE HERE\n", "\n", "sol=opt.minimize_scalar(f, bounds=(-6,6), method='bounded')\n", "x = np.linspace(-6, 6, 100)\n", "\n", "print('min value over the array x:',sol.x)\n", "\n", "plt.xlabel('x', fontsize=15) #Label x\n", "plt.ylabel('f(x)', fontsize=15)#Label y\n", "plt.plot(x, f(x));\n", "plt.axvline(sol.x, c='red')\n", "plt.show()\n"]}, {"cell_type": "markdown", "id": "ebb1bad3", "metadata": {"tags": ["md", "learner", "learner_chopped"]}, "source": [">#### Follow-up 4.5.1a (ungraded)\n", ">\n", ">What makes this minimum particularly interesting? "]}, {"cell_type": "markdown", "id": "76c3d071", "metadata": {"tags": ["md", "learner", "learner_chopped"]}, "source": [">#### Follow-up 4.5.1b (ungraded)\n", ">\n", ">Change the bounds of the minimizer. Does this lead to different results?"]}, {"cell_type": "markdown", "id": "ff15a1b2", "metadata": {"tags": ["md", "learner", "learner_chopped"]}, "source": [">#### Follow-up 4.5.1c (ungraded)\n", ">\n", ">Try changing the function to create an asymmetric potential (for example, add $x$). Does the minimizer find the global mimimum, or is it stuck in the local minimum? Do different methods (for instance `brent`) do a better or worse job?"]}, {"cell_type": "markdown", "id": "c1fdd310", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_4_6'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L4.6 Gradient Descent</h2>  \n", "\n", "| [Top](#section_4_0) | [Previous Section](#section_4_5) | [Exercises](#exercises_4_6) | [Next Section](#section_4_7) |\n"]}, {"cell_type": "markdown", "id": "e0664b3d", "metadata": {"tags": ["learner", "8S50x", "md"]}, "source": ["*The material in this section is discussed in the video **<a href=\"https://courses.mitxonline.mit.edu/learn/course/course-v1:MITxT+8.S50.1x+3T2022/block-v1:MITxT+8.S50.1x+3T2022+type@sequential+block@seq_LS4/block-v1:MITxT+8.S50.1x+3T2022+type@vertical+block@vert_LS4_vid6\" target=\"_blank\">HERE</a>.** You are encouraged to watch that video and use this notebook concurrently.*"]}, {"cell_type": "markdown", "id": "2738d81d", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Slides</h3>\n", "\n", "Run the code below to view the slides for this section, which are discussed in the related video. You can also open the slides in a separate window <a href=\"https://mitx-8s50.github.io/slides/L04/slides_L04_06.html\" target=\"_blank\">HERE</a>."]}, {"cell_type": "code", "execution_count": null, "id": "5ec04fac", "metadata": {"tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L4.6-slides\n", "\n", "from IPython.display import IFrame\n", "IFrame(src='https://mitx-8s50.github.io/slides/L04/slides_L04_06.html', width=975, height=550)"]}, {"cell_type": "markdown", "id": "31af3c33", "metadata": {"tags": ["learner", "md", "lect_06"]}, "source": ["<h3>Overview</h3>\n", "\n", "Another way to think about minimization: Imagine a ball sliding around on our function, under gravity. The value of our function, then, is like the value of the gravitational potential, which we'll call $U$.\n", "\n", "How does this ball move? Recall that for a potential given by $U$ the corresponding force an object experiences is\n", "\n", "$$F=-\\nabla U$$ \n", "\n", "where $\\nabla$ is the gradient (or in 1D, derivative). Since $F=ma$, we know\n", "\n", "$$\\vec{a}=-\\frac{1}{m}\\nabla U$$\n", "\n", "which, integrating, gives the velocity:\n", "\n", "$$\\vec{v}=\\int \\vec{a} dt =-\\int \\frac{1}{m}\\nabla U dt$$,\n", "\n", "and then position:\n", "\n", "$$\\vec{x}=\\int \\vec{v} dt =\\int \\int \\vec{a} dt dt =-\\int \\int \\frac{1}{m}\\nabla U dt dt$$\n", "\n", "up to initial conditions.\n", "\n", "With this as inspiration, if we're trying to minimize some function $U$, we can start at a point and then iterate in discrete timesteps $\\delta t$, approximating the motion of such a ball along that function:\n", "\n", "$$\n", "\\begin{eqnarray}\n", "\\vec{v} = \\vec{v_0} - \\delta t \\nabla U \\\\\n", "\\vec{x} = \\vec{x_0} + \\delta t \\vec{v} - (\\delta t)^2\\nabla U\n", "\\end{eqnarray}\n", "$$\n", "\n", "Let's code this up by hand. The resulting algorithm is a type of **gradient descent**."]}, {"cell_type": "code", "execution_count": null, "id": "58e0417b", "metadata": {"tags": ["learner", "py", "lect_06", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L4.6-runcell01\n", "\n", "#Let's find the minimum of x^2\n", "def f(x):\n", "    return x**2\n", "# We need to define the gradient of f\n", "def grad(x):\n", "    return 2*x\n", "\n", "#Simple gradient descent (this one is not physics-based, but rather just steps along the gradient)\n", "#Starting on x, move in the direction of negative gradient, scaled by alpha (delta t from earlier)\n", "def gd(x, grad, alpha, max_iter=10):\n", "    xs = np.zeros(1 + max_iter)\n", "    xs[0] = x #start at x\n", "    for i in range(max_iter):\n", "        x = x - alpha * grad(x)\n", "        xs[i+1] = x\n", "    return xs\n", "\n", "#Physics based gradient descent, which comes from our equations above\n", "#Now will update both position and velocity in timesteps alpha\n", "def gd_momentum_nodamp(x, grad, alpha, max_iter=10):\n", "    xs = np.zeros(1 + max_iter)\n", "    xs[0] = x\n", "    v = 0\n", "    for i in range(max_iter):\n", "        v = v - alpha*grad(x)\n", "        x = x + alpha * v  - 0.5 * alpha * alpha*grad(x)\n", "        xs[i+1] = x\n", "    return xs\n", "\n", "def plotGDAlgo(iGDAlgo,alpha=0.1,beta=None,x0=1):\n", "    if beta is not None:\n", "        xs = iGDAlgo(x0, grad, alpha, beta)\n", "    else:\n", "        xs = iGDAlgo(x0, grad, alpha)\n", "    xp = np.linspace(-1.2, 1.2, 100)\n", "\n", "    #Now just plotting code\n", "    plt.xlabel('x', fontsize=15) #Label x\n", "    plt.ylabel('f(x)', fontsize=15)#Label y\n", "    plt.plot(xp, f(xp)) #function\n", "    plt.plot(xs, f(xs), 'o-', c='red') #varied function\n", "    for i, (x, y) in enumerate(zip(xs, f(xs)), 1):\n", "        plt.text(x, y+0.2, i,bbox=dict(facecolor='yellow', alpha=0.5), fontsize=14)\n", "    plt.show()    \n", "\n", "plotGDAlgo(gd)\n", "plotGDAlgo(gd,0.95) #large timestep alpha\n", "plotGDAlgo(gd_momentum_nodamp,0.95) #Large timestep, with momentum"]}, {"cell_type": "markdown", "id": "ee79f660", "metadata": {"tags": ["learner", "md", "lect_06"]}, "source": ["The simple, non-physics based gradient descent with small timestep (first plot) works. The one with large timestep (second plot) works as well, though it overshoots. Our physics-based one (third plot) jumps around.\n", "\n", "Actually, if our discrete time stepping accurately modeled the equations of motion of a ball on the potential energy surface, the physics-based method would never converge, because energy is conserved (as we've defined it, the system is frictionless)! Rather, the ball would roll back and forth forever. In this case we get lucky, and errors in our discrete implementation lead us to the minimum. But this is not guaranteed!\n", "\n", "We need to add a friction term, which we'll parameterize by a number damp$<1$. In this case, we will define it as \n", "\n", "$$\n", "\\begin{equation}\n", " \\rm{damp} = \\frac{1}{1+\\beta^{\\rm{iter}+1}}\n", "\\end{equation}\n", "$$\n", "\n", "for a new parameter $\\beta<1$  and $iter$ the iteration step number. "]}, {"cell_type": "code", "execution_count": null, "id": "37dd0ae5", "metadata": {"tags": ["learner", "py", "lect_06", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L4.6-runcell02\n", "\n", "#Now will update both momentum and velocity in timesteps alpha but with a dilution factor of beta\n", "def gd_momentum(x, grad, alpha, beta=0.9, max_iter=10):\n", "    xs = np.zeros(1 + max_iter)\n", "    xs[0] = x\n", "    v = 0\n", "    for i in range(max_iter):\n", "        v = beta*v + (1-beta)*grad(x) # beta effectively scales how much the velocity is affected by the local gradient\n", "        vc = v/(1+beta**(i+1)) # and also controls the damping of the velocity\n", "        x = x - alpha * vc\n", "        xs[i+1] = x\n", "    return xs\n", "\n", "plotGDAlgo(gd)\n", "plotGDAlgo(gd,0.95) #large timestep alpha\n", "plotGDAlgo(gd_momentum_nodamp,0.95) #Large timestep\n", "plotGDAlgo(gd_momentum,0.95) #Large timestep\n", "plotGDAlgo(gd_momentum,0.95,beta=0.7) #Large timestep\n", "plotGDAlgo(gd_momentum,0.95,beta=0.9) #Large timestep"]}, {"cell_type": "markdown", "id": "ba3b6e52", "metadata": {"tags": ["learner", "md", "lect_06"]}, "source": ["All of these guys approach the minimum and you can be quite flexible with the behavior. We can, in fact, use any minimization function we want in `scipy.optimize`. It will do the iterative stepping for us. Let's write our own minimizer and throw it in. We can do it for our previous quartic equation."]}, {"cell_type": "code", "execution_count": null, "id": "a8469b7f", "metadata": {"tags": ["learner", "py", "lect_06", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L4.6-runcell03\n", "\n", "#Now let's do this with a general tool that optimizes\n", "import scipy.linalg as la\n", "def f(x):\n", "    return x**4 + 3*(x-2)**3 - 15*(x)**2 + 1\n", "\n", "def fprime(x):\n", "    return 4*x**3 + 9*(x-2)**2 - 30*(x)\n", "\n", "#Where is our minimizer\n", "def custmin(fun, x0, args=(), maxfev=None, alpha=0.0002,\n", "        maxiter=100000, tol=1e-10, callback=None, **options):\n", "    \"\"\"Implements simple gradient descent for the function above.\"\"\"\n", "    bestx = x0\n", "    bestf = fun(x0)\n", "    funcalls = 1\n", "    niter = 0\n", "    improved = True\n", "    stop = False\n", "\n", "    while improved and not stop and niter < maxiter:\n", "        niter += 1\n", "        # the next 2 lines are gradient descent\n", "        step = alpha * fprime(bestx)\n", "        bestx = bestx - step\n", "        \n", "        bestf = fun(bestx)\n", "        funcalls += 1\n", "\n", "        if la.norm(step) < tol:\n", "            improved = False\n", "        if callback is not None:\n", "            callback(bestx)\n", "        if maxfev is not None and funcalls >= maxfev:\n", "            stop = True\n", "            break\n", "\n", "    return opt.OptimizeResult(fun=bestf, x=bestx, nit=niter,nfev=funcalls, success=(niter > 1))\n", "\n", "def reporter(p):\n", "    \"\"\"Reporter function to capture intermediate states of optimization.\"\"\"\n", "    global ps\n", "    ps.append(p)\n", "\n", "x0=-7\n", "x0 = np.array([x0])\n", "ps = [x0]\n", "sol=opt.minimize(f, x0, method=custmin, callback=reporter)\n", "x = np.linspace(-8, 5, 100)\n", "plt.xlabel('x', fontsize=15) #Label x\n", "plt.ylabel('f(x)', fontsize=15)#Label y\n", "plt.plot(x, f(x));\n", "plt.axvline(sol.x, c='red')\n", "plt.show()\n", "\n", "#Now let's trick it\n", "x0=5\n", "x0 = np.array([x0])\n", "ps = [x0]\n", "sol=opt.minimize(f, x0, method=custmin, callback=reporter)\n", "x = np.linspace(-8, 5, 100)\n", "plt.xlabel('x', fontsize=15) #Label x\n", "plt.ylabel('f(x)', fontsize=15)#Label y\n", "plt.plot(x, f(x));\n", "plt.axvline(sol.x, c='red')\n", "plt.show()"]}, {"cell_type": "markdown", "id": "047631da", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='exercises_4_6'></a>     \n", "\n", "| [Top](#section_4_0) | [Restart Section](#section_4_6) | [Next Section](#section_4_7) |\n"]}, {"cell_type": "markdown", "id": "54938451", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-4.6.1</span>\n", "\n", "Let's say you were stuck using the simple minimizer that we developed above. Let's also assume that the global mimimum is within some range that you know. What could you do to make sure your minimizer finds the global minimum, instead of getting stuck in a local minimum? Select all that apply:\n", "\n", "- Use a larger number of iterations.\n", "- Use a smaller number for the tolerance.\n", "- Choose random starting points for the minimizer, and save the result that yields the lowest value.\n", "- Restart your minimizer once it finds it's first minimum, and use a new starting point that is some pre-defined (perhaps random) distance away from the minimum that is found; do this several times and save the result that yields the lowest value.\n"]}, {"cell_type": "markdown", "id": "fc0a1189", "metadata": {"tags": ["md", "learner", "learner_chopped"]}, "source": [">#### Follow-up 4.6.1a (ungraded)\n", ">\n", ">Try some of the strategies above. What works? What does not?"]}, {"cell_type": "markdown", "id": "f85adb92", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_4_7'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L4.7 Fitting the Full Range of Hubble Data</h2>  \n", "\n", "| [Top](#section_4_0) | [Previous Section](#section_4_6) | [Exercises](#exercises_4_7) | [Next Section](#section_4_8) |\n"]}, {"cell_type": "markdown", "id": "13dda79c", "metadata": {"tags": ["learner", "8S50x", "md"]}, "source": ["*The material in this section is discussed in the video **<a href=\"https://courses.mitxonline.mit.edu/learn/course/course-v1:MITxT+8.S50.1x+3T2022/block-v1:MITxT+8.S50.1x+3T2022+type@sequential+block@seq_LS4/block-v1:MITxT+8.S50.1x+3T2022+type@vertical+block@vert_LS4_vid7\" target=\"_blank\">HERE</a>.** You are encouraged to watch that video and use this notebook concurrently.*"]}, {"cell_type": "markdown", "id": "48289f6e", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Slides</h3>\n", "\n", "Run the code below to view the slides for this section, which are discussed in the related video. You can also open the slides in a separate window <a href=\"https://mitx-8s50.github.io/slides/L04/slides_L04_07.html\" target=\"_blank\">HERE</a>."]}, {"cell_type": "code", "execution_count": null, "id": "df377154", "metadata": {"tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L4.7-slides\n", "\n", "from IPython.display import IFrame\n", "IFrame(src='https://mitx-8s50.github.io/slides/L04/slides_L04_07.html', width=975, height=550)"]}, {"cell_type": "markdown", "id": "70bb4a04", "metadata": {"tags": ["learner", "md", "lect_07"]}, "source": ["<h3>Overview</h3>\n", "\n", "Now, what if we want to minimize a function a fit function in two dimensions.  To do that, we are going to go back to our supernova data, and fit the line with a modified slope expansion given by:\n", "\n", "$$ f(x) = \\frac{x}{h_{0}}\\left(1+\\frac{1-q}{2}x\\right) $$\n", "\n", "Additionally, we have removed $b$, since this form is a closer approximation of the true form of the expansion of the universe, and in that instance $f(0)=0$\n", "\n", "Recall that we needed to minimize: \n", "\n", "$$\n", "\\begin{equation}\n", " \\frac{\\partial Q}{\\partial\\theta_{i}} = \\frac{\\partial}{\\partial \\theta_{i}}\\sum_{i=1}^{N}\\left(y_{i}-f(x|\\theta_{i}\\right)^2=\\sum_{i=1}^{N} 2 \\left(y_{i}-f(x|\\theta_{i}\\right)\\frac{\\partial f(x|\\theta_{i})}{\\partial \\theta_{i}}\n", "\\end{equation}\n", "$$\n", "\n", ", which means that we need to compute $\\frac{\\partial f}{\\partial h_{0}}$ and $\\frac{\\partial f}{\\partial q}$. We do it below in `fprime` and output a vector with two entries for $h_{0}$ and $q$. "]}, {"cell_type": "code", "execution_count": null, "id": "89258c61", "metadata": {"tags": ["learner", "py", "lect_07", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L4.7-runcell01\n", "\n", "#Ok let's do a custom minimization of our fit function with gradient descent\n", "#Note that since we are fitting two parameters we need to do this in 2D\n", "redshift,distance,distance_err = load(label,10)\n", "weights=np.array([])\n", "for pVal in distance_err:\n", "    weights = np.append(weights,1./pVal/pVal)\n", "\n", "def f(x,h0,q):\n", "    val=x*(1e6*3e5/h0)*(1 + ((1-q)*0.5)*x)\n", "    return val\n", "\n", "def fprime(x,h0,q):\n", "    der=np.zeros(2)\n", "    der[0]=-1*x*(1e6*3e5/h0/h0)*(1 + ((1-q)*0.5)*x)\n", "    der[1]=x*(1e6*3e5/h0)*(-0.5*x)\n", "    return der\n", "\n", "def algof(inputs):\n", "    d=0\n", "    for i0 in range(len(redshift)):\n", "        yhat=f(redshift[i0],inputs[0],inputs[1])\n", "        pD=(distance[i0]-yhat)**2\n", "        #pD=pD/dmean\n", "        d+=pD*weights[i0]\n", "    return d\n", "\n", "def algofprime(inputs):\n", "    d=np.zeros(2)\n", "    for i0 in range(len(redshift)):\n", "        yhat=f(redshift[i0],inputs[0],inputs[1])\n", "        pD=2*(distance[i0]-yhat)\n", "        yhatprime=fprime(redshift[i0],inputs[0],inputs[1])\n", "        #print(yhatprime,pD,d)\n", "        #pD=pD/dmean\n", "        d=d+(yhatprime*pD)*weights[i0]\n", "    return d\n", "\n", "def custmin(fun, x0, args=(), maxfev=None, alpha=0.0001,maxiter=10000, tol=1e-10, callback=None, **options):\n", "    \"\"\"Implements simple gradient descent for the function above.\"\"\"\n", "    bestx = x0\n", "    bestf = fun(x0)\n", "    funcalls = 1\n", "    niter = 0\n", "    improved = True\n", "    stop = False\n", "\n", "    while improved and not stop and niter < maxiter:\n", "        niter += 1\n", "        if niter % 1000 == 0: \n", "            print(niter,bestx)\n", "        # the next 2 lines are gradient descent\n", "        step = alpha * algofprime(bestx)\n", "        #print(bestx,step)\n", "        bestx = bestx + step\n", "\n", "        bestf = fun(bestx)\n", "        funcalls += 1\n", "\n", "        if la.norm(step) < tol:\n", "            improved = False\n", "        if callback is not None:\n", "            callback(bestx)\n", "        if maxfev is not None and funcalls >= maxfev:\n", "            stop = True\n", "            break\n", "\n", "    return opt.OptimizeResult(fun=bestf, x=bestx, nit=niter,nfev=funcalls, success=(niter > 1))\n", "\n", "def reporter(p):\n", "    \"\"\"Reporter function to capture intermediate states of optimization.\"\"\"\n", "    global ps\n", "    ps.append(p)\n", "\n", "#tmp=algofprime([70,0.03])\n", "x0 = np.array([40,0])\n", "ps = [x0]\n", "sol0=opt.minimize(algof, x0, method=custmin, callback=reporter)\n", "print(sol0)\n", "print()\n", "\n", "sol1=opt.minimize(algof, x0)#, method=custmin, callback=reporter)\n", "print(sol1)\n", "    "]}, {"cell_type": "markdown", "id": "2c8111e2", "metadata": {"tags": ["learner", "md", "lect_07"]}, "source": ["There are problems with this approach, but we obtain 69.18 as our fit for the Hubble constant, which is actually closer to currently published values than our simple linear regression."]}, {"cell_type": "markdown", "id": "9c6f255f", "metadata": {"tags": ["learner", "md", "lect_07"]}, "source": ["<h3>Optimized Minimizers: Newton Step</h3>\n", "\n", "Typically, more advanced optimization methods are used beyond gradient descent. If we don't use our custom minimizer, but rather one off the shelf, we get the same answer but in a fraction of the time. These minimizers are based on a mathematical dance move called the Newton step. It goes like this. Recall from a Taylor expansion we have\n", "\n", "$$\n", "\\begin{equation}\n", "f(x+h)=f(x)+h\\frac{df}{dx}(x)+\\frac{h^2}{2}\\frac{d^2f}{dx^2} \\\\\n", "\\end{equation}\n", "$$\n", "\n", "Now we know that at the function's minimum, the derivative is zero and so we have\n", "\n", "$$\n", "\\begin{equation}\n", "\\frac{f(x+h)-f(x)}{h}=0=\\frac{df}{dx}(x)+\\frac{h}{2}\\frac{d^2f}{dx^2} \\\\\n", "\\end{equation}\n", "$$\n", "\n", "and so for $h/2=\\Delta x$, we get the Newton step:  \n", "\n", "$$\n", "\\begin{equation}\n", "0=\\frac{df}{dx}(x)+\\Delta x \\frac{d^2f}{dx^2} \\\\\n", "\\Delta x=-\\frac{\\frac{df}{dx}}{\\frac{d^2f}{dx^2}}(x)\\\\\n", "\\Delta x=-\\frac{f^{\\prime}(x)}{f^{\\prime\\prime}(x)}\\\\\n", "\\end{equation}\n", "$$\n", "\n", "This defines an iteration that we can do step by step. Namely, we have $x\\rightarrow x+\\Delta x$. You might recall this same procedure from Newtons' method of finding roots $x_{k+1}=x_{k}-\\frac{f(x)}{f^{\\prime}(x)}$.  We can generalize this into N dimensions. This gives us: \n", "\n", "$$\n", "\\begin{equation}\n", "f(\\vec{x}+\\vec{h})=f(\\vec{x})+\\vec{h}^{T}\\nabla f(x)+ \\frac{1}{2}\\vec{h}^{T}\\frac{\\partial^2 f}{dx_{i}dx_{j}}\\vec{h} \\\\\n", "\\vec{h}=-\\left(\\frac{\\partial^2 f}{dx_{i}dx_{j}} \\right)^{-1}\\nabla f(\\vec{x})\n", "\\end{equation}\n", "$$\n", "\n", "where we have the Hessian given by $\\frac{\\partial^2 f}{dx_{i}dx_{j}}$. This gives us a way to step through the optimization very efficiently. There are many variations on a theme, Adam, RMSProp, ... All of these are minimization algorithms that work in about the same way. \n", "\n", "Finally, we should note that in the examples above, we computed the derivative analytically. However, it's often the case that the function is not analytic. One way to get around this (not the best way, but one that is easy to explain) is to do numerical differentiation. We can define the numerical derivative as \n", "\n", "$$\n", "\\begin{equation}\n", "\\frac{\\partial}{\\partial \\theta_{i}} \\vec{f} = \\frac{f\\left(\\vec{x} + \\Delta \\hat{\\theta}_{i}\\right)-f\\left(\\vec{x} - \\Delta \\hat{\\theta}_{i}\\right)}{2\\Delta\\theta}\n", "\\end{equation}\n", "$$"]}, {"cell_type": "markdown", "id": "b4bc0726", "metadata": {"tags": ["learner", "md", "lect_07"]}, "source": ["<h3>Computing the Derivative Numerically</h3>\n", "\n", "Finally, we should note that in the examples above, we computed the derivative analytically. However, it's often the case that the function is not analytic. One way to get around this (not the best way, but one that is easy to explain) is to do numerical differentiation. We can define the numerical derivative as \n", "\n", "$$\n", "\\begin{equation}\n", "\\frac{\\partial}{\\partial \\theta_{i}} \\vec{f} = \\frac{f\\left(\\vec{x} + \\Delta \\hat{\\theta}_{i}\\right)-f\\left(\\vec{x} - \\Delta \\hat{\\theta}_{i}\\right)}{2\\Delta\\theta}\n", "\\end{equation}\n", "$$"]}, {"cell_type": "code", "execution_count": null, "id": "a4b4caf6", "metadata": {"tags": ["learner", "py", "lect_07", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L4.7-runcell02\n", "\n", "#Ok let's do a custom minimization of our fit function with gradient descent\n", "#Note that since we are fitting two parameters we need to do this in 2D\n", "def f(x,h0,q):\n", "    val=x*(1e6*3e5/h0)*(1 + ((1-q)*0.5)*x)\n", "    return val\n", "\n", "def algof(inputs):\n", "    d=0\n", "    for i0 in range(len(redshift)):\n", "        yhat=f(redshift[i0],inputs[0],inputs[1])\n", "        pD=(distance[i0]-yhat)**2\n", "        #pD=pD/dmean\n", "        d+=pD*weights[i0]\n", "    return d\n", "\n", "def algofprime(inputs):\n", "    delta1=np.array([inputs[0]*0.001,0])\n", "    delta2=np.array([0,inputs[1]*0.001])\n", "    dp11=algof(inputs-delta1)\n", "    dp21=algof(inputs-delta2)\n", "    dp12=algof(inputs+delta1)\n", "    dp22=algof(inputs+delta2)\n", "    deriv=np.array([0,0])\n", "    deriv[0]=(dp11-dp12)/(2*delta1[0])\n", "    deriv[1]=(dp21-dp22)/(2*delta2[1])\n", "    return deriv\n", "\n", "def algofprime2(inputs):\n", "    d=np.zeros(2)\n", "    for i0 in range(len(redshift)):\n", "        yhat=f(redshift[i0],inputs[0],inputs[1])\n", "        pD=2*(distance[i0]-yhat)\n", "        yhatprime=fprime(redshift[i0],inputs[0],inputs[1])\n", "        #print(yhatprime,pD,d)\n", "        #pD=pD/dmean\n", "        d=d+(yhatprime*pD)*weights[i0]\n", "    return d\n", "\n", "def custmin(fun, x0, args=(), maxfev=None, alpha=0.01,maxiter=10000, tol=1e-10, callback=None, **options):\n", "    \"\"\"Implements simple gradient descent for the function above.\"\"\"\n", "    bestx = x0\n", "    bestf = fun(x0)\n", "    funcalls = 1\n", "    niter = 0\n", "    improved = True\n", "    stop = False\n", "\n", "    while improved and not stop and niter < maxiter:\n", "        niter += 1\n", "        if niter % 1000 == 0: \n", "            print(niter,bestx)\n", "        # the next 2 lines are gradient descent\n", "        step = alpha * algofprime(bestx)\n", "        #print(bestx,step)\n", "        bestx = bestx + step\n", "\n", "        bestf = fun(bestx)\n", "        funcalls += 1\n", "\n", "        if la.norm(step) < tol:\n", "            improved = False\n", "        if callback is not None:\n", "            callback(bestx)\n", "        if maxfev is not None and funcalls >= maxfev:\n", "            stop = True\n", "            break\n", "\n", "    return opt.OptimizeResult(fun=bestf, x=bestx, nit=niter,nfev=funcalls, success=(niter > 1))\n", "\n", "def reporter(p):\n", "    \"\"\"Reporter function to capture intermediate states of optimization.\"\"\"\n", "    global ps\n", "    ps.append(p)\n", "\n", "x0 = np.array([40,0.1])\n", "ps = [x0]\n", "sol0=opt.minimize(algof, x0, method=custmin, callback=reporter)\n", "print(sol0)\n", "print()\n", "\n", "sol1=opt.minimize(algof, x0)#, method=custmin, callback=reporter)\n", "print(sol1)"]}, {"cell_type": "markdown", "id": "9ab24fee", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='exercises_4_7'></a>     \n", "\n", "| [Top](#section_4_0) | [Restart Section](#section_4_7) | [Next Section](#section_4_8) |\n"]}, {"cell_type": "markdown", "id": "6631afab", "metadata": {"tags": ["md", "learner", "learner_chopped"]}, "source": [">#### Follow-up 4.7.1a (ungraded)\n", ">\n", ">The `custmin` minimizer in the code cell `L4.7-runcell02` is not working properly. Can you get it to work? Is it faster than defining the derivative analytically?"]}, {"cell_type": "markdown", "id": "620cf448", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_4_8'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L4.8 Fitting with lmfit</h2>     \n", "\n", "| [Top](#section_4_0) | [Previous Section](#section_4_7) | [Exercises](#exercises_4_8) |\n"]}, {"cell_type": "markdown", "id": "abc97939", "metadata": {"tags": ["learner", "8S50x", "md"]}, "source": ["*The material in this section is discussed in the video **<a href=\"https://courses.mitxonline.mit.edu/learn/course/course-v1:MITxT+8.S50.1x+3T2022/block-v1:MITxT+8.S50.1x+3T2022+type@sequential+block@seq_LS4/block-v1:MITxT+8.S50.1x+3T2022+type@vertical+block@vert_LS4_vid8\" target=\"_blank\">HERE</a>.** You are encouraged to watch that video and use this notebook concurrently.*"]}, {"cell_type": "markdown", "id": "254ebe51", "metadata": {"tags": ["learner", "md", "lect_08"]}, "source": ["<h3>Overview</h3>\n", "\n", "Finally, let's write this with the package `lmfit`, which is the way I would actually do the problem. Again, no need to reinvent the wheel (except when learning how it works!)"]}, {"cell_type": "code", "execution_count": null, "id": "b0bcd953", "metadata": {"tags": ["learner", "py", "lect_08", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L4.8-runcell01\n", "\n", "import lmfit\n", "\n", "weights=np.array([])\n", "for pVal in distance_err:\n", "    #weighted fits in lmfit require you to ass in 1/sigma (not 1/sigma^2\n", "    weights = np.append(weights,1./pVal)\n", "\n", "#Clearly that's not working so lets use an approximation to this\n", "def f(x,h0,q):\n", "    val=x*(1e6*3e5/h0)*(1 + ((1-q)*0.5)*x)\n", "    return val\n", "\n", "model  = lmfit.Model(f)\n", "p = model.make_params(h0=50,q=0)\n", "result = model.fit(data=distance, params=p, x=redshift, weights=weights)\n", "lmfit.report_fit(result)\n", "plt.figure()\n", "result.plot();"]}, {"cell_type": "markdown", "id": "f189672d", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='exercises_4_8'></a>   \n", "\n", "| [Top](#section_4_0) | [Restart Section](#section_4_8) |\n"]}, {"cell_type": "markdown", "id": "d5062262", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-4.8.1 Modified Hubble Fit</span>\n", "\n", "Add a constant term to the fit, and see if the fitted parameter is consistent with zero. Your fit function should have the following form:   \n", "\n", "$$ f(x) = c + \\frac{x}{h_{0}}\\left(1+\\frac{1-q}{2}x\\right) $$\n", "\n", "Run the fit with this modified form. What is your value for the constant $c$ in units of megaparsecs (Mpc)? How does this compare with the typical distance between galaxies?\n", "\n", "Report your answer with precision 1e-2."]}, {"cell_type": "code", "execution_count": null, "id": "b80cbcdf", "metadata": {"tags": ["draft", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>EXERCISE: L4.8.1\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "def f(x,h0,q,c):\n", "    return #YOUR CODE HERE\n", "\n", "\n", "###FILL IN THE CODE BELOW###\n", "\n", "model = #YOUR CODE HERE\n", "p = #YOUR CORE HERE \n", "\n", "######\n", "\n", "model  = lmfit.Model(f)\n", "p = model.make_params(h0=50,q=0,c=0)\n", "\n", "result = model.fit(data=distance, params=p, x=redshift, weights=weights)\n", "lmfit.report_fit(result)\n", "plt.figure()\n", "result.plot();\n"]}], "metadata": {"celltoolbar": "Tags", "kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}