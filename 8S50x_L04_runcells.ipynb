{"cells": [{"cell_type": "markdown", "id": "0378bae4", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<hr style=\"height: 1px;\">\n", "<i>This notebook was authored by the 8.S50x Course Team, Copyright 2022 MIT All Rights Reserved.</i>\n", "<hr style=\"height: 1px;\">\n", "<br>\n", "\n", "<h1>Lesson 4: Introduction to Data Fitting</h1>\n"]}, {"cell_type": "markdown", "id": "cebc864d", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_4_0'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L4.0 Overview</h2>\n"]}, {"cell_type": "markdown", "id": "fea35b2c", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<h3>Navigation</h3>\n", "\n", "<table style=\"width:100%\">\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_4_1\">L4.1 An Example: Hubble Constant</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_4_1\">L4.1 Exercises</a></td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_4_2\">L4.2 Derivation of Linear Regression</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_4_2\">L4.2 Exercises</a></td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_4_3\">L4.3 Linear Regression: Coding Example</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_4_3\">L4.3 Exercises</a></td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_4_4\">L4.4 Weighted Linear Regression</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_4_4\">L4.4 Exercises</a></td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_4_5\">L4.5 Minimization without the Math</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_4_5\">L4.5 Exercises</a></td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_4_6\">L4.6 Gradient Descent</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_4_6\">L4.6 Exercises</a></td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_4_7\">L4.7 Fitting the Full Range of Hubble Data</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_4_7\">L4.7 Exercises</a></td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_4_8\">L4.8 Fitting with lmfit</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_4_8\">L4.8 Exercises</a></td>\n", "    </tr>\n", "</table>\n", "\n"]}, {"cell_type": "code", "execution_count": null, "id": "d25fc1df", "metadata": {"tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L4.0-runcell01\n", "\n", "#importing data from git repository\n", "\n", "!git init\n", "!git remote add -f origin https://github.com/mitx-8s50/nb_LEARNER/\n", "!git config core.sparseCheckout true\n", "!echo 'data/L04' >> .git/info/sparse-checkout\n", "!git pull origin main"]}, {"cell_type": "code", "execution_count": null, "id": "eae588a6", "metadata": {"tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L4.0-runcell02\n", "\n", "!pip install lmfit"]}, {"cell_type": "code", "execution_count": null, "id": "ca7976b7", "metadata": {"tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L4.0-runcell03\n", "\n", "#import packages\n", "import math as math                   #https://docs.python.org/3/library/math.html\n", "import matplotlib.pyplot as plt       #https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.html\n", "import numpy as np                    #https://numpy.org/doc/stable/\n", "import csv                            #https://docs.python.org/3/library/csv.html \n", "from scipy import stats               #https://docs.scipy.org/doc/scipy/reference/stats.html\n", "from scipy.optimize import curve_fit  #https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.curve_fit.html\n", "import scipy.linalg as la             #https://docs.scipy.org/doc/scipy/reference/linalg.html\n", "import lmfit                          #https://lmfit.github.io/lmfit-py/ "]}, {"cell_type": "code", "execution_count": null, "id": "600cbada", "metadata": {"tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L4.0-runcell04\n", "\n", "#set plot resolution\n", "%config InlineBackend.figure_format = 'retina'\n", "\n", "#set default figure parameters\n", "plt.rcParams['figure.figsize'] = (9,6)\n", "\n", "medium_size = 12\n", "large_size = 15\n", "\n", "plt.rc('font', size=medium_size)          # default text sizes\n", "plt.rc('xtick', labelsize=medium_size)    # xtick labels\n", "plt.rc('ytick', labelsize=medium_size)    # ytick labels\n", "plt.rc('legend', fontsize=medium_size)    # legend\n", "plt.rc('axes', titlesize=large_size)      # axes title\n", "plt.rc('axes', labelsize=large_size)      # x and y labels\n", "plt.rc('figure', titlesize=large_size)    # figure title"]}, {"cell_type": "markdown", "id": "f3e315e9", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_4_1'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L4.1 An Example: Hubble Constant</h2>  \n", "\n", "| [Top](#section_4_0) | [Previous Section](#section_4_0) | [Exercises](#exercises_4_1) | [Next Section](#section_4_2) |\n"]}, {"cell_type": "code", "execution_count": null, "id": "50b994dd", "metadata": {"tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L4.1-slides\n", "\n", "from IPython.display import IFrame\n", "IFrame(src='https://mitx-8s50.github.io/slides/L04/slides_L04_01.html', width=975, height=550)"]}, {"cell_type": "markdown", "id": "006eb8b5", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='exercises_4_1'></a>     \n", "\n", "| [Top](#section_4_0) | [Restart Section](#section_4_1) | [Next Section](#section_4_2) |\n"]}, {"cell_type": "markdown", "id": "b0687430", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-4.1.1 Expansion and Recessional Velocity</span>\n", "\n", "True or False: If the universe is expanding, the apparent recessional velocity of galaxies further away will be larger.\n"]}, {"cell_type": "markdown", "id": "7beb47e6", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_4_2'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L4.2 Derivation of Linear Regression</h2>  \n", "\n", "| [Top](#section_4_0) | [Previous Section](#section_4_1) | [Exercises](#exercises_4_2) | [Next Section](#section_4_3) |\n"]}, {"cell_type": "code", "execution_count": null, "id": "7468f053", "metadata": {"tags": ["learner", "py", "lect_02", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L4.2-runcell01\n", "\n", "import math as math\n", "import matplotlib.pyplot as plt\n", "import numpy as np\n", "import csv\n", "\n", "#Today we are going to start with astro data from here : \n", "#http://supernova.lbl.gov/Union/\n", "#Let's load the data\n", "\n", "label='data/L04/sn_z_mu_dmu_plow_union2.1.txt'\n", "\n", "\n", "#Table stores name, redshift, distance modulus, distance modulus error\n", "#Let's convert from distance modulus to distance\n", "#See here https://en.wikipedia.org/wiki/Distance_modulus\n", "def distanceconv(iMu):\n", "    power=iMu/5+1\n", "    return 10**power\n", "\n", "# and the same for uncertainty in distance modulus to uncertainty in distance\n", "def distanceconverr(iMu,iMuErr):\n", "    power=iMu/5+1\n", "    const=math.log(10)/5.\n", "    return const*(10**power)*iMuErr\n", "\n", "redshift=[]\n", "distance=[]\n", "distance_err=[]\n", "# read in the data\n", "with open(label,'r') as csvfile:\n", "    plots = csv.reader(csvfile, delimiter='\\t')\n", "    # name, redshift, distance modulus, distance modulus uncertainty\n", "    for row in plots:\n", "        redshift.append(float(row[1]))\n", "        distance.append(distanceconv(float(row[2])))\n", "        distance_err.append(distanceconverr(float(row[2]),float(row[3])))\n", "\n", "plt.xlabel('redshift(z)', fontsize=15) #Label x\n", "plt.ylabel('distances(parsec)', fontsize=15)#Label y\n", "# plot with errorbars\n", "plt.errorbar(redshift,distance,yerr=distance_err,marker='.',linestyle = 'None', color = 'black')\n", "plt.show()"]}, {"cell_type": "code", "execution_count": null, "id": "5b8265b8", "metadata": {"tags": ["learner", "py", "lect_02", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L4.2-runcell02\n", "\n", "#Now let's zoom in on the small redshift data.\n", "def load(iLabel,iZMax):\n", "    # loads data from file iLabel with redshift below iZMax:\n", "    redshift=[]\n", "    distance=[]\n", "    distance_err=[]\n", "    with open(label,'r') as csvfile:\n", "        plots = csv.reader(csvfile, delimiter='\\t')\n", "        for row in plots:\n", "            if float(row[1]) > iZMax:\n", "                continue\n", "            redshift.append(float(row[1]))\n", "            distance.append(distanceconv(float(row[2])))\n", "            distance_err.append(distanceconverr(float(row[2]),float(row[3])))\n", "    return np.array(redshift), np.array(distance), np.array(distance_err)\n", "\n", "redshift,distance,distance_err = load(label,0.1)\n", "plt.xlabel('redshift(z)', fontsize=15) #Label x\n", "plt.ylabel('distances(parsec)', fontsize=15)#Label y\n", "plt.errorbar(redshift,distance,yerr=distance_err,marker='.',linestyle = 'None', color = 'black')\n", "plt.show()"]}, {"cell_type": "markdown", "id": "5eee7b1b", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='exercises_4_2'></a>     \n", "\n", "| [Top](#section_4_0) | [Restart Section](#section_4_2) | [Next Section](#section_4_3) |\n"]}, {"cell_type": "markdown", "id": "250b2631", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-4.2.1 Complete the Derivation</span>\n", "\n", "In the derivation shown previously, how did we get the strange formulation in that second step? Part of the answer is that we could add a term $\\sum_{i} x_{i}\\bar{x} - \\bar{x}^2$ to the denominator because $\\sum_{i} x_{i}\\bar{x} - \\bar{x}^2 = 0$. This helped simplify the derivation. Below, we show a partial proof that $\\sum_{i} x_{i}\\bar{x} - \\bar{x}^2 = 0$:\n", "\n", "<br>\n", "\n", "$$\n", "\\begin{eqnarray} \n", "\\sum_{i} (x_{i}\\bar{x}-\\bar{x}^{2}) & = & \\sum_{i} x_{i}\\bar{x} -\\sum_{i} \\bar{x}^{2} \\\\\n", "& = & [\\mathrm{insert\\,missing\\,step}]\\\\\n", "& = & 0 \\\\\n", "\\end{eqnarray}\n", "$$  \n", "\n", "<br>\n", "\n", "In order for this to be true, what must the value of $\\sum_{i} x_{i}\\bar{x}$ be? Express your answer in terms of `N` for $N$ and `xbar` for $\\bar{x}$.\n"]}, {"cell_type": "markdown", "id": "13c0ed13", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_4_3'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L4.3 Linear Regression: Coding Example</h2>  \n", "\n", "| [Top](#section_4_0) | [Previous Section](#section_4_2) | [Exercises](#exercises_4_3) | [Next Section](#section_4_4) |\n"]}, {"cell_type": "code", "execution_count": null, "id": "9a66e14d", "metadata": {"tags": ["learner", "py", "lect_03", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L4.3-runcell01\n", "\n", "#Let's run the regression again\n", "def variance(isamples):\n", "    mean=isamples.mean()\n", "    n=len(isamples)\n", "    tot=0\n", "    for pVal in isamples:\n", "        tot+=(pVal-mean)**2\n", "    return tot/n\n", "\n", "def covariance(ixs,iys):\n", "    meanx=ixs.mean()\n", "    meany=iys.mean()\n", "    n=len(ixs)\n", "    tot=0\n", "    for i0 in range(len(ixs)):\n", "        tot+=(ixs[i0]-meanx)*(iys[i0]-meany)\n", "    return tot/n\n", "\n", "def linear(ix,ia,ib):\n", "    return ia*ix+ib\n", "\n", "def regress(redshift,distance):\n", "    #Let's regress\n", "    var=variance(redshift)\n", "    cov=covariance(redshift,distance)\n", "    A=cov/var\n", "    b=distance.mean()-A*redshift.mean()\n", "    #Done!\n", "    return A,b\n", "\n", "def plotAll(redshift,distance,distance_err,A,b):\n", "    #now let's plot it\n", "    xvals = np.linspace(0,0.1,100)\n", "    yvals = []\n", "    for pX in xvals:\n", "        yvals.append(linear(pX,A,b))\n", "\n", "    #Plot the line\n", "    plt.plot(xvals,yvals)\n", "    plt.errorbar(redshift,distance,yerr=distance_err,marker='.',linestyle = 'None', color = 'black')\n", "    plt.xlabel('redshift(z)', fontsize=15) #Label x\n", "    plt.ylabel('distances(parsec)', fontsize=15)#Label y\n", "    plt.show()\n", "    #Print it out\n", "    print(\"Hubbles Constant:\",1e6*3e5/A,\"intercept\",b)#Note 1e6 is from pc to Mpc and 3e5 is c in km/s\n", "\n", "A,b=regress(redshift,distance)\n", "plotAll(redshift,distance,distance_err,A,b)"]}, {"cell_type": "markdown", "id": "e96d1703", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='exercises_4_3'></a>     \n", "\n", "| [Top](#section_4_0) | [Restart Section](#section_4_3) | [Next Section](#section_4_4) |\n"]}, {"cell_type": "markdown", "id": "4f5b1717", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-4.3.1 Uncertainty in $h_{0}$</span>\n", "\n", "Now we wish to obtain the uncertainty on $h_{0}$, which we call $\\sigma_{h_0}$, where we will use the definition:\n", "\n", "$$h_{0} =\\frac{1}{A}\\frac{\\rm 3\\times10^{5}~(km/s)}{10^{-6}} $$\n", "\n", "\n", "Assume that we know the uncertainty on $A$, which we call $\\sigma_{A}$. What is the value of $\\sigma_{h_0}$? Express your answer in terms of `A` for $A$, `h_0` for $h_{0}$, and `sigma_A` for $\\sigma_{A}$.\n", "\n", "**Hint: To answer this question, you need to use propagation of uncertainty!**"]}, {"cell_type": "markdown", "id": "297aab3b", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": [">#### Follow-up 4.3.1a (ungraded)\n", ">\n", ">What is the uncertainty $\\sigma_{h_0}$ written only in terms of the slope $A$ and its uncertainty? \n"]}, {"cell_type": "markdown", "id": "f309d6fe", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_4_4'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L4.4 Weighted Linear Regression</h2>  \n", "\n", "| [Top](#section_4_0) | [Previous Section](#section_4_3) | [Exercises](#exercises_4_4) | [Next Section](#section_4_5) |\n"]}, {"cell_type": "code", "execution_count": null, "id": "b339a82f", "metadata": {"tags": ["learner", "py", "lect_04", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L4.4-runcell01\n", "\n", "weights=[]\n", "for err in distance_err:\n", "    weights.append(1./err**2)\n", "    \n", "weights = np.array(weights)\n", "\n", "#Now let's do it with weights\n", "def variance_w(isamples,iweights):\n", "    mean=np.average(isamples,weights=iweights)\n", "    sumw=np.sum(iweights)\n", "    tot=0\n", "    for i0 in range(len(isamples)):\n", "        tot+=iweights[i0]*(isamples[i0]-mean)**2\n", "    return tot/sumw\n", "\n", "def covariance_w(ixs,iys,iweights):\n", "    meanx=np.average(ixs,weights=iweights)\n", "    meany=np.average(iys,weights=iweights)\n", "    sumw=np.sum(iweights)\n", "    tot=0\n", "    for i0 in range(len(ixs)):\n", "        tot+=iweights[i0]*(ixs[i0]-meanx)*(iys[i0]-meany)\n", "    return tot/sumw\n", "\n", "def regress_w(redshift,weights,distance):\n", "    varw=variance_w(redshift,weights)\n", "    covw=covariance_w(redshift,distance,weights)\n", "    Aw=covw/varw\n", "    bw=np.average(distance,weights=weights)-Aw*np.average(redshift,weights=weights)\n", "    return Aw,bw\n", "\n", "Aw,bw=regress_w(redshift,weights,distance)\n", "plotAll(redshift,distance,distance_err,Aw,bw)"]}, {"cell_type": "markdown", "id": "78fcbf0e", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='exercises_4_4'></a>     \n", "\n", "| [Top](#section_4_0) | [Restart Section](#section_4_4) | [Next Section](#section_4_5) |\n"]}, {"cell_type": "markdown", "id": "39321475", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-4.4.1 Effects of Changing Uncertainty I</span>\n", "\n", "Let's imagine, for no particular reason, that our uncertainty on any given observation is multiplied by a factor of the redshift. That is, we define a new $\\sigma_{i}^{new} = z \\sigma_{i}^{old}$. How would the Hubble constant found by fitting the data with these modified uncertainties change, compared to the case when $\\sigma_{i}^{old}$ was used? Choose from the following options:\n", "\n", "- The fitted value of the Hubble constant would increase\n", "- The fitted value of the Hubble constant would decrease\n", "- The fitted value of the Hubble constant would stay the same\n", "\n", "You can modify the starting code below to find the answer."]}, {"cell_type": "code", "execution_count": null, "id": "635954f4", "metadata": {"tags": ["py", "draft", "learner_chopped"]}, "outputs": [], "source": ["#>>>EXERCISE: L4.4.1\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "#The code below scales the uncertainties being used in our weighted regression. \n", "\n", "import numpy as np\n", "\n", "redshift,distance,distance_err = load(label,0.1)\n", "\n", "# scale the errors by the redshift\n", "new_errors = #YOUR CODE HERE\n", "\n", "# translate these scaled uncertainties into new weights for regression (1/sigma^2 for each)\n", "weights = 1 / (new_errors * new_errors)\n", "\n", "Aw,bw=regress_w(redshift,weights,distance)\n", "plotAll(redshift,distance,new_errors,Aw,bw)"]}, {"cell_type": "markdown", "id": "f6895d01", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-4.4.2 Effects of Changing Uncertainty II</span>\n", "\n", "What happens if I made a mistake and all of the errors are 100 times larger than what I thought, such that $\\sigma_{i}^{new} = 100 \\sigma_{i}^{old}$? How would the Hubble constant found by fitting the data with these modified uncertainties change, compared to the case when $\\sigma_{i}^{old}$ was used?\n", "\n", "Again, choose from the following options:\n", "\n", "- The fitted value of the Hubble constant would increase\n", "- The fitted value of the Hubble constant would decrease\n", "- The fitted value of the Hubble constant would stay the same\n", "\n", "You can modify the starting code below to find the answer."]}, {"cell_type": "code", "execution_count": null, "id": "55c2f7ec", "metadata": {"tags": ["draft", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>EXERCISE: L4.4.2\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "import numpy as np\n", "\n", "redshift,distance,distance_err = load(label,0.1)\n", "\n", "# scale the errors by the redshift\n", "new_errors = #YOUR CODE HERE\n", "\n", "# translate these scaled uncertainties into new weights for regression (1/sigma^2 for each)\n", "weights = 1 / (new_errors * new_errors)\n", "\n", "Aw,bw=regress_w(redshift,weights,distance)\n", "plotAll(redshift,distance,new_errors,Aw,bw)"]}, {"cell_type": "markdown", "id": "f4c0ef0f", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_4_5'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L4.5 Minimization without the Math</h2>  \n", "\n", "| [Top](#section_4_0) | [Previous Section](#section_4_4) | [Exercises](#exercises_4_5) | [Next Section](#section_4_6) |\n"]}, {"cell_type": "code", "execution_count": null, "id": "ffe5a404", "metadata": {"tags": ["learner", "py", "lect_05", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L4.5-runcell01\n", "\n", "from scipy import stats\n", "\n", "#Now lets do the same thing with scipy\n", "slope, intercept, r_value, p_value, std_err = stats.linregress(redshift,distance)\n", "print(\"UnWeighted Fit:\",\"Hubbles Constant:\",1e6*3e5/slope,\"intercept\",intercept)\n", "\n", "#Now the weighted version; we use scikit-learn, a package dedicated to fitting\n", "from sklearn.linear_model import LinearRegression\n", "model = LinearRegression()\n", "redshifthack=np.reshape(redshift,(len(redshift),1))#line to get the fit code to work\n", "model.fit(redshifthack,distance,weights)\n", "slope=model.coef_\n", "const=model.intercept_\n", "print(\"Weighted Fit:\",\"Hubbles Constant:\",1e6*3e5/slope,\"intercept\",const)"]}, {"cell_type": "code", "execution_count": null, "id": "dccfb9d0", "metadata": {"tags": ["learner", "py", "lect_05", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L4.5-runcell02\n", "\n", "#scipy provides the general tool that optimizes\n", "from scipy import optimize as opt\n", "\n", "#First an example minimizing a simple polynomial\n", "def f(x):\n", "    return x**4 + 3*(x-2)**3 - 15*(x)**2 + 1\n", "sol=opt.minimize_scalar(f, method='Brent')\n", "x = np.linspace(-8, 5, 100)\n", "\n", "plt.xlabel('x', fontsize=15) #Label x\n", "plt.ylabel('f(x)', fontsize=15)#Label y\n", "plt.plot(x, f(x));\n", "plt.axvline(sol.x, c='red')\n", "plt.show()"]}, {"cell_type": "code", "execution_count": null, "id": "000a886a", "metadata": {"tags": ["learner", "py", "lect_05", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L4.5-runcell03\n", "\n", "# scipy offers a tool for fitting a generic curve to data.\n", "# It's using the same sort of optimizer, to optimize its choice of error metric\n", "from scipy.optimize import curve_fit \n", "\n", "# We define a curve functional form\n", "def f(x,a,b):\n", "    return a*x+b\n", "\n", "popt, pcov = curve_fit(f, redshift,distance) # returns optimal values of a, b in popt\n", "perr = np.sqrt(np.diag(pcov))\n", "print(\"Unweighted Hubbles Constant:\",1e6*3e5/popt[0],\"+/-\",(1e6*3e5/popt[0]/popt[0])*perr[0],\"intercept\",popt[1],\"+/-\",perr[1])\n", "\n", "#Now let's do it weighted by uncertainties, by passing them in the \"sigma\" optional argument\n", "popt, pcov = curve_fit(f, redshift,distance,sigma=distance_err)\n", "perr = np.sqrt(np.diag(pcov))\n", "print(\"Weighted Hubbles Constant:\",1e6*3e5/popt[0],\"+/-\",(1e6*3e5/popt[0]/popt[0])*perr[0],\"intercept\",popt[1],\"+/-\",perr[1])"]}, {"cell_type": "markdown", "id": "d2c654c1", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='exercises_4_5'></a>     \n", "\n", "| [Top](#section_4_0) | [Restart Section](#section_4_5) | [Next Section](#section_4_6) |\n"]}, {"cell_type": "markdown", "id": "8f073d26", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-4.5.1 Finding the Minimum of a Potential</span>\n", "\n", "One of the most important distributions used in high energy physics is the Mexcian Hat Potential distribution. A generic form is given by: \n", "\n", "$$\n", "\\begin{eqnarray} \n", "  V(x,y) & = & (x^2+y^2)^2-A(x^2+y^2)\n", "\\end{eqnarray}\n", "$$\n", "\n", "Here, let's consider a 1D version where we fix $y$ and $A$, so we can write this as: \n", "\n", "$$\n", "\\begin{equation}\n", "f(x) = (x-2)^{2}(x+2)^2-24x^2\n", "\\end{equation}\n", "$$\n", "\n", "**Complete the code below to find ONE minimum of this new potential, using bounded minimization. What is the value of the minimum that you obtained? Enter your answer as a number with precision 1e-3.**\n", "\n", "*Note, in the next video we will look at a potential with slightly different values, and discuss some aspects of the minimization.*\n", "\n", "*Check out the `scipy.optimize.minimize` documentation here: https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html*\n", "\n", "*Also, for your interest, similarly shaped potentials include the Ricker potential, which you can read more about here: https://en.wikipedia.org/wiki/Ricker_wavelet*"]}, {"cell_type": "code", "execution_count": null, "id": "feed6a57", "metadata": {"tags": ["draft", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>EXERCISE: L4.5.1\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "from scipy import optimize as opt\n", "\n", "def f(x):\n", "    return #YOUR CODE HERE\n", "\n", "sol=opt.minimize_scalar(f, bounds=(-6,6), method='bounded')\n", "x = np.linspace(-6, 6, 100)\n", "\n", "print('min value over the array x:',sol.x)\n", "\n", "plt.xlabel('x', fontsize=15) #Label x\n", "plt.ylabel('f(x)', fontsize=15)#Label y\n", "plt.plot(x, f(x));\n", "plt.axvline(sol.x, c='red')\n", "plt.show()\n"]}, {"cell_type": "markdown", "id": "ebb1bad3", "metadata": {"tags": ["md", "learner", "learner_chopped"]}, "source": [">#### Follow-up 4.5.1a (ungraded)\n", ">\n", ">What makes this minimum particularly interesting? "]}, {"cell_type": "markdown", "id": "76c3d071", "metadata": {"tags": ["md", "learner", "learner_chopped"]}, "source": [">#### Follow-up 4.5.1b (ungraded)\n", ">\n", ">Change the bounds of the minimizer. Does this lead to different results?"]}, {"cell_type": "markdown", "id": "ff15a1b2", "metadata": {"tags": ["md", "learner", "learner_chopped"]}, "source": [">#### Follow-up 4.5.1c (ungraded)\n", ">\n", ">Try changing the function to create an asymmetric potential (for example, add $x$). Does the minimizer find the global mimimum, or is it stuck in the local minimum? Do different methods (for instance `brent`) do a better or worse job?"]}, {"cell_type": "markdown", "id": "c1fdd310", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_4_6'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L4.6 Gradient Descent</h2>  \n", "\n", "| [Top](#section_4_0) | [Previous Section](#section_4_5) | [Exercises](#exercises_4_6) | [Next Section](#section_4_7) |\n"]}, {"cell_type": "code", "execution_count": null, "id": "5ec04fac", "metadata": {"tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L4.6-slides\n", "\n", "from IPython.display import IFrame\n", "IFrame(src='https://mitx-8s50.github.io/slides/L04/slides_L04_06.html', width=975, height=550)"]}, {"cell_type": "code", "execution_count": null, "id": "58e0417b", "metadata": {"tags": ["learner", "py", "lect_06", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L4.6-runcell01\n", "\n", "#Let's find the minimum of x^2\n", "def f(x):\n", "    return x**2\n", "# We need to define the gradient of f\n", "def grad(x):\n", "    return 2*x\n", "\n", "#Simple gradient descent (this one is not physics-based, but rather just steps along the gradient)\n", "#Starting on x, move in the direction of negative gradient, scaled by alpha (delta t from earlier)\n", "def gd(x, grad, alpha, max_iter=10):\n", "    xs = np.zeros(1 + max_iter)\n", "    xs[0] = x #start at x\n", "    for i in range(max_iter):\n", "        x = x - alpha * grad(x)\n", "        xs[i+1] = x\n", "    return xs\n", "\n", "#Physics based gradient descent, which comes from our equations above\n", "#Now will update both position and velocity in timesteps alpha\n", "def gd_momentum_nodamp(x, grad, alpha, max_iter=10):\n", "    xs = np.zeros(1 + max_iter)\n", "    xs[0] = x\n", "    v = 0\n", "    for i in range(max_iter):\n", "        v = v - alpha*grad(x)\n", "        x = x + alpha * v  - 0.5 * alpha * alpha*grad(x)\n", "        xs[i+1] = x\n", "    return xs\n", "\n", "def plotGDAlgo(iGDAlgo,alpha=0.1,beta=None,x0=1):\n", "    if beta is not None:\n", "        xs = iGDAlgo(x0, grad, alpha, beta)\n", "    else:\n", "        xs = iGDAlgo(x0, grad, alpha)\n", "    xp = np.linspace(-1.2, 1.2, 100)\n", "\n", "    #Now just plotting code\n", "    plt.xlabel('x', fontsize=15) #Label x\n", "    plt.ylabel('f(x)', fontsize=15)#Label y\n", "    plt.plot(xp, f(xp)) #function\n", "    plt.plot(xs, f(xs), 'o-', c='red') #varied function\n", "    for i, (x, y) in enumerate(zip(xs, f(xs)), 1):\n", "        plt.text(x, y+0.2, i,bbox=dict(facecolor='yellow', alpha=0.5), fontsize=14)\n", "    plt.show()    \n", "\n", "plotGDAlgo(gd)\n", "plotGDAlgo(gd,0.95) #large timestep alpha\n", "plotGDAlgo(gd_momentum_nodamp,0.95) #Large timestep, with momentum"]}, {"cell_type": "code", "execution_count": null, "id": "37dd0ae5", "metadata": {"tags": ["learner", "py", "lect_06", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L4.6-runcell02\n", "\n", "#Now will update both momentum and velocity in timesteps alpha but with a dilution factor of beta\n", "def gd_momentum(x, grad, alpha, beta=0.9, max_iter=10):\n", "    xs = np.zeros(1 + max_iter)\n", "    xs[0] = x\n", "    v = 0\n", "    for i in range(max_iter):\n", "        v = beta*v + (1-beta)*grad(x) # beta effectively scales how much the velocity is affected by the local gradient\n", "        vc = v/(1+beta**(i+1)) # and also controls the damping of the velocity\n", "        x = x - alpha * vc\n", "        xs[i+1] = x\n", "    return xs\n", "\n", "plotGDAlgo(gd)\n", "plotGDAlgo(gd,0.95) #large timestep alpha\n", "plotGDAlgo(gd_momentum_nodamp,0.95) #Large timestep\n", "plotGDAlgo(gd_momentum,0.95) #Large timestep\n", "plotGDAlgo(gd_momentum,0.95,beta=0.7) #Large timestep\n", "plotGDAlgo(gd_momentum,0.95,beta=0.9) #Large timestep"]}, {"cell_type": "code", "execution_count": null, "id": "a8469b7f", "metadata": {"tags": ["learner", "py", "lect_06", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L4.6-runcell03\n", "\n", "#Now let's do this with a general tool that optimizes\n", "import scipy.linalg as la\n", "def f(x):\n", "    return x**4 + 3*(x-2)**3 - 15*(x)**2 + 1\n", "\n", "def fprime(x):\n", "    return 4*x**3 + 9*(x-2)**2 - 30*(x)\n", "\n", "#Where is our minimizer\n", "def custmin(fun, x0, args=(), maxfev=None, alpha=0.0002,\n", "        maxiter=100000, tol=1e-10, callback=None, **options):\n", "    \"\"\"Implements simple gradient descent for the function above.\"\"\"\n", "    bestx = x0\n", "    bestf = fun(x0)\n", "    funcalls = 1\n", "    niter = 0\n", "    improved = True\n", "    stop = False\n", "\n", "    while improved and not stop and niter < maxiter:\n", "        niter += 1\n", "        # the next 2 lines are gradient descent\n", "        step = alpha * fprime(bestx)\n", "        bestx = bestx - step\n", "        \n", "        bestf = fun(bestx)\n", "        funcalls += 1\n", "\n", "        if la.norm(step) < tol:\n", "            improved = False\n", "        if callback is not None:\n", "            callback(bestx)\n", "        if maxfev is not None and funcalls >= maxfev:\n", "            stop = True\n", "            break\n", "\n", "    return opt.OptimizeResult(fun=bestf, x=bestx, nit=niter,nfev=funcalls, success=(niter > 1))\n", "\n", "def reporter(p):\n", "    \"\"\"Reporter function to capture intermediate states of optimization.\"\"\"\n", "    global ps\n", "    ps.append(p)\n", "\n", "x0=-7\n", "x0 = np.array([x0])\n", "ps = [x0]\n", "sol=opt.minimize(f, x0, method=custmin, callback=reporter)\n", "x = np.linspace(-8, 5, 100)\n", "plt.xlabel('x', fontsize=15) #Label x\n", "plt.ylabel('f(x)', fontsize=15)#Label y\n", "plt.plot(x, f(x));\n", "plt.axvline(sol.x, c='red')\n", "plt.show()\n", "\n", "#Now let's trick it\n", "x0=5\n", "x0 = np.array([x0])\n", "ps = [x0]\n", "sol=opt.minimize(f, x0, method=custmin, callback=reporter)\n", "x = np.linspace(-8, 5, 100)\n", "plt.xlabel('x', fontsize=15) #Label x\n", "plt.ylabel('f(x)', fontsize=15)#Label y\n", "plt.plot(x, f(x));\n", "plt.axvline(sol.x, c='red')\n", "plt.show()"]}, {"cell_type": "markdown", "id": "047631da", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='exercises_4_6'></a>     \n", "\n", "| [Top](#section_4_0) | [Restart Section](#section_4_6) | [Next Section](#section_4_7) |\n"]}, {"cell_type": "markdown", "id": "54938451", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-4.6.1</span>\n", "\n", "Let's say you were stuck using the simple minimizer that we developed above. Let's also assume that the global mimimum is within some range that you know. What could you do to make sure your minimizer finds the global minimum, instead of getting stuck in a local minimum? Select all that apply:\n", "\n", "- Use a larger number of iterations.\n", "- Use a smaller number for the tolerance.\n", "- Choose random starting points for the minimizer, and save the result that yields the lowest value.\n", "- Restart your minimizer once it finds it's first minimum, and use a new starting point that is some pre-defined (perhaps random) distance away from the minimum that is found; do this several times and save the result that yields the lowest value.\n"]}, {"cell_type": "markdown", "id": "fc0a1189", "metadata": {"tags": ["md", "learner", "learner_chopped"]}, "source": [">#### Follow-up 4.6.1a (ungraded)\n", ">\n", ">Try some of the strategies above. What works? What does not?"]}, {"cell_type": "markdown", "id": "f85adb92", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_4_7'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L4.7 Fitting the Full Range of Hubble Data</h2>  \n", "\n", "| [Top](#section_4_0) | [Previous Section](#section_4_6) | [Exercises](#exercises_4_7) | [Next Section](#section_4_8) |\n"]}, {"cell_type": "code", "execution_count": null, "id": "df377154", "metadata": {"tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L4.7-slides\n", "\n", "from IPython.display import IFrame\n", "IFrame(src='https://mitx-8s50.github.io/slides/L04/slides_L04_07.html', width=975, height=550)"]}, {"cell_type": "code", "execution_count": null, "id": "89258c61", "metadata": {"tags": ["learner", "py", "lect_07", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L4.7-runcell01\n", "\n", "#Ok let's do a custom minimization of our fit function with gradient descent\n", "#Note that since we are fitting two parameters we need to do this in 2D\n", "redshift,distance,distance_err = load(label,10)\n", "weights=np.array([])\n", "for pVal in distance_err:\n", "    weights = np.append(weights,1./pVal/pVal)\n", "\n", "def f(x,h0,q):\n", "    val=x*(1e6*3e5/h0)*(1 + ((1-q)*0.5)*x)\n", "    return val\n", "\n", "def fprime(x,h0,q):\n", "    der=np.zeros(2)\n", "    der[0]=-1*x*(1e6*3e5/h0/h0)*(1 + ((1-q)*0.5)*x)\n", "    der[1]=x*(1e6*3e5/h0)*(-0.5*x)\n", "    return der\n", "\n", "def algof(inputs):\n", "    d=0\n", "    for i0 in range(len(redshift)):\n", "        yhat=f(redshift[i0],inputs[0],inputs[1])\n", "        pD=(distance[i0]-yhat)**2\n", "        #pD=pD/dmean\n", "        d+=pD*weights[i0]\n", "    return d\n", "\n", "def algofprime(inputs):\n", "    d=np.zeros(2)\n", "    for i0 in range(len(redshift)):\n", "        yhat=f(redshift[i0],inputs[0],inputs[1])\n", "        pD=2*(distance[i0]-yhat)\n", "        yhatprime=fprime(redshift[i0],inputs[0],inputs[1])\n", "        #print(yhatprime,pD,d)\n", "        #pD=pD/dmean\n", "        d=d+(yhatprime*pD)*weights[i0]\n", "    return d\n", "\n", "def custmin(fun, x0, args=(), maxfev=None, alpha=0.0001,maxiter=10000, tol=1e-10, callback=None, **options):\n", "    \"\"\"Implements simple gradient descent for the function above.\"\"\"\n", "    bestx = x0\n", "    bestf = fun(x0)\n", "    funcalls = 1\n", "    niter = 0\n", "    improved = True\n", "    stop = False\n", "\n", "    while improved and not stop and niter < maxiter:\n", "        niter += 1\n", "        if niter % 1000 == 0: \n", "            print(niter,bestx)\n", "        # the next 2 lines are gradient descent\n", "        step = alpha * algofprime(bestx)\n", "        #print(bestx,step)\n", "        bestx = bestx + step\n", "\n", "        bestf = fun(bestx)\n", "        funcalls += 1\n", "\n", "        if la.norm(step) < tol:\n", "            improved = False\n", "        if callback is not None:\n", "            callback(bestx)\n", "        if maxfev is not None and funcalls >= maxfev:\n", "            stop = True\n", "            break\n", "\n", "    return opt.OptimizeResult(fun=bestf, x=bestx, nit=niter,nfev=funcalls, success=(niter > 1))\n", "\n", "def reporter(p):\n", "    \"\"\"Reporter function to capture intermediate states of optimization.\"\"\"\n", "    global ps\n", "    ps.append(p)\n", "\n", "#tmp=algofprime([70,0.03])\n", "x0 = np.array([40,0])\n", "ps = [x0]\n", "sol0=opt.minimize(algof, x0, method=custmin, callback=reporter)\n", "print(sol0)\n", "print()\n", "\n", "sol1=opt.minimize(algof, x0)#, method=custmin, callback=reporter)\n", "print(sol1)\n", "    "]}, {"cell_type": "code", "execution_count": null, "id": "a4b4caf6", "metadata": {"tags": ["learner", "py", "lect_07", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L4.7-runcell02\n", "\n", "#Ok let's do a custom minimization of our fit function with gradient descent\n", "#Note that since we are fitting two parameters we need to do this in 2D\n", "def f(x,h0,q):\n", "    val=x*(1e6*3e5/h0)*(1 + ((1-q)*0.5)*x)\n", "    return val\n", "\n", "def algof(inputs):\n", "    d=0\n", "    for i0 in range(len(redshift)):\n", "        yhat=f(redshift[i0],inputs[0],inputs[1])\n", "        pD=(distance[i0]-yhat)**2\n", "        #pD=pD/dmean\n", "        d+=pD*weights[i0]\n", "    return d\n", "\n", "def algofprime(inputs):\n", "    delta1=np.array([inputs[0]*0.001,0])\n", "    delta2=np.array([0,inputs[1]*0.001])\n", "    dp11=algof(inputs-delta1)\n", "    dp21=algof(inputs-delta2)\n", "    dp12=algof(inputs+delta1)\n", "    dp22=algof(inputs+delta2)\n", "    deriv=np.array([0,0])\n", "    deriv[0]=(dp11-dp12)/(2*delta1[0])\n", "    deriv[1]=(dp21-dp22)/(2*delta2[1])\n", "    return deriv\n", "\n", "def algofprime2(inputs):\n", "    d=np.zeros(2)\n", "    for i0 in range(len(redshift)):\n", "        yhat=f(redshift[i0],inputs[0],inputs[1])\n", "        pD=2*(distance[i0]-yhat)\n", "        yhatprime=fprime(redshift[i0],inputs[0],inputs[1])\n", "        #print(yhatprime,pD,d)\n", "        #pD=pD/dmean\n", "        d=d+(yhatprime*pD)*weights[i0]\n", "    return d\n", "\n", "def custmin(fun, x0, args=(), maxfev=None, alpha=0.01,maxiter=10000, tol=1e-10, callback=None, **options):\n", "    \"\"\"Implements simple gradient descent for the function above.\"\"\"\n", "    bestx = x0\n", "    bestf = fun(x0)\n", "    funcalls = 1\n", "    niter = 0\n", "    improved = True\n", "    stop = False\n", "\n", "    while improved and not stop and niter < maxiter:\n", "        niter += 1\n", "        if niter % 1000 == 0: \n", "            print(niter,bestx)\n", "        # the next 2 lines are gradient descent\n", "        step = alpha * algofprime(bestx)\n", "        #print(bestx,step)\n", "        bestx = bestx + step\n", "\n", "        bestf = fun(bestx)\n", "        funcalls += 1\n", "\n", "        if la.norm(step) < tol:\n", "            improved = False\n", "        if callback is not None:\n", "            callback(bestx)\n", "        if maxfev is not None and funcalls >= maxfev:\n", "            stop = True\n", "            break\n", "\n", "    return opt.OptimizeResult(fun=bestf, x=bestx, nit=niter,nfev=funcalls, success=(niter > 1))\n", "\n", "def reporter(p):\n", "    \"\"\"Reporter function to capture intermediate states of optimization.\"\"\"\n", "    global ps\n", "    ps.append(p)\n", "\n", "x0 = np.array([40,0.1])\n", "ps = [x0]\n", "sol0=opt.minimize(algof, x0, method=custmin, callback=reporter)\n", "print(sol0)\n", "print()\n", "\n", "sol1=opt.minimize(algof, x0)#, method=custmin, callback=reporter)\n", "print(sol1)"]}, {"cell_type": "markdown", "id": "9ab24fee", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='exercises_4_7'></a>     \n", "\n", "| [Top](#section_4_0) | [Restart Section](#section_4_7) | [Next Section](#section_4_8) |\n"]}, {"cell_type": "markdown", "id": "6631afab", "metadata": {"tags": ["md", "learner", "learner_chopped"]}, "source": [">#### Follow-up 4.7.1a (ungraded)\n", ">\n", ">The `custmin` minimizer in the code cell `L4.7-runcell02` is not working properly. Can you get it to work? Is it faster than defining the derivative analytically?"]}, {"cell_type": "markdown", "id": "620cf448", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_4_8'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L4.8 Fitting with lmfit</h2>     \n", "\n", "| [Top](#section_4_0) | [Previous Section](#section_4_7) | [Exercises](#exercises_4_8) |\n"]}, {"cell_type": "code", "execution_count": null, "id": "b0bcd953", "metadata": {"tags": ["learner", "py", "lect_08", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L4.8-runcell01\n", "\n", "import lmfit\n", "\n", "weights=np.array([])\n", "for pVal in distance_err:\n", "    #weighted fits in lmfit require you to ass in 1/sigma (not 1/sigma^2\n", "    weights = np.append(weights,1./pVal)\n", "\n", "#Clearly that's not working so lets use an approximation to this\n", "def f(x,h0,q):\n", "    val=x*(1e6*3e5/h0)*(1 + ((1-q)*0.5)*x)\n", "    return val\n", "\n", "model  = lmfit.Model(f)\n", "p = model.make_params(h0=50,q=0)\n", "result = model.fit(data=distance, params=p, x=redshift, weights=weights)\n", "lmfit.report_fit(result)\n", "plt.figure()\n", "result.plot();"]}, {"cell_type": "markdown", "id": "d5062262", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-4.8.1 Modified Hubble Fit</span>\n", "\n", "Add a constant term to the fit, and see if the fitted parameter is consistent with zero. Your fit function should have the following form:   \n", "\n", "$$ f(x) = c + \\frac{x}{h_{0}}\\left(1+\\frac{1-q}{2}x\\right) $$\n", "\n", "Run the fit with this modified form. What is your value for the constant $c$ in units of megaparsecs (Mpc)? How does this compare with the typical distance between galaxies?\n", "\n", "Report your answer with precision 1e-2."]}, {"cell_type": "code", "execution_count": null, "id": "b80cbcdf", "metadata": {"tags": ["draft", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>EXERCISE: L4.8.1\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "def f(x,h0,q,c):\n", "    return #YOUR CODE HERE\n", "\n", "\n", "###FILL IN THE CODE BELOW###\n", "\n", "model = #YOUR CODE HERE\n", "p = #YOUR CORE HERE \n", "\n", "######\n", "\n", "model  = lmfit.Model(f)\n", "p = model.make_params(h0=50,q=0,c=0)\n", "\n", "result = model.fit(data=distance, params=p, x=redshift, weights=weights)\n", "lmfit.report_fit(result)\n", "plt.figure()\n", "result.plot();\n"]}], "metadata": {"celltoolbar": "Tags", "kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}