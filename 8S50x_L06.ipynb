{"cells": [{"cell_type": "markdown", "id": "6f16d094", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<hr style=\"height: 1px;\">\n", "<i>This notebook was authored by the 8.S50x Course Team, Copyright 2022 MIT All Rights Reserved.</i>\n", "<hr style=\"height: 1px;\">\n", "<br>\n", "\n", "<h1>Lesson 6: Confidence</h1>\n"]}, {"cell_type": "markdown", "id": "e2ec7aa8", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_6_0'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L6.0 Overview</h2>\n"]}, {"cell_type": "markdown", "id": "005484fa", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<h3>Navigation</h3>\n", "\n", "<table style=\"width:100%\">\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_6_1\">L6.1 Introduction to Confidence Intervals</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_6_1\">L6.1 Exercises</a></td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_6_2\">L6.2 z-Scores and Confidence Intervals for Other Distributions</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_6_2\">L6.2 Exercises</a></td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_6_3\">L6.3 Another Example and Rules of Significance</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_6_3\">L6.3 Exercises</a></td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_6_4\">L6.4 Moments of Distributions and Mapping</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_6_4\">L6.4 Exercises</a></td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_6_5\">L6.5 Monte Carlo Integration</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_6_5\">L6.5 Exercises</a></td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_6_6\">L6.6 Returning to Fitting Supernova Data</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_6_6\">L6.6 Exercises</a></td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_6_7\">L6.7 Fitting with a More Accurate Model</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_6_7\">L6.7 Exercises</a></td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_6_8\">L6.8 Fit to Full Cosmological Model</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_6_8\">L6.8 Exercises</a></td>\n", "    </tr>\n", "</table>\n", "\n"]}, {"cell_type": "markdown", "id": "f148569a", "metadata": {"tags": ["learner", "catsoop_00", "md"]}, "source": ["<h3>Learning Objectives</h3>\n", "\n", "In this Lesson, we will spend some time further understanding uncertainty in more complicated scenarios, exploring the following objectives:\n", "\n", "\n", "- How do we quote significance?\n", "- Asymmetric distributions\n", "- Moments of distributions\n", "- Numerical Integration\n", "- A more sophisticated fit\n", "- What have we learned about the properties of the Universe"]}, {"cell_type": "markdown", "id": "61244edd", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<h3>Importing Data (Colab Only)</h3>\n", "\n", "If you are in a Google Colab environment, run the cell below to import the data for this notebook. Otherwise, if you have downloaded the course repository, you do not have to run the cell below.\n", "\n", "See the source and attribution information below:\n", "\n", ">data: data/L04/sn_z_mu_dmu_plow_union2.1.txt <br>\n", ">source: http://supernova.lbl.gov/Union/, https://arxiv.org/abs/1105.3470 <br>\n", ">attribution: The Supernova Cosmology Project, arXiv:1105.3470v1 <br>\n", ">license type: https://arxiv.org/licenses/nonexclusive-distrib/1.0/license.html <br>"]}, {"cell_type": "code", "execution_count": null, "id": "014547bb", "metadata": {"tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L6.0-runcell00\n", "\n", "#importing data from git repository\n", "\n", "!git init\n", "!git remote add -f origin https://github.com/mitx-8s50/nb_LEARNER/\n", "!git config core.sparseCheckout true\n", "!echo 'data/L04' >> .git/info/sparse-checkout\n", "!git pull origin main"]}, {"cell_type": "markdown", "id": "39ec2430", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Importing Libraries</h3>\n", "\n", "Before beginning, run the cells below to import the relevant libraries for this notebook.\n"]}, {"cell_type": "code", "execution_count": null, "id": "2b06c9e9", "metadata": {"tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L6.0-runcell01\n", "\n", "!pip install lmfit"]}, {"cell_type": "code", "execution_count": null, "id": "45ae60a9", "metadata": {"tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L6.0-runcell02\n", "\n", "import numpy as np                #https://numpy.org/doc/stable/\n", "from scipy import stats           #https://docs.scipy.org/doc/scipy/reference/stats.html\n", "from scipy import optimize as opt #https://docs.scipy.org/doc/scipy/reference/optimize.html\n", "import matplotlib.pyplot as plt   #https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.html\n", "import math                       #https://docs.python.org/3/library/math.html\n", "import csv                        #https://docs.python.org/3/library/csv.html\n", "import lmfit                      #https://lmfit.github.io/lmfit-py/ "]}, {"cell_type": "markdown", "id": "bcc82efa", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Setting Default Figure Parameters</h3>\n", "\n", "The following code cell sets default values for figure parameters."]}, {"cell_type": "code", "execution_count": null, "id": "7663d1de", "metadata": {"tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L6.0-runcell03\n", "\n", "#set plot resolution\n", "%config InlineBackend.figure_format = 'retina'\n", "\n", "#set default figure parameters\n", "plt.rcParams['figure.figsize'] = (9,6)\n", "\n", "medium_size = 12\n", "large_size = 15\n", "\n", "plt.rc('font', size=medium_size)          # default text sizes\n", "plt.rc('xtick', labelsize=medium_size)    # xtick labels\n", "plt.rc('ytick', labelsize=medium_size)    # ytick labels\n", "plt.rc('legend', fontsize=medium_size)    # legend\n", "plt.rc('axes', titlesize=large_size)      # axes title\n", "plt.rc('axes', labelsize=large_size)      # x and y labels\n", "plt.rc('figure', titlesize=large_size)    # figure title\n"]}, {"cell_type": "markdown", "id": "33743ad1", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_6_1'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L6.1 Introduction to Confidence Intervals</h2>  \n", "\n", "| [Top](#section_6_0) | [Previous Section](#section_6_0) | [Exercises](#exercises_6_1) | [Next Section](#section_6_2) |\n"]}, {"cell_type": "markdown", "id": "1362b42c", "metadata": {"tags": ["learner", "md", "8S50x"]}, "source": ["*The material in this section is discussed in the video **<a href=\"https://courses.mitxonline.mit.edu/learn/course/course-v1:MITxT+8.S50.1x+3T2024/block-v1:MITxT+8.S50.1x+3T2024+type@sequential+block@seq_LS6/block-v1:MITxT+8.S50.1x+3T2024+type@vertical+block@vert_LS6_vid1\" target=\"_blank\">HERE</a>.** You are encouraged to watch that video and use this notebook concurrently.*"]}, {"cell_type": "markdown", "id": "2264871d", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Slides</h3>\n", "\n", "Run the code below to view the slides for this section, which are discussed in the related video. You can also open the slides in a separate window <a href=\"https://mitx-8s50.github.io/slides/L06/slides_L06_01.html\" target=\"_blank\">HERE</a>."]}, {"cell_type": "code", "execution_count": null, "id": "929f9553", "metadata": {"tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L6.1-slides\n", "\n", "from IPython.display import IFrame\n", "IFrame(src='https://mitx-8s50.github.io/slides/L06/slides_L06_01.html', width=970, height=550)"]}, {"cell_type": "markdown", "id": "aac3cbdc", "metadata": {"tags": ["learner", "md", "lect_01"]}, "source": ["<h3>Overview</h3>\n", "\n", "For the past few Lessons, we have been considering uncertainties that we have defined to be the variance of a distribution, but what exactly do they mean? Let's go back to the p-value definition, given a pdf $p\\left(x|\\theta\\right)$\n", "\n", "$$\n", "\\begin{eqnarray}\n", "P(x|\\theta,x\\in \\Delta) & = &  \\int_{x}^{x+\\Delta} p\\left(x|\\theta\\right) dx \\\\\n", "P_{right}(x|\\theta, x\\leq x_{0}) & = &  \\int_{-\\infty}^{x_{0}} p\\left(x|\\theta\\right) dx\n", "\\end{eqnarray}\n", "$$\n", "\n", "The bottom integral is the cumulative distribution function (cdf), we can use this to derive relationships to the variances of distributions. Simply put, we can compute the probability within various intervals of a distribution.  For the normal (Gaussian) distribution variable $x$, we typically transform $x$ so that it can be written in terms of a normal distribution of unit $1$.\n", "\n", "\n", "$$\n", "\\begin{equation}\n", "\\mathcal{N}(0,1) = \\frac{x-\\bar{x}}{\\sigma}\n", "\\end{equation}\n", "$$\n", "The above shifts $x$ to be center about 0, and rescales the data so that the standard deviation is 1. \n", "\n", "Let's play around with a few things. First, let's compute the probability that an event falls 2 standard deviations outside of a Gaussian distribution. We can do this by using the CDF for -2 and 2 standard deviations, and noting that the integral of a probability distribution is 1.  "]}, {"cell_type": "code", "execution_count": null, "id": "dad5f01c", "metadata": {"tags": ["learner", "py", "lect_01", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L6.1-runcell01\n", "\n", "#probability for events within 1 sigma\n", "pM1=stats.norm.cdf(-1)\n", "p1=stats.norm.cdf(1)\n", "print(\"probability for events within 1 sigma: \", p1-pM1)\n", "print()\n", "\n", "#probability for events strictly < 1 sigma upper bound\n", "pM1=stats.norm.cdf(-1)\n", "p1=stats.norm.cdf(1)\n", "print(\"probability for events strictly < 1 sigma upper bound: \", p1)\n", "print()\n", "\n", "#probability for events within 2 sigma\n", "pM2=stats.norm.cdf(-2)\n", "p2=stats.norm.cdf(2)\n", "print(\"probability for events within 2 sigma: \", p2-pM2)\n", "print()\n", "\n", "#probability for events strictly > 2 sigma upper bound\n", "pM2=stats.norm.cdf(-2)\n", "p2=stats.norm.cdf(2)\n", "print(\"probability for events strictly > 2 sigma upper bound: \", 1-p2)\n", "print()"]}, {"cell_type": "markdown", "id": "9f4c8e0d", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='exercises_6_1'></a>     \n", "\n", "| [Top](#section_6_0) | [Restart Section](#section_6_1) | [Next Section](#section_6_2) |\n"]}, {"cell_type": "markdown", "id": "1ed76c79", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-6.1.1</span>\n", "\n", "Using stats.norm.cdf(), calculate the p value corresponding to $x$ being within $\\pm 3\\sigma$ of the mean in a normal distribution. Enter your answer as a number (not a percentage) with precision 1e-4.\n"]}, {"cell_type": "code", "execution_count": null, "id": "0ca14c6f", "metadata": {"tags": ["draft", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>EXERCISE: L6.1.1\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "#YOUR CODE HERE\n"]}, {"cell_type": "markdown", "id": "ba0227af", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": [">#### Follow-up 6.1.1a (ungraded)\n", ">\n", ">What is the delta log likelihood that corresponds to a 3 sigma deviation?"]}, {"cell_type": "markdown", "id": "c4b129fb", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-6.1.2</span>\n", "\n", "When computing p-values, we often refer to the region of the distribution that we are working with as \"left-handed\" or \"right-handed\". For instance, if we wanted to know the probability of a left-handed $3\\sigma$ deviation, this would be the area under the curve (the CDF) to the left of the lower $3\\sigma$ bound.\n", "\n", "What is the probability for a right-handed 5$\\sigma$ deviation? What is the probability of ANY deviation greater than 5$\\sigma$?\n", "\n", "Enter your answer as a list of two numbers (not a percentage) times 1e-7, with precision 1e-3. For instance, an answer of `6.238572e-7` would be reported as `6.238`.\n", "    \n", "Your list should be `[p_right_handed_5sigma, p_any_5sigma]`."]}, {"cell_type": "code", "execution_count": null, "id": "b990de3d", "metadata": {"tags": ["draft", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>EXERCISE: L6.1.2\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "#YOUR CODE HERE\n"]}, {"cell_type": "markdown", "id": "c6df8d24", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_6_2'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L6.2 z-Scores and Confidence Intervals for Other Distributions</h2>  \n", "\n", "| [Top](#section_6_0) | [Previous Section](#section_6_1) | [Exercises](#exercises_6_2) | [Next Section](#section_6_3) |\n"]}, {"cell_type": "markdown", "id": "32fdfa97", "metadata": {"tags": ["learner", "md", "8S50x"]}, "source": ["*The material in this section is discussed in the video **<a href=\"https://courses.mitxonline.mit.edu/learn/course/course-v1:MITxT+8.S50.1x+3T2024/block-v1:MITxT+8.S50.1x+3T2024+type@sequential+block@seq_LS6/block-v1:MITxT+8.S50.1x+3T2024+type@vertical+block@vert_LS6_vid2\" target=\"_blank\">HERE</a>.** You are encouraged to watch that video and use this notebook concurrently.*"]}, {"cell_type": "markdown", "id": "cc2bb95a", "metadata": {"tags": ["learner", "md", "lect_02"]}, "source": ["<h3>Overview</h3>\n", "\n", "We often want to characterize our probability by something that is easier to speak about in terms of deviations. Because of the central limit theorem and everything essentially becoming a Gaussian distribution in the large $N$ limit, we tend to refer to probabilities in units of standard deviations ($\\sigma$) of a normal distribution. A 1$\\sigma$ deviation thus refers to a deviation that is within the x$\\pm1\\sigma$ of the mean for a Gaussian distribution, or in other words with 1-68.2% = 31.8% probability that a fluctuation is larger than the observed fluctuation. \n", "\n", "To compute these probabilities, we can gain rely on the CDF distribution of the Gaussian distribution. Let's compute the probabilities for 1 to 5 standard deviations.  \n"]}, {"cell_type": "code", "execution_count": null, "id": "38a831c7", "metadata": {"tags": ["learner", "py", "lect_02", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L6.2-runcell01\n", "\n", "from scipy import stats\n", "\n", "#Let's do some integrals\n", "p50=stats.norm.cdf(0)\n", "p1=stats.norm.cdf(1)\n", "p2=stats.norm.cdf(2)\n", "p3=stats.norm.cdf(3)\n", "p5=stats.norm.cdf(5)\n", "pM1=stats.norm.cdf(-1)\n", "pM2=stats.norm.cdf(-2)\n", "pM3=stats.norm.cdf(-3)\n", "pM5=stats.norm.cdf(-5)\n", "#print(p50,p1,p2,p3,pM1,pM2,pM3)\n", "\n", "#Whats the probability of things fluctuation more that 1\\sigma\n", "print(p1-pM1,\"within 1 standard deviations\")\n", "print(p2-pM2,\"within 2 standard deviations\")\n", "print(p3-pM3,\"within 3 standard deviations\")\n", "print(p5-pM5,\"within 5 standard deviations\")\n", "\n", "#Sometimes we only consider 1-sided p-values\n", "print((1.-p1),\"to fluctuate above 1 standard deviation\")\n", "print((1.-p3),\"to fluctuate above 3 standard deviation\")\n", "print((1.-p5),\"to fluctuate above 5 standard deviation\")"]}, {"cell_type": "markdown", "id": "ab043add", "metadata": {"tags": ["learner", "md", "lect_02"]}, "source": ["These probability values define what we call confidence intervals. We also often write these as z-scores. For a measurement, the z-score is the probability that a measurement is within $z$ standard deviations of a distribution. \n", "\n", "$$\n", "\\begin{equation}\n", "\\bar{x}\\pm z \\sigma\n", "\\end{equation}\n", "$$\n", "\n", "Z-scores are often considered in the context of Gaussian distributions with z-scores corresponding to 68%, 95%, and 99.75% for 1,2, and 3$\\sigma$ deviations, respectively. However, for more complicated distributions, the approach is a bit different. "]}, {"cell_type": "markdown", "id": "a214477d", "metadata": {"tags": ["learner", "md", "lect_02"]}, "source": ["<h3>Asymmetric distributions</h3>\n", "\n", "In the discussion above, we considered the simplest scenario where we looked at the variation of distributions which we assumed were symmetrically distributed about zero. What if these distributions are asymmetric, how do we define the variations? Let's take a look at some asymmetric distributions. \n", "\n", "In the code below, we are going to use a few additional functions:\n", "`pdf` $\\rightarrow$ this computes the pdf value, `ppf` $\\rightarrow$ this goes the reverse direction of the cdf function by giving the input value (i.e. $\\sigma$ value) for a specific probability. Let's do this for a $\\chi^{2}$ distribution."]}, {"cell_type": "code", "execution_count": null, "id": "073f94bf", "metadata": {"tags": ["learner", "py", "lect_02", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L6.2-runcell03\n", "\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "#code from here(\u00a9 Eric Kim): https://aegis4048.github.io/comprehensive_confidence_intervals_for_python_developers\n", "\n", "#Let's plot a chi2 with 9 degrees of freedom (df)\n", "df = 9 \n", "x = np.linspace(-1, 28, 1000)\n", "y = stats.chi2.pdf(x, df, loc=0, scale=1)\n", "\n", "# two-tailed\n", "#Note we will use this function percent point function(ppf), \n", "#which inverts the cdf and gives a z from a probability\n", "two_right_tail = stats.chi2.ppf(1 - 0.025, df) #left value\n", "two_left_tail  = stats.chi2.ppf(1 - 0.975, df) #right value\n", "print(\"two tail values:\",two_right_tail,two_left_tail)\n", "\n", "# one tailed\n", "one_right_tail = stats.chi2.ppf(1 - 0.05, df)\n", "one_left_tail  = stats.chi2.ppf(1 - 0.95, df)\n", "print(\"one tail values:\",one_right_tail,one_left_tail)\n", "\n", "plt.style.use('seaborn-whitegrid')\n", "fig, axes = plt.subplots(1, 3, figsize=(12, 3))\n", "\n", "for ax in axes:\n", "    ax.plot(x, y, c='black')\n", "    ax.grid(False)\n", "    #ax.xaxis.set_major_formatter(plt.NullFormatter())\n", "    #ax.yaxis.set_major_formatter(plt.NullFormatter())\n", "\n", "#now let's fill this from the left\n", "axes[0].fill_between(x, 0, y, where=(np.array(x) > min(x)) & (np.array(x) <= two_left_tail), facecolor='grey')\n", "axes[0].fill_between(x, 0, y, where=(np.array(x) > two_left_tail) & (np.array(x) < two_right_tail), facecolor='lightgrey')\n", "axes[0].fill_between(x, 0, y, where=(np.array(x) > two_right_tail) & (np.array(x) <= max(x)), facecolor='grey')\n", "\n", "axes[1].fill_between(x, 0, y, where=(np.array(x) > min(x)) & (np.array(x) < one_right_tail), facecolor='lightgrey')\n", "axes[1].fill_between(x, 0, y, where=(np.array(x) > one_right_tail) & (np.array(x) <= max(x)), facecolor='grey')\n", "\n", "axes[2].fill_between(x, 0, y, where=(np.array(x) > min(x)) & (np.array(x) <= one_left_tail), facecolor='grey')\n", "axes[2].fill_between(x, 0, y, where=(np.array(x) > one_left_tail) & (np.array(x) <= max(x)), facecolor='lightgrey')\n", "\n", "fig.tight_layout()"]}, {"cell_type": "markdown", "id": "bb5e613d", "metadata": {"tags": ["learner", "md", "lect_02"]}, "source": ["Now that we have taken a look at these distributions, we see a clear asymmetry in the lengths of the z-values to p-values of the left and right. What is conserved is the integrals, but the values are different. Let's go ahead and compute the mean and variance of these distributions. To do this, we will sample a distribution using some new functions, denoted `rvs` (\"random variate samples\"); this function allows us to sample specific distributions so that we are able to generate toy events off of these sampled events. "]}, {"cell_type": "code", "execution_count": null, "id": "e2ccf7ae", "metadata": {"tags": ["learner", "py", "lect_02", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L6.2-runcell04\n", "\n", "#Let's compute the mean and RMS of a sample from this distribution\n", "df=9\n", "#Let's sample this distribution\n", "y_chi2 = stats.chi2.rvs(size=1000,df=df)\n", "print(y_chi2[0:5]) #print some valued\n", "print(\"Sampled Mean:\",y_chi2.mean(),\"Sampled Stddev:\",y_chi2.std())\n", "\n", "z=1.5 #Let's deviation corresponding to 1.5sigma in a Gauassian\n", "x = np.linspace(-1, 28, 1000)\n", "y = stats.chi2.pdf(x, df, loc=0, scale=1)\n", "two_right_tail = stats.chi2.ppf(1 - stats.norm.cdf(-z), df)\n", "two_left_tail = stats.chi2.ppf(1 - stats.norm.cdf(z), df)\n", "\n", "def plotItAll(x,y,y_chi2,z):\n", "    #Now let's plot the filled area using the true pdfs and the assumed variations if it were a Gaussian\n", "    plt.style.use('seaborn-whitegrid')\n", "    #plot distribution\n", "    plt.plot(x, y, c='black',label=\"$\\chi^2$\")\n", "    #plot chi2\n", "    plt.hist(y_chi2, histtype='stepfilled', edgecolor='k', alpha=0.4, color='gray', density=True,bins=20,label=\"events\")\n", "    #true values\n", "    plt.fill_between(x, 0, y, where=(np.array(x) > min(x)) & (np.array(x) <= two_left_tail), facecolor='grey')\n", "    plt.fill_between(x, 0, y, where=(np.array(x) > two_right_tail) & (np.array(x) <= max(x)), facecolor='grey')\n", "    #Mean +/- 1 sigma\n", "    plt.axvline(y_chi2.mean(), c='red',label=\"mean\")\n", "    plt.axvline(y_chi2.mean()+y_chi2.std()*z, c='blue',label=\"+/-$\\sigma_{gaus}$\")\n", "    plt.axvline(y_chi2.mean()-y_chi2.std()*z, c='blue')\n", "    plt.xlabel(\"x\")\n", "    plt.ylabel(\"$\\chi^{2}$\")\n", "    plt.legend(loc='upper right')\n", "    plt.show()\n", "plotItAll(x,y,y_chi2,z)"]}, {"cell_type": "markdown", "id": "3781abab", "metadata": {"tags": ["learner", "md", "lect_02"]}, "source": ["What we see now is that if we take deviations characteristic of two standard deviations or greater on the left and right of the distribution (these are the dark gray distributions) and we compare them to the expected variation that is up and down one standard deviation (the blue lines), we find that the p-values and the blue lines do not directly match to one another. This is because the standard deviation for an asymmetric distribution does not reflect the true width well, since the distribution is asymmetric. "]}, {"cell_type": "markdown", "id": "bf006ac3", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='exercises_6_2'></a>     \n", "\n", "| [Top](#section_6_0) | [Restart Section](#section_6_2) | [Next Section](#section_6_3) |\n"]}, {"cell_type": "markdown", "id": "7d52d14b", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-6.2.1</span>\n", "\n", "In the last section, we were using the language of \"p-values\" and standard deviations. Now we will talk about things in terms of \"confidence\" and \"z-score\". \n", "\n", "What is the confidence level corresponding to a z-score of 4, for a normal distribution? Enter your answer as a number with precision 1e-6."]}, {"cell_type": "code", "execution_count": null, "id": "89c2d385", "metadata": {"tags": ["draft", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>EXERCISE: L6.2.1\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "pass\n"]}, {"cell_type": "markdown", "id": "e6ed91c8", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-6.2.2</span>\n", "\n", "Again, consider a normal distribution. What is the z-score for an event to be less than some value $x$, where we know the probability that the value is $>x$ is $p=0.40$?\n", "\n", "Enter your answer as a number with precision 1e-3."]}, {"cell_type": "code", "execution_count": null, "id": "1344050b", "metadata": {"tags": ["draft", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>EXERCISE: L6.2.2\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "pass\n"]}, {"cell_type": "markdown", "id": "e8f22232", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_6_3'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L6.3 Another Example and Rules of Significance</h2>  \n", "\n", "| [Top](#section_6_0) | [Previous Section](#section_6_2) | [Exercises](#exercises_6_3) | [Next Section](#section_6_4) |\n"]}, {"cell_type": "markdown", "id": "c7d678cd", "metadata": {"tags": ["learner", "md", "8S50x"]}, "source": ["*The material in this section is discussed in the video **<a href=\"https://courses.mitxonline.mit.edu/learn/course/course-v1:MITxT+8.S50.1x+3T2024/block-v1:MITxT+8.S50.1x+3T2024+type@sequential+block@seq_LS6/block-v1:MITxT+8.S50.1x+3T2024+type@vertical+block@vert_LS6_vid3\" target=\"_blank\">HERE</a>.** You are encouraged to watch that video and use this notebook concurrently.*"]}, {"cell_type": "markdown", "id": "0430d377", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Slides</h3>\n", "\n", "Run the code below to view the slides for this section, which are discussed in the related video. You can also open the slides in a separate window <a href=\"https://mitx-8s50.github.io/slides/L06/slides_L06_03.html\" target=\"_blank\">HERE</a>."]}, {"cell_type": "code", "execution_count": null, "id": "d17712ea", "metadata": {"tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L6.3-slides\n", "\n", "from IPython.display import IFrame\n", "IFrame(src='https://mitx-8s50.github.io/slides/L06/slides_L06_03.html', width=970, height=550)"]}, {"cell_type": "markdown", "id": "0f629dee", "metadata": {"tags": ["learner", "md", "lect_03"]}, "source": ["<h3>Overview</h3>\n", "\n", "Finally, let's consider a more complicated scenario where the shape of the distribution is very far away from a normal distribution. In this scenario, we want to show that conventional estimates for the width of a distribution break down dramatically. It's important to know this, since it can be very easy to use standard deviation to characterize the properties of a distribution. "]}, {"cell_type": "markdown", "id": "71b7e9b0", "metadata": {"tags": ["learner", "md", "lect_03"]}, "source": ["Let's consider a Cauchy distribution (described <a href=\"https://en.wikipedia.org/wiki/Cauchy_distribution\" target=\"_blank\">here</a>) and given by the form:\n", "\n", "$$\n", "\\begin{equation}\n", "f(x;x_{0},\\gamma) = \\frac{1}{\\pi \\gamma} \\left(\\frac{1}{\\left(x-x_0\\right)^{2} + \\gamma^{2}}\\right)\n", "\\end{equation}\n", "$$\n", "\n", "This function is available also in scipy stats with ```scipy.stats.cauchy```. For this function we will compare the Gaussian 1 standard deviation with the true Cauchy p-values to see what the differences are. "]}, {"cell_type": "code", "execution_count": null, "id": "92d1d411", "metadata": {"tags": ["learner", "py", "lect_03", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L6.3-runcell01\n", "\n", "#NOTE: below we define a random seed for identical results between runs.\n", "#These results may appear slightly different from the related video.\n", "\n", "\n", "#Generate Cauchy data\n", "np.random.seed(6)\n", "y_cauchy = stats.cauchy.rvs(size=10000)\n", "\n", "\n", "#choose z-score\n", "z=1.0\n", "two_right_tail = stats.cauchy.ppf(1 - stats.norm.cdf(-z))\n", "two_left_tail = stats.cauchy.ppf(1 - stats.norm.cdf(z))\n", "print('left/right-handed values corresponding to z-score:', two_left_tail,two_right_tail)\n", "\n", "\n", "#print the mean and stdev of the distribution\n", "fig, ax = plt.subplots(figsize=(12, 6))\n", "y_cauchy = stats.cauchy.rvs(size=10000)\n", "print('[mean of cauchy data, stdev of cauchy data]:', y_cauchy.mean(),z*y_cauchy.std())\n", "\n", "\n", "#plot distribution\n", "ymin = -300\n", "ymax = 300\n", "plt.xlim([ymin,ymax])\n", "x = np.linspace(ymin, ymax, 10000)\n", "y = stats.cauchy.pdf(x, loc=0, scale=1)\n", "plt.plot(x, y, '--', c='black',label=\"f\")\n", "plt.hist(y_cauchy, histtype='stepfilled', edgecolor='k', alpha=0.4, color='gray', density=True,bins=10000)\n", "\n", "\n", "#plot true-sigma\n", "#plt.fill_between(x, 0, y, where=(np.array(x) > min(x)) & (np.array(x) <= two_left_tail), facecolor='grey',label='+/- true $\\sigma$')\n", "#plt.fill_between(x, 0, y, where=(np.array(x) > two_right_tail) & (np.array(x) <= max(x)), facecolor='grey')\n", "plt.fill_between(x, 0, y, where=(np.array(x) < two_right_tail) & (np.array(x) >= two_left_tail), facecolor='grey',label='+/- gaus $\\sigma$')\n", "\n", "\n", "plt.axvline(y_cauchy.mean(), c='red',label=\"mean\")\n", "plt.axvline(y_cauchy.mean()+y_cauchy.std()*z, c='blue',label='+/- true $\\sigma$')\n", "plt.axvline(y_cauchy.mean()-y_cauchy.std()*z, c='blue')\n", "plt.xlabel(\"x\")\n", "plt.ylabel(\"f(x)\")\n", "plt.legend(loc='upper right')\n", "plt.yscale('log')\n", "plt.show()"]}, {"cell_type": "markdown", "id": "f8dc9886", "metadata": {"tags": ["learner", "md", "lect_03"]}, "source": ["As you can see, the standard deviation(blue) and the p-values(dark gray) are completely different in scale. This is because the Cauchy distribution can have a few events that fluctuate very far in the tails away from the mean of the Cauchy distribution. "]}, {"cell_type": "markdown", "id": "c9ee437a", "metadata": {"tags": ["learner", "md", "lect_03"]}, "source": ["In the preceding example with the Cauchy distribution, we found the positions of the left tail, `two_left_tail`, and the right tail, `two_right_tail`, such that the probability that a measurement drawn from the Cauchy distribution falls within the range `[two_left_tail,two_right_tail]` is equal to 0.6827. We chose the bounds to coincide with the confidence level of a Gaussian with z-score equal to 1.\n", "\n", "To see that, run the code below."]}, {"cell_type": "code", "execution_count": null, "id": "ab7f81a1", "metadata": {"tags": ["learner", "py", "lect_03", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L6.3-runcell02\n", "\n", "#calculate p_value from data\n", "def get_p_value_data(idist, two_left_tail, two_right_tail):\n", "  within_range = []\n", "  for ielem in idist:\n", "    if ielem <= two_right_tail and ielem >= two_left_tail:\n", "      within_range.append(ielem)\n", "  return len(within_range)/len(idist)\n", "\n", "\n", "#Generate Cauchy data\n", "np.random.seed(6)\n", "y_cauchy = stats.cauchy.rvs(size=10000)\n", "\n", "#choose z-score\n", "z=1.0\n", "two_right_tail = stats.cauchy.ppf(1 - stats.norm.cdf(-z))\n", "two_left_tail = stats.cauchy.ppf(1 - stats.norm.cdf(z))\n", "\n", "cauchy_p_value = get_p_value_data(y_cauchy, two_left_tail, two_right_tail)\n", "print(\"p-value from left/right values and data:\", cauchy_p_value)\n", "print()"]}, {"cell_type": "markdown", "id": "ca9fc613", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='exercises_6_3'></a>     \n", "\n", "| [Top](#section_6_0) | [Restart Section](#section_6_3) | [Next Section](#section_6_4) |\n"]}, {"cell_type": "markdown", "id": "80b35f84", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-6.3.1</span>\n", "\n", "What is the probability than an event drawn from a Cauchy distribution falls within 1-sigma of the mean? Use the data that we generated above to calculate your probability, and consider using the function `get_p_value_data()` (or write your own).\n", "\n", "Enter your answer as a number with precision 1e-3."]}, {"cell_type": "code", "execution_count": null, "id": "205d04e6", "metadata": {"tags": ["draft", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>EXERCISE: L6.3.1\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "#calculate p_value from data\n", "def get_p_value_data(idist, two_left_tail, two_right_tail):\n", "  within_range = []\n", "  for ielem in idist:\n", "    if ielem <= two_right_tail and ielem >= two_left_tail:\n", "      within_range.append(ielem)\n", "  return len(within_range)/len(idist)\n", "\n", "#Generate Cauchy data\n", "np.random.seed(6)\n", "y_cauchy = stats.cauchy.rvs(size=10000)\n", "\n", "#choose z-score\n", "z=1.0\n", "two_right_tail = #YOUR CODE HERE\n", "two_left_tail = #YOUR CODE HERE\n", "\n", "cauchy_p_value = get_p_value_data(y_cauchy, two_left_tail, two_right_tail)\n", "print(\"p-value from left/right values and data:\", cauchy_p_value)\n", "print()\n"]}, {"cell_type": "markdown", "id": "bcd59bbd", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-6.3.2</span>\n", "\n", "What is the p-value that corresponds to a z-score of 1 for a Cauchy distribution? Enter your answer as a number with precision 1e-3."]}, {"cell_type": "code", "execution_count": null, "id": "c15c778b", "metadata": {"tags": ["draft", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>EXERCISE: L6.3.2\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "pass\n"]}, {"cell_type": "markdown", "id": "ddca039b", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-6.3.3</span>\n", "\n", "Which of the following options best describes a 3$\\sigma$ detection:\n", "\n", "- background noise\n", "- evidence\n", "- discovery\n"]}, {"cell_type": "markdown", "id": "6b8f1fbf", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-6.3.4</span>\n", "\n", "Which of the following options best describes a 5$\\sigma$ detection:\n", "\n", "- background noise\n", "- evidence\n", "- discovery\n"]}, {"cell_type": "markdown", "id": "65acc2bc", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_6_4'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L6.4 Moments of Distributions and Mapping</h2>  \n", "\n", "| [Top](#section_6_0) | [Previous Section](#section_6_3) | [Exercises](#exercises_6_4) | [Next Section](#section_6_5) |\n"]}, {"cell_type": "markdown", "id": "b1f7630b", "metadata": {"tags": ["learner", "md", "8S50x"]}, "source": ["*The material in this section is discussed in the video **<a href=\"https://courses.mitxonline.mit.edu/learn/course/course-v1:MITxT+8.S50.1x+3T2024/block-v1:MITxT+8.S50.1x+3T2024+type@sequential+block@seq_LS6/block-v1:MITxT+8.S50.1x+3T2024+type@vertical+block@vert_LS6_vid4\" target=\"_blank\">HERE</a>.** You are encouraged to watch that video and use this notebook concurrently.*"]}, {"cell_type": "markdown", "id": "4eeed69e", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Slides</h3>\n", "\n", "Run the code below to view the slides for this section, which are discussed in the related video. You can also open the slides in a separate window <a href=\"https://mitx-8s50.github.io/slides/L06/slides_L06_04.html\" target=\"_blank\">HERE</a>."]}, {"cell_type": "code", "execution_count": null, "id": "8645cd45", "metadata": {"tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L6.4-slides\n", "\n", "from IPython.display import IFrame\n", "IFrame(src='https://mitx-8s50.github.io/slides/L06/slides_L06_04.html', width=970, height=550)"]}, {"cell_type": "markdown", "id": "46e39f1f", "metadata": {"tags": ["learner", "md", "lect_04"]}, "source": ["<h3>Moments of distributions</h3>\n", "\n", "So, you have seen that with certain types of distributions, things can deviate wildly from what is expected. Namely, in the case of a Cauchy or asymmetric distribution, the standard deviation of a data sample is not a reflection of the p-value of the distribution at all. To describe these sorts of distributions, there are a number of ways to mitigate this problem. The first is to introduce higher order moments of a pdf. \n", "\n", "Let's assume our distributions are centered about zero (we can always recenter them using a new variable $x'=x-\\bar{x}$) ). Then, we can write moments of order $n$ as\n", "\n", "\n", "$$\n", "\\begin{equation}\n", " \\mu_{n}=m^{n}(x)=E[x^{n}p(x)] = \\int_{-\\infty}^{\\infty} x^{n} p(x) dx\n", "\\end{equation}\n", "$$\n", "\n", "\n", "The mean is the moment of order 1, $E[p(x)]=m^{1}(x)$, and the variance of the distribution is the 2$^{\\textrm{nd}}$ order moment, $V[p(x)]=m^{2}(x)$. These are the two properties of a distribution that you already know and love, but we can keep going to higher order moments in order to describe more complicated distributions. The next moment, called \"skewness\", tells you how asymmetric a distribution is; $\\mathrm{Skew}=m^{3}(x)$, and going further we have the $\\mathrm{Kurtosis}=m^{4}(x)$, which tells you how important the tails of a distribution are. \n", "\n", "Lots of different distributions can be found in data. Here is a <a href=\"http://bois.caltech.edu/dist_stories/t3b_probability_stories.html\" target=\"_blank\">\"story book of distributions.\"</a>\n", "\n", "\n", "Note that this link is also a nice concise summary of a wide range of distributions, including their functional forms and a brief description of their properties.\n", "\n", "Let's take a look at the moments of our two example distributions discussed above. \n"]}, {"cell_type": "code", "execution_count": null, "id": "102ad113", "metadata": {"tags": ["learner", "py", "lect_04", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L6.4-runcell01\n", "\n", "def raw_moment(X, k, c=0):\n", "    return ((X - c)**k).mean()\n", "\n", "def central_moment(X, k):\n", "    if k == 1:\n", "        return X.mean()\n", "    return raw_moment(X=X, k=k, c=X.mean())\n", "\n", "def print_moments(X,label):\n", "    print(label+\" mean:\",central_moment(X,1))\n", "    print(label+\" var:\" ,central_moment(X,2))\n", "    print(label+\" skew:\",central_moment(X,3))\n", "    print(label+\" kurtosis:\",central_moment(X,4))\n", "\n", "N=1000000\n", "y_norm = stats.norm.rvs(size=N)\n", "print_moments(y_norm,\"normal\")\n", "\n", "df=9\n", "y_chi2 = stats.chi2.rvs(size=N,df=df)\n", "print_moments(y_chi2,\"chi2 df \"+str(df))\n", "\n", "y_cauchy = stats.cauchy.rvs(size=N)\n", "print_moments(y_cauchy,\"cauchy\")\n"]}, {"cell_type": "markdown", "id": "36df1a42", "metadata": {"tags": ["learner", "md", "lect_04"]}, "source": ["Now, the nice thing about moments is that you can use these values to map any distribution to any other distribution. \n", "\n", "I am not really doing justice to the above, but moments are in fact incredibly useful. A chunk of my PhD thesis is dedicated to using moments of a Gaussian (cumulants) to model distributions with a small amount of data. \n", "\n", "In light of this, what we can imagine doing is to use a transform, like we do with a Fourier transform. Through the use of z-scores and moments, you can transform one probability distribution function into any another. The strategy here is to match quantiles, or z-scores, with each other, such that you have a transform from $x\\rightarrow x^{\\prime}$, which does this matching for distributions $p_{1}$ and $p_2$:\n", "\n", "$$\n", "\\begin{equation}\n", "\\int_{-\\infty}^{x} p_1(x) dx = \\int_{-\\infty}^{x^\\prime} p_2(x^{\\prime}) dx^{\\prime}\n", "\\end{equation}\n", "$$\n", "\n", "To do this mapping, there are many different approaches. All of these approaches do roughly the same thing, map one pdf to another pdf.  One such approach of mapping distributions is the <a href=\"https://en.wikipedia.org/wiki/Power_transform\" target=\"_blank\">box-cox method</a>, which aims to use the moments to transform any distribution into a Gaussian distribution. Let's see how well it works.\n", "\n", "To do this we are going to sample a lognormal distribution using `stats.lognorm`.  This distribution is very much not a Gaussian, as we will see. "]}, {"cell_type": "code", "execution_count": null, "id": "f4299604", "metadata": {"tags": ["learner", "py", "lect_04", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L6.4-runcell02\n", "\n", "x = stats.lognorm.rvs(s=1, loc=0, scale=5, size=1000, random_state=4)\n", "\n", "# plot\n", "def plotdistandq(x,xaxis,evals):\n", "    fig, axes = plt.subplots(1, 2, figsize=(10, 3))\n", "    axes[0].hist(x,density=True)\n", "    axes[0].plot(xaxis,evals)\n", "    stats.probplot(x, dist=stats.norm, plot=axes[1])\n", "    fig.tight_layout()\n", "\n", "xaxis=np.linspace(0,80, 1000)\n", "evals=stats.lognorm.pdf(xaxis,s=1, loc=0, scale=5)\n", "plotdistandq(x,xaxis,evals)\n", "# box-cox transform\n", "xt, lmbda = stats.boxcox(x)\n", "\n", "#now plot Gaussian\n", "gxaxis = np.linspace(-2, 5, 1000)\n", "gevals = stats.norm.pdf(gxaxis,xt.mean(),1) \n", "plotdistandq(xt,gxaxis,gevals)\n", "\n", "stats.probplot(xt, dist=stats.norm, plot=axes[1])\n", "fig.tight_layout()"]}, {"cell_type": "markdown", "id": "4327157a", "metadata": {"tags": ["learner", "md", "lect_04"]}, "source": ["These types of transforms are particularly useful when we are trying to match a simulated distribution with a data distribution. If both the simulated and true distributions are trying to describe the same thing, you can use a transform like this to map one distribution onto the other."]}, {"cell_type": "markdown", "id": "2b07802a", "metadata": {"tags": ["learner", "md", "lect_04"]}, "source": ["<h3>Applying this Method to Cauchy Distribution</h3>\n", "\n", "Let's now do this for a more complicated Cauchy distribution, and see how it behaves. In fact, let's take the absolute value of it. "]}, {"cell_type": "code", "execution_count": null, "id": "aed81b7f", "metadata": {"tags": ["learner", "py", "lect_04", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L6.4-runcell03\n", "\n", "x = np.abs(stats.cauchy.rvs(size=10000))\n", "\n", "xaxis=np.linspace(0,80, 1000)\n", "evals=np.abs(stats.cauchy.pdf(xaxis))\n", "plotdistandq(x,xaxis,evals)\n", "# box-cox transform\n", "xt, lmbda = stats.boxcox(x)\n", "\n", "#now plot Gaussian\n", "gxaxis = np.linspace(-2, 5, 1000)\n", "gevals = stats.norm.pdf(gxaxis,xt.mean(),1) \n", "plotdistandq(xt,gxaxis,gevals)\n", "\n", "stats.probplot(xt, dist=stats.norm, plot=axes[1])\n", "fig.tight_layout()\n", "\n", "print(\"mean:\",xt.mean())"]}, {"cell_type": "markdown", "id": "3fff138f", "metadata": {"tags": ["learner", "md"]}, "source": ["The Cauchy distribution is a hard distribution to model because of its giant tails. However, in the bulk it is very much Gaussian, so using a box-cox does surprisingly well in the cental region. However, in the far tails it starts to deviate quite a bit."]}, {"cell_type": "markdown", "id": "6275dbd8", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='exercises_6_4'></a>     \n", "\n", "| [Top](#section_6_0) | [Restart Section](#section_6_4) | [Next Section](#section_6_5) |\n"]}, {"cell_type": "markdown", "id": "31bab432", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-6.4.1</span>\n", "\n", "Take a $\\chi^2$ distribution with 1 degree of freedom, which is very asymmetric, and map it onto a Gaussian. What is the mean of this distribution?\n", "\n", "Complete the code below, which uses a random seed to ensure your answer will match ours. Enter your answer as a number with precision 1e-3.\n"]}, {"cell_type": "code", "execution_count": null, "id": "77a546b4", "metadata": {"tags": ["draft", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>EXERCISE: L6.4.1\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "np.random.seed(10)\n", "x = stats.chi2.rvs(size=10000,df=1)\n", "\n", "#YOUR CODE HERE"]}, {"cell_type": "markdown", "id": "bfebe511", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_6_5'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L6.5 Monte Carlo Integration</h2>  \n", "\n", "| [Top](#section_6_0) | [Previous Section](#section_6_4) | [Exercises](#exercises_6_5) | [Next Section](#section_6_6) |\n"]}, {"cell_type": "markdown", "id": "4667dd27", "metadata": {"tags": ["learner", "md", "8S50x"]}, "source": ["*The material in this section is discussed in the video **<a href=\"https://courses.mitxonline.mit.edu/learn/course/course-v1:MITxT+8.S50.1x+3T2024/block-v1:MITxT+8.S50.1x+3T2024+type@sequential+block@seq_LS6/block-v1:MITxT+8.S50.1x+3T2024+type@vertical+block@vert_LS6_vid5\" target=\"_blank\">HERE</a>.** You are encouraged to watch that video and use this notebook concurrently.*"]}, {"cell_type": "markdown", "id": "00bfc9bb", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Slides</h3>\n", "\n", "Run the code below to view the slides for this section, which are discussed in the related video. You can also open the slides in a separate window <a href=\"https://mitx-8s50.github.io/slides/L06/slides_L06_05.html\" target=\"_blank\">HERE</a>."]}, {"cell_type": "code", "execution_count": null, "id": "dde400ef", "metadata": {"tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L6.5-slides\n", "\n", "from IPython.display import IFrame\n", "IFrame(src='https://mitx-8s50.github.io/slides/L06/slides_L06_05.html', width=970, height=550)"]}, {"cell_type": "markdown", "id": "ff4cbd5c", "metadata": {"tags": ["learner", "md", "lect_05"]}, "source": ["<h3>Numerical Integration</h3>\n", "\n", "You have probably noticed that I didn't bother to compute integrals to get the moments above, I just computed the means of the moments of sampled distributions. Instead, I did what is referred to as Monte-Carlo Integration (or more generally bootstrapping) where I just sampled a distribution and integrated by sampling. Namely, \n", "\n", "$$\n", "\\begin{equation}\n", "E[x^{n}p(x)] = \\int_{-\\infty}^{\\infty} x^{n} p(x) dx = \\frac{1}{N}\\sum_{i=1}^{N} x_{i}^{n}\n", "\\end{equation}\n", "$$\n", "\n", "where here the $x_{i}\\in p(x)$ are sampled from the probability distribution function. So what happens, when we have a distribution, but we don't know the analytic form. How can we sample it?  \n", "\n", "There are a lot of ways to do this, perhaps the best well known is Markov Chain Monte Carlo (MCMC). However the simplest is to just turn our distribution into a 2D image and randomly sample points on the image. Instead of writing the points out, let's just do it. \n", "\n", "To see how this works, let's walk through a calculation of integrating a quarter circle. We know that the integral is given by $A=\\frac{\\pi}{4}$, so we can check our math. What we will do is \n", "\n", "1. Sample randomly in x\n", "2. Sample randomly in y\n", "3. Check to see if x and y are within our quarter circle. \n", "4. Compute the number of samples within our quarter circle, compared to all points. \n", "\n", "Let's look at the code. Note that since we are randomly sampling, our uncertainty on our random sample will just be the poisson uncertainty or $\\frac{a}{\\sqrt{N_{\\rm samples}}}$.\n"]}, {"cell_type": "code", "execution_count": null, "id": "8fb01aa0", "metadata": {"tags": ["learner", "py", "lect_05", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L6.5-runcell01\n", "\n", "import math\n", "\n", "#First let's just compute the area of a quarter circle with radius 1\n", "def quarterarea(iN):\n", "    area=0\n", "    lXin = np.array([])\n", "    lYin = np.array([])\n", "    lXout = np.array([])\n", "    lYout = np.array([])\n", "    for i0 in range(iN):\n", "        #Sample X and Y\n", "        pX = np.random.uniform(0,1)\n", "        pY = np.random.uniform(0,1)\n", "        #Check if its radius is in 1\n", "        if math.sqrt(pX**2+pY**2) < 1:\n", "            lXin = np.append(lXin,pX)\n", "            lYin = np.append(lYin,pY)\n", "            area += 1 # count it\n", "        else:\n", "            lXout = np.append(lXout,pX)\n", "            lYout = np.append(lYout,pY)\n", "    return (float(area)/float(iN)),lXin,lYin,lXout,lYout\n", "\n", "#sample points\n", "lN=1000\n", "#lN=100000\n", "a,lXin,lYin,lXout,lYout=quarterarea(lN)\n", "print(\"Pi (4*area):\",a*4,\"+/-\",4*a/math.sqrt(lN)) #gotta put an uncertainty\n", "plt.plot(lXin,lYin,marker='.',linestyle = 'None')\n", "plt.plot(lXout,lYout,marker='.',linestyle = 'None')\n", "plt.show()"]}, {"cell_type": "markdown", "id": "ba5a50df", "metadata": {"tags": ["learner", "md", "lect_05"]}, "source": ["The idea with Monte Carlo integration is that we calculate an integral by evaluating the function. We don't actually have to compute the integral. This avoids what is potentially a very complicated step.  As you probably well know, computing integrals can be very hard, this way gets at our answer just through functional evaluation. \n", "\n", "Now, let's compute the integral of some arbitrary function $y=f(x)$. As we did with our circle, we can compute the integral by plotting this function over a range. Here we will do $-6 < x < 3$. From this, what we will do is sample over a 2D Gaussian distribution of width given by the x and y ranges and center given by the minimum x and y values. Why this distribution to sample? Because we can."]}, {"cell_type": "code", "execution_count": null, "id": "5955f973", "metadata": {"tags": ["learner", "py", "lect_05", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L6.5-runcell02\n", "\n", "from scipy import optimize as opt \n", "\n", "#use this random seed\n", "np.random.seed(10)\n", "\n", "#Now let's consider integrating some random function\n", "def f(x):\n", "    return x**4 + 3*(x-2)**3 - 15*(x)**2 + 1\n", "\n", "#Now let's multiply it by -1 to make the range calculation fast\n", "def fneg(x):\n", "    return -1*(x**4 + 3*(x-2)**3 - 15*(x)**2 + 1)\n", "\n", "#First thing is to define a range in x\n", "xmin=-6\n", "xmax=3\n", "x = np.linspace(xmin, xmax, 100)\n", "#plt.plot(x, f(x));\n", "\n", "#Now we need to find a range in y\n", "sol=opt.minimize_scalar(f,bounds=(xmin, xmax), method='Brent')\n", "ymin=sol.fun\n", "#y-max is to get the minimum of negative f\n", "sol=opt.minimize_scalar(fneg,bounds=(xmin, xmax), method='Brent')\n", "ymax=-1*sol.fun\n", "print('[ymin,ymax]:', ymin, ymax)\n", "\n", "lN=100000\n", "#now, let's sample a 2D grid y-min and y-max and compute the integral\n", "lXin = np.array([])\n", "lYin = np.array([])\n", "lXout = np.array([])\n", "lYout = np.array([])\n", "for i0 in range(lN):\n", "    #Try a uniform distribution\n", "    pX = abs(xmax-xmin)*np.random.uniform(0,1)+xmin\n", "    pY = abs(ymax-ymin)*np.random.uniform(0,1)+ymin\n", "    #Try a normal distribution\n", "    #pX = abs(xmax-xmin)*np.random.normal(0,1)+xmin\n", "    #pY = abs(ymax-ymin)*np.random.normal(0,1)+ymin\n", "    pYMin = f(pX)\n", "    if pY < pYMin:\n", "        lXin = np.append(lXin,pX)\n", "        lYin = np.append(lYin,pY)\n", "    else:\n", "        lXout = np.append(lXout,pX)\n", "        lYout = np.append(lYout,pY)\n", "\n", "\n", "plt.plot(lXin,lYin,marker='.',linestyle = 'None', color='orange')\n", "plt.plot(lXout,lYout,marker='.',linestyle = 'None', color='green')\n", "plt.axvline(sol.x, c='red', lw=3)\n", "plt.plot(x, f(x), 'b-', lw=3)\n", "#plt.ylim(-800,0)\n", "#plt.xlim(-6,3)\n", "plt.show();"]}, {"cell_type": "markdown", "id": "237384db", "metadata": {"tags": ["learner", "md", "lect_05"]}, "source": ["From the above scenario, we can see that the orange points are below the line from roughly -800 to 0, and the green points are above the line. We can make a histogram of this."]}, {"cell_type": "code", "execution_count": null, "id": "f9fa5537", "metadata": {"tags": ["learner", "py", "lect_05", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L6.5-runcell03\n", "\n", "_,bins,_=plt.hist(lXin,bins=10,alpha=0.5,density=True, label='below')\n", "plt.hist(lXout,alpha=0.5,bins=bins,density=True, label='above')\n", "#plt.xlim(-6,3)\n", "plt.legend(loc=1)\n", "plt.show();\n", "\n", "print(\"number below:\",len(lXin),\"number above:\",len(lXout))\n", "#Note, these numbers will be slightly different for each run\n", "#due to random generation of data"]}, {"cell_type": "markdown", "id": "b9335fd1", "metadata": {"tags": ["learner", "md", "lect_05"]}, "source": ["We have to be careful with the above integral if we are sampling a Gaussian distribution. In that case, our sampling is biased, not uniform, which means our integral will not be correct, and it will be biased towards the Gaussian we chose. We can write this out as the product of our function times the phase space we are sampling. In this case, the phase space is given by the Hessian of a 2D Gaussian $\\frac{\\partial^{2} G(x,y) }{\\partial x \\partial y} $\n", "\n", "$\\mathcal{I}=\\int_{\\rm space} f(x,y) \\frac{\\partial^{2} G(x,y) }{\\partial x \\partial y} dx dy$\n", "\n", "This procedure is known as \"Area-based\" sampling, and is considered a method of Monte-Carlo Integration. Monte-Carlo integration is a rich field. All high energy physics simulations are based on it. Basically, the function we sample from starts with a collision and computing the probability that this could be any other collision. We then proceed to put this single collision through a point by point simulation, of each particle going through all the detectors. Finally we aggregate our distributions based on this. You will see the usefulness of Monte Carlo Simulation later on. "]}, {"cell_type": "markdown", "id": "e1e2edda", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='exercises_6_5'></a>     \n", "\n", "| [Top](#section_6_0) | [Restart Section](#section_6_5) | [Next Section](#section_6_6) |\n"]}, {"cell_type": "markdown", "id": "5e0fcb32", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-6.5.1</span>\n", "\n", "Compute the fraction of events above the line corresponding to the function that we defined previously:\n", "\n", "<pre>\n", "def f(x):\n", "    return x**4 + 3*(x-2)**3 - 15*(x)**2 + 1\n", "</pre>\n", "\n", "Specifically, sample `100000` points from a uniform distribution over the x and y ranges specified above (i.e., $-6 < x < 3$ and $f(-6) < y < f(3)$ to compute this fraction.\n", "\n", "Enter your answer as a number with precision 1e-3.\n", "\n", "Hint: Use the `lXout` value from `L6.5-runcell02`. This means the solution is only 3 lines."]}, {"cell_type": "code", "execution_count": null, "id": "e13b4040", "metadata": {"tags": ["draft", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>EXERCISE: L6.5.1\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "#YOUR CODE HERE\n", "#Note, if you are computing the distributions again, use the following random seed\n", "np.random.seed(10)\n", "pass\n"]}, {"cell_type": "markdown", "id": "93c77464", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-6.5.2</span>\n", "\n", "How would you compute the integral of this function over the range `[-6,3]`? Select the correct answer below:\n", "\n", "- The integral is equal to the number of points above the line defined by our function.\n", "- The integral is equal to the number of points between zero and the line defined by our function.\n", "- The integral is equal to the fraction of points that we calculated, times the area of the region that we defined.\n", "- The integral is equal to the fraction of points between zero and our function, times the area of the region where the simulation is carried out, which must use `0` as an upper bound.\n"]}, {"cell_type": "markdown", "id": "17035d2c", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": [">#### Follow-up 6.5.2a (ungraded)\n", ">\n", ">Calculate the integral numerically. Remember to correctly apply a negative sign, if applicable.\n", ">\n", ">How does your numerically calculated value compare to the expected value? You can explicitly solve the integral mathematically, or use a built-in `scipy` integration method to compare to your Monte Carlo integration."]}, {"cell_type": "code", "execution_count": null, "id": "6ea6ab33", "metadata": {"tags": ["draft", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>FOLLOW-UP: L6.5.2a\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "#YOUR CODE HERE\n", "pass\n"]}, {"cell_type": "markdown", "id": "03d5d7fa", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_6_6'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L6.6 Returning to Fitting Supernova Data</h2>  \n", "\n", "| [Top](#section_6_0) | [Previous Section](#section_6_5) | [Exercises](#exercises_6_6) | [Next Section](#section_6_7) |\n"]}, {"cell_type": "markdown", "id": "d916d9a3", "metadata": {"tags": ["learner", "md", "8S50x"]}, "source": ["*The material in this section is discussed in the video **<a href=\"https://courses.mitxonline.mit.edu/learn/course/course-v1:MITxT+8.S50.1x+3T2024/block-v1:MITxT+8.S50.1x+3T2024+type@sequential+block@seq_LS6/block-v1:MITxT+8.S50.1x+3T2024+type@vertical+block@vert_LS6_vid6\" target=\"_blank\">HERE</a>.** You are encouraged to watch that video and use this notebook concurrently.*"]}, {"cell_type": "markdown", "id": "04d85321", "metadata": {"tags": ["learner", "md", "lect_06"]}, "source": ["<h3>A more sophisticated fit</h3>\n", "\n", "Now that we have gone on an excursion to understand properties of fits, let's go ahead and analyze our supernovae data, and try to pull in all of the information that we can. Let's first look at our linear fit.  One sec, while we load it all:\n"]}, {"cell_type": "code", "execution_count": null, "id": "b2ce6935", "metadata": {"tags": ["learner", "py", "lect_06", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L6.6-runcell01\n", "import math\n", "import numpy as np\n", "import csv\n", "import matplotlib.pyplot as plt\n", "from scipy import stats\n", "\n", "#Let's try to understand how good the fits we made in last Lesson are, let's load the supernova data again\n", "label='data/L04/sn_z_mu_dmu_plow_union2.1.txt'\n", "\n", "def distanceconv(iMu):\n", "    power=iMu/5+1\n", "    return 10**power\n", "\n", "def distanceconverr(iMu,iMuErr):\n", "    power=iMu/5+1\n", "    const=math.log(10)/5.\n", "    return const*(10**power)*iMuErr\n", "\n", "#only reads data up to z=0.1.\n", "def load(iLabel,iMaxZ=0.1):\n", "    redshift=np.array([])\n", "    distance=np.array([])\n", "    distance_err=np.array([])\n", "    with open(iLabel,'r') as csvfile:\n", "        plots = csv.reader(csvfile, delimiter='\\t')\n", "        for row in plots:\n", "            if float(row[1]) > iMaxZ:\n", "                continue\n", "            redshift = np.append(redshift,float(row[1]))\n", "            distance = np.append(distance,distanceconv(float(row[2])))\n", "            distance_err = np.append(distance_err,distanceconverr(float(row[2]),float(row[3])))\n", "    return redshift,distance,distance_err  \n", "        \n", "#Now let's run the regression again\n", "def variance(isamples):\n", "    mean=isamples.mean()\n", "    n=len(isamples)\n", "    tot=0\n", "    for pVal in isamples:\n", "        tot+=(pVal-mean)**2\n", "    return tot/n\n", "\n", "def covariance(ixs,iys):\n", "    meanx=ixs.mean()\n", "    meany=iys.mean()\n", "    n=len(ixs)\n", "    tot=0\n", "    for i0 in range(len(ixs)):\n", "        tot+=(ixs[i0]-meanx)*(iys[i0]-meany)\n", "    return tot/n\n", "\n", "def linear(ix,ia,ib):\n", "    return ia*ix+ib\n", "\n", "redshift,distance,distance_err=load(label)\n", "var=variance(redshift)\n", "cov=covariance(redshift,distance)\n", "A=cov/var\n", "const=distance.mean()-A*redshift.mean()\n", "xvals = np.linspace(0,0.1,100)\n", "yvals = []\n", "for pX in xvals:\n", "    yvals.append(linear(pX,A,const))\n", "\n", "plt.plot(xvals,yvals)\n", "plt.errorbar(redshift,distance,yerr=distance_err,marker='.',linestyle = 'None', color = 'black')\n", "plt.xlabel(\"z(redshift)\")\n", "plt.ylabel(\"distance(pc)\")\n", "plt.show()"]}, {"cell_type": "markdown", "id": "e15fd847", "metadata": {"tags": ["learner", "md", "lect_06"]}, "source": ["Now that we have loaded the data, let's actually look at the residuals. What this means is that we are going to compute the difference between the mean predicted value in $y$ and the true data points. "]}, {"cell_type": "code", "execution_count": null, "id": "ebede43b", "metadata": {"tags": ["learner", "py", "lect_06", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L6.6-runcell02\n", "\n", "def residualsComp(redshift,distance,distance_err):\n", "    #Compute residuals\n", "    residuals=np.array([])\n", "    for i0 in range(len(redshift)):\n", "        pResid=linear(redshift[i0],A,const)-distance[i0]\n", "        residuals = np.append(residuals,pResid/distance_err[i0])\n", "    \n", "    #Make a histogram\n", "    y0, bin_edges = np.histogram(residuals, bins=30)\n", "    bin_centers = 0.5*(bin_edges[1:] + bin_edges[:-1])\n", "    norm0=len(residuals)*(bin_edges[-1]-bin_edges[0])/30.\n", "    plt.errorbar(bin_centers,y0/norm0,yerr=y0**0.5/norm0,marker='.',drawstyle = 'steps-mid')\n", "    \n", "    #Plot a Gaussian\n", "    k=np.arange(bin_edges[0],bin_edges[-1],0.05)\n", "    normal=stats.norm.pdf(k,0,1)\n", "    #First let's look at the moments \n", "    normalpoints=stats.norm.rvs(0,1,1000)\n", "    print_moments(residuals,\"residuals\")\n", "    print_moments(normalpoints,\"normal distribution\")\n", "\n", "    #Now let's plot it\n", "    plt.plot(k,normal,'o-')\n", "    plt.xlabel(\"number of successes\")\n", "    plt.ylabel(\"probability\")\n", "    plt.show()\n", "    return residuals\n", "\n", "residuals=residualsComp(redshift,distance,distance_err)"]}, {"cell_type": "markdown", "id": "94b824f4", "metadata": {"tags": ["learner", "md", "lect_06"]}, "source": ["What we see is that the mean of the residuals is very close to 0 with a variance of 1, and a skew and kurtosis really close to a normal distribution. This looks likes our residuals are Gaussian (in fact, we have plotted a Gaussian distribution on top of our fit). What do you think that means for our fit? \n", "\n", "Recall that a $\\chi^{2}$ distribution is defined as a sum of $N$ Gaussian distributed variables with mean $0$ and width $1$. For a fit this means that if we compute the residuals divided by their uncertainties, we get the value\n", "\n", "$\\chi^{2} = \\sum_{i} \\left(\\frac{y_{i}-f(x{i})}{\\sigma}\\right)^{2}$ \n", "\n", "which should be distributed by a $\\chi^{2}$ distribution. This is why we call this parameter $\\chi^2$. I know this sounds a little circular, but it does make sense.  Anyway the fit should be distributed by a $\\chi^{2}$ distribution given by the number of degrees of freedom, which in this case is the number of points minus the number of parameters we floated in the fit, which in this case is $2$. \n", "\n", "Let's check it out. "]}, {"cell_type": "code", "execution_count": null, "id": "e032b337", "metadata": {"tags": ["learner", "py", "lect_06", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L6.6-runcell03\n", "\n", "#now let's look at the chi2\n", "chi2=np.sum(residuals**2)\n", "\n", "print(\"Total chi2:\",chi2,\"NDOF\",len(residuals)-2)\n", "print(\"Normalized chi2:\",chi2/(len(residuals)-2))\n", "print(\"Probability of chi2:\",1-stats.chi2.cdf(chi2,(len(residuals)-2)))\n", "print()\n", "\n", "#Let's plot it for good measure too\n", "x = np.linspace(0,len(residuals)*2)\n", "chi2d=stats.chi2.pdf(x,len(residuals-2)) # 40 bins\n", "plt.plot(x,chi2d,label='chi2')\n", "plt.axvline(chi2, c='red')\n", "plt.legend(loc='lower right')\n", "\n", "ndof=(len(residuals)-2)\n", "chi2ppf0=stats.chi2.ppf(0.5,ndof)\n", "chi2ppf1=stats.chi2.ppf(0.15,ndof)\n", "chi2ppf2=stats.chi2.ppf(0.85,ndof)\n", "#chi2ppf1=stats.chi2.ppf(0.025,ndof)\n", "#chi2ppf2=stats.chi2.ppf(1-0.025,ndof)\n", "print(\"Central Vvalue\",chi2ppf0)\n", "print(\"Sigma Low\",chi2ppf1-chi2ppf0)\n", "print(\"Sigma High\",chi2ppf2-chi2ppf0)\n", "\n", "plt.axvline(chi2ppf1, c='blue')\n", "plt.axvline(chi2ppf2, c='blue')\n", "plt.show()"]}, {"cell_type": "markdown", "id": "e614e944", "metadata": {"tags": ["learner", "md", "lect_06"]}, "source": ["We see that the $\\chi^{2}$ value is very close to the number of degrees of freedom, with a normalized $\\chi^{2}/NDF \\approx1$, that is a legitimately good fit! Let's loosen the data by increasing the overall range of redshifts that we will fit, since here we only loaded and fit redshifts up to 0.1."]}, {"cell_type": "markdown", "id": "f208eee9", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='exercises_6_6'></a>     \n", "\n", "| [Top](#section_6_0) | [Restart Section](#section_6_6) | [Next Section](#section_6_7) |\n"]}, {"cell_type": "markdown", "id": "c87f8921", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-6.6.1</span>\n", "\n", "Fill in the blank: Anything above ___ for chi squared probability is a sign of a good fit. Enter your answer as a number with precision 1e-2."]}, {"cell_type": "markdown", "id": "cc95fbdd", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-6.6.2</span>\n", "\n", "Fill in the blank: For a normal distribution, a good fit has a normalized chi squared value near ___ . Enter your answer as a number with precision 1e-2."]}, {"cell_type": "markdown", "id": "5cfc7106", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-6.6.3</span>\n", "\n", "Ok, now let's repeat the linear fit above without a cut on the redshift. Let's run our full slew of metrics on it like we did before. Complete the code below and analyze the output.\n", "\n", "What is the value of the normalized (or reduced) chi-square metric? Is this a good fit or a bad fit? Enter your answer as a number with precision 1e-2."]}, {"cell_type": "code", "execution_count": null, "id": "06dba6c3", "metadata": {"tags": ["py", "learner_chopped", "draft"]}, "outputs": [], "source": ["#>>>RUN: L6.6.3\n", "\n", "#run regression\n", "redshift,distance,distance_err=load(label,10000)\n", "var=variance(redshift)\n", "cov=covariance(redshift,distance)\n", "A=cov/var\n", "const=distance.mean()-A*redshift.mean()\n", "xvals = np.linspace(0,1.4,100)\n", "yvals = []\n", "for pX in xvals:\n", "    yvals.append(linear(pX,A,const))\n", "\n", "#plot it\n", "plt.plot(xvals,yvals)\n", "plt.errorbar(redshift,distance,yerr=distance_err,marker='.',linestyle = 'None', color = 'black')\n", "plt.xlabel(\"z(redshift)\")\n", "plt.ylabel(\"distance(pc)\")\n", "plt.show()\n", "\n", "residuals=residualsComp(redshift,distance,distance_err)\n", "#now let's look at the chi2\n", "#YOUR CODE HERE"]}, {"cell_type": "markdown", "id": "77bf65ae", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_6_7'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L6.7 Fitting with a More Accurate Model</h2>  \n", "\n", "| [Top](#section_6_0) | [Previous Section](#section_6_6) | [Exercises](#exercises_6_7) | [Next Section](#section_6_8) |\n"]}, {"cell_type": "markdown", "id": "6cd65f9b", "metadata": {"tags": ["learner", "md", "8S50x"]}, "source": ["*The material in this section is discussed in the video **<a href=\"https://courses.mitxonline.mit.edu/learn/course/course-v1:MITxT+8.S50.1x+3T2024/block-v1:MITxT+8.S50.1x+3T2024+type@sequential+block@seq_LS6/block-v1:MITxT+8.S50.1x+3T2024+type@vertical+block@vert_LS6_vid7\" target=\"_blank\">HERE</a>.** You are encouraged to watch that video and use this notebook concurrently.*"]}, {"cell_type": "markdown", "id": "8edba907", "metadata": {"tags": ["learner", "md", "lect_07"]}, "source": ["<h3>Overview</h3>\n", "\n", "When our fits don't work well, what we need to do is come up with a better fit function. There are many ways to come up with a better function. Sometimes we just guess what a better function is. However, the best way to come up with a better function is to use our knowledge of the data and physics. \n", "\n", "When we expand the range of redshifts, what we are doing is looking at a larger range of the universe. That means that we need to come up with a better model of the universe. Fortunately, in this case there has been a lot of work towards building a better model of the universe. This work has led us to the so called Friedmann equations for the expansion of the universe. \n", "\n", "We can write the <a href=\"https://en.wikipedia.org/wiki/Friedmann_equations\" target=\"_blank\">Friedmann equations</a> in terms of the Hubble constant $h$ relative to its current value $h_{0}$ and the density parameters of the universe for matter $\\Omega_{m}$, Dark Matter $\\Omega_{DM}$, Radiation density $\\Omega_{r}$, curvature of the universe $\\Omega_{\\kappa}$ and dark Energy $\\Omega_{\\Lambda}$ as  \n", "\n", "$\\left(\\frac{\\dot{h}}{h_0}\\right)^{2} = (\\Omega_{m} + \\Omega_{\\rm DM})a^{-3}+\\Omega_{r}a^{-4} + \\Omega_{\\kappa} a^{-2} + \\Omega_{\\Lambda}$\n", "\n", "Here, the parameter $a$ is the scale parameter of the universe. This is often referred to as the cosmic scale. The way to think about this is that this is the average distance between all the galaxies in the universe at any period of time in the uiverse. When the big bang happened the scale $a\\rightarrow0$ or in otherwords all the galaxies are next to each other. As the universe expands, $a$ gets larger and at some point $a\\rightarrow\\infty$ or if the universe collapses back in on itself in a phenomenon known as the big crunch $a$ would actually tend towards 0 again. To make life simple, we tend to take $a=1$ for our current period of time. What this means is we are choosing our units conveniently so that this distance is 1 now in our galactic equation. At our time we also have that Hubble's constant $h$, which varies with the age of the universe thus becomes $h_{0}$. As a result, we can write the above equation as \n", "\n", "$$1 = (\\Omega_{m} + \\Omega_{\\rm DM})+\\Omega_{r}+ \\Omega_{\\kappa} + \\Omega_{\\Lambda}$$\n", "\n", "Now, we can further simplify this by our knowledge of the universe $\\Omega_{r}$ is the density from radiation. We don't see a huge amount of radiation appearing in our current universe, so we can set $\\Omega_{r}=0$. In reality, we can actually test this. To make life simpler, we can't really separate $\\Omega_{m}$ from $\\Omega_{\\rm DM}$, so let's call this $\\Omega_{M}$. Finally, for now let's set $\\Omega_{\\kappa}=0$. You will investigate this later. \n", "\n", "This gives us the following equation: \n", "\n", "$$1 = \\Omega_{M}+ \\Omega_{\\Lambda} {\\rm  ~or~ } \\Omega_{\\Lambda} = 1- \\Omega_{M}$$  \n", "\n", "and furthermore, we can rewrite the above equation as \n", "\n", "$$\\left(\\frac{\\dot{h}}{h_0}\\right)^{2} = (\\Omega_{M})a^{-3} + 1-\\Omega_{M}$$\n", "\n", "So at this point you might be asking what does this have to do with our data, which is distance versus redhift. It turns out we can define both distance and redshift by parameters in the above equation. First, let's define distance. What we are looking for is the distance that light travels to a galaxy far away. As we go further in distance, we go earlier in the universe. So if we know the age of the universe $t$ relative to our current time, we know that the light travelled $d=ct$. Anyway, let's derive $t$ as a function of our scale parameter $a$ to see how it evolves. If it's not exactly clear what is going on, just wait. \n", "\n", "Hubbles constant $h$ is a measure of the rate of expansion of the universe. This is just equal to the time derivative of the scale parameter $a$, or in other words:\n", "\n", "$$\\frac{da}{dt}= h =h_{0} \\sqrt{(\\Omega_{M})a^{-3} + 1-\\Omega_{M}}$$\n", "\n", "From this we can separate our equation into parts to notice that \n", "\n", "$$\\int h_{0} dt = h_{0} t = \\int \\frac{da}{\\sqrt{(\\Omega_{M})a^{-3} + 1-\\Omega_{M}}}$$\n", "\n", "Finally, the distance that light travels for a time $t$ is $d=ct$, so we can define distance as \n", "\n", "$$ d(a) = ct = \\frac{c}{h_{0}}\\int \\frac{da}{\\sqrt{(\\Omega_{M})a^{-3} + 1-\\Omega_{M}}}$$\n", "\n", "This $d$ is often referred to as the comoving distance, this is the distance on your y axis. \n", "\n", "Now the x-axis is redshift. Redshift is how much light got stretched compared to the present day. As the universe expands, it also stretches light outwards. That's because light moving in an expanding medium will stretch out. The redshift is thus proportional to our scale parameter $a$. Since the redshift is currently $0$ and proportional to $a$ we have that for $a=1$ $z=0$, and for $a\\rightarrow0$ $z\\rightarrow\\infty$ or $z\\propto 1/a$, this allows us to immediatly write the value for $z$ as below\n", "\n", "$$z = \\frac{1}{a}-1$$\n", "\n", "Likewise\n", "\n", "$$a=\\frac{1}{1+z}$$\n", "\n", "this finally gives us a function $d(z)$\n", "\n", "$$ d(z) = ct = \\frac{c}{h_{0}}\\int_{0}^{z} \\frac{dz^{\\prime}}{\\sqrt{\\Omega_{M}(1+z^{\\prime})^{3} + 1-\\Omega_{M}}}$$\n", "\n", "Where now we have finally put bounds on the integral to correspond to the actual value of $z$ we are choosing. Note that we integrate over a range of $z^{\\prime}$ to get the value of $z$ that we want to use. The above is referred to as the co-moving distance. \n", "\n", "Finally, before we actually go to our data, we need to correct the co-moving distance by a correction to account for the fact that we are measuring the luminosity distance and not the co-moving distance.  The luminosity distance is defined as  the change in brightness of a star, or the flux $S$ at any given point is defined as the surface area of the luminosity distance times the luminosity $L$\n", "\n", "$$ 4\\pi d_{L}^{2} L = S$$  \n", "\n", "Now since the universe is expanding as light is moving out, the light is getting redishifted or in otherwords since the universe is expanding it takes longer for the same wave to pass by you since light moves at a constant speed, but space is expanding. This additional length is just the additional redshift scale scale ie $d_{L} = (1+z) d_{\\rm original}$. This means we finally write our equation as: \n", "\n", "$$ d(z) = ct^{\\prime} = (1+z)ct = (1+z)\\frac{c}{h_{0}}\\int_{0}^{z} \\frac{dz^{\\prime}}{\\sqrt{\\Omega_{M}(1+z^{\\prime})^{3} + 1-\\Omega_{M}}}$$\n"]}, {"cell_type": "markdown", "id": "c7a51089", "metadata": {"tags": ["learner", "md", "lect_07"]}, "source": ["<h3>Numerical properties of the universe</h3>\n", "\n", "We can take the above integral and now write the whole thing numerically. This is just going to be the speed of light over Hubble's constant in the right units $(c/h_{0})$ multiplied by the integral and the multiplied by $1+z$. That's it. \n", "\n", "To perform the integral, all we do is divide our range into 100 pieces and no a numerical integral by doing a for loop. In other words, we write \n", "\n", "$$ d(z) = \\frac{c}{h_{0}}(1+z) \\sum_{i=0}^{i=100} \\frac{dz}{\\sqrt{\\Omega_{M}(1+z_{i}^{\\prime})^{3} + 1-\\Omega_{M}}}$$\n", "\n", "where $dz=z/100$ and $z_{i}= dz \\times i$. Let's go ahead and write this out. \n", "\n"]}, {"cell_type": "code", "execution_count": null, "id": "64de7f26", "metadata": {"tags": ["learner", "py", "lect_07", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L6.7-runcell02\n", "\n", "#We are not going to plot the fit first, let's just use our barrage of statistics to check if its ok\n", "def hubble(z,Om):\n", "    pVal=Om*(1+z)**3+(1.-Om)\n", "    return np.sqrt(pVal)\n", "\n", "def lumidistance(z,h0,Om):\n", "    integral=0\n", "    nint=100\n", "    for i0 in range(nint):\n", "        zp=z*float(i0)/100.\n", "        dz=z/float(nint)\n", "        pVal=1./(1e-5+hubble(zp,Om))\n", "        integral += pVal*dz\n", "    d=(1.+z)*integral*(1e6*3e5/h0)\n", "    return d\n", "\n", "print(\"test Lumi\",lumidistance(1,70,0.3))\n"]}, {"cell_type": "markdown", "id": "536483ce", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='exercises_6_7'></a>     \n", "\n", "| [Top](#section_6_0) | [Restart Section](#section_6_7) | [Next Section](#section_6_8) |\n"]}, {"cell_type": "markdown", "id": "366c9fb5", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-6.7.1</span>\n", "\n", "How would more dark energy change the luminosity distance over redshift? You can approach this problem mathematically, or simply change the apppropriate parameter in the function `lumidistance()` to compute the luminosity distance at different values.\n", "\n", "Based on your observations, if there is more dark energy, the luminosity distance should:\n", "\n", "- get larger\n", "- get smaller\n", "- stay the same\n"]}, {"cell_type": "code", "execution_count": null, "id": "98588ef5", "metadata": {"tags": ["draft", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>EXERCISE: L6.7.1\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "pass"]}, {"cell_type": "markdown", "id": "7741827a", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_6_8'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L6.8 Fit to Full Cosmological Model</h2>     \n", "\n", "| [Top](#section_6_0) | [Previous Section](#section_6_7) | [Exercises](#exercises_6_8) |\n"]}, {"cell_type": "markdown", "id": "7f85588d", "metadata": {"tags": ["learner", "md", "8S50x"]}, "source": ["*The material in this section is discussed in the video **<a href=\"https://courses.mitxonline.mit.edu/learn/course/course-v1:MITxT+8.S50.1x+3T2024/block-v1:MITxT+8.S50.1x+3T2024+type@sequential+block@seq_LS6/block-v1:MITxT+8.S50.1x+3T2024+type@vertical+block@vert_LS6_vid8\" target=\"_blank\">HERE</a>.** You are encouraged to watch that video and use this notebook concurrently.*"]}, {"cell_type": "markdown", "id": "c9f92078", "metadata": {"tags": ["learner", "md"]}, "source": ["Now let's go and run our fit function that included the parameters of the universe. As a first pass, let's just pipe this through lmfit and see if it works. "]}, {"cell_type": "code", "execution_count": null, "id": "aec03b27", "metadata": {"tags": ["learner", "py", "lect_08", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L6.8-runcell01\n", "\n", "import lmfit\n", "\n", "model  = lmfit.Model(lumidistance)\n", "p = model.make_params(h0=70,Om=0.2)\n", "result = model.fit(data=distance, params=p, z=redshift, weights=1./distance_err)\n", "lmfit.report_fit(result)\n", "plt.figure()\n", "result.plot()"]}, {"cell_type": "markdown", "id": "9c9398f1", "metadata": {"tags": ["learner", "md", "lect_08"]}, "source": ["Now, since we are experts at fitting, let's go ahead and compute the log likelihood . We are going to minimize the (negative of) loglikelihood with `scipy.optimize` in order to do the fit."]}, {"cell_type": "code", "execution_count": null, "id": "eb1f572a", "metadata": {"tags": ["learner", "py", "lect_08", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L6.8-runcell02\n", "from scipy import optimize as opt \n", "\n", "def loglike(x):\n", "    lTot=0\n", "    for i0 in range(len(redshift)):\n", "        xtest=lumidistance(redshift[i0],x[0],x[1])\n", "        #lTot = lTot+(distance[i0]-xtest)**2\n", "        lTot = lTot+((1./distance_err[i0])**2)*(distance[i0]-xtest)**2\n", "    return lTot #*0.5 The above is 2 times loglike\n", "\n", "def residuals(x):\n", "    residuals=np.array([])\n", "    for i0 in range(len(redshift)):\n", "        pResid=lumidistance(redshift[i0],sol.x[0],sol.x[1])-distance[i0]\n", "        residuals = np.append(residuals,pResid/distance_err[i0])\n", "    return residuals\n", "\n", "\n", "x0 = np.array([60.,0.2])\n", "ps = [x0]\n", "bnds = ((0, 1000), (0, 1.0))\n", "sol=opt.minimize(loglike, x0,bounds=bnds, tol=1e-6)\n", "print(sol)\n", "residuals=residuals(sol.x)\n", "print_moments(residuals,\"residuals\")\n", "chi2=np.sum(residuals**2)\n", "print(\"Total chi2:\",chi2,\"NDOF\",len(residuals)-2)\n", "print(\"Normalized chi2:\",chi2/(len(residuals)-2))\n", "print(\"Probability of chi2:\",1-stats.chi2.cdf(chi2,(len(residuals)-2)))\n", "\n", "#Let's plot it for good measure too\n", "x = np.linspace(0,len(residuals)*2)\n", "chi2d=stats.chi2.pdf(x,len(residuals-2)) # 40 bins\n", "plt.plot(x,chi2d,label='chi2')\n", "plt.axvline(chi2, c='red')\n", "plt.legend(loc='lower right')\n", "plt.show();"]}, {"cell_type": "markdown", "id": "97d7e1b9", "metadata": {"tags": ["learner", "md", "lect_08"]}, "source": ["What can we say about this fit, is there something off?\n", "\n", "Let's plot the residuals and the fit function and also scan the likelihood for our parameter uncertainties. There is one thing off, can you figure it out?\n"]}, {"cell_type": "code", "execution_count": null, "id": "f856518b", "metadata": {"tags": ["learner", "py", "lect_08", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L6.8-runcell03\n", "\n", "#Plot it against the data\n", "xvals = np.linspace(0,1.4,100)\n", "yvals = []\n", "for pX in xvals:\n", "    yvals.append(lumidistance(pX,sol.x[0],sol.x[1]))\n", "\n", "plt.errorbar(redshift,distance,yerr=distance_err,marker='.',linestyle = 'None', color = 'black')\n", "plt.plot(xvals,yvals)\n", "plt.show()\n", "\n", "#Histogram the residuals\n", "y0, bin_edges = np.histogram(residuals, bins=30)\n", "bin_centers = 0.5*(bin_edges[1:] + bin_edges[:-1])\n", "norm0=len(residuals)*(bin_edges[-1]-bin_edges[0])/30.\n", "plt.errorbar(bin_centers,y0/norm0,yerr=y0**0.5/norm0,marker='.',drawstyle = 'steps-mid')\n", "k=np.arange(bin_edges[0],bin_edges[-1],0.05)\n", "normal=stats.norm.pdf(k,0,1)\n", "plt.plot(k,normal,'o-')\n", "plt.show()\n", "\n", "x = np.linspace(len(residuals)*0.5,len(residuals)*1.5)\n", "chi2d=stats.chi2.pdf(x,len(residuals-2)) # 40 bins\n", "plt.plot(x,chi2d,label='chi2')\n", "plt.axvline(chi2, c='red')\n", "plt.legend(loc='lower right')\n", "plt.show()\n", "\n", "\n"]}, {"cell_type": "markdown", "id": "fff3f8e5", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='exercises_6_8'></a>   \n", "\n", "| [Top](#section_6_0) | [Restart Section](#section_6_8) |\n"]}, {"cell_type": "markdown", "id": "bff37c58", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-6.8.1</span>\n", "\n", "So, given the Friedmann equations, we can add back the curvature term:\n", "\n", "$$ d(z) = ct^{\\prime} = (1+z)ct = (1+z)\\frac{c}{h_{0}}\\int_{0}^{z} \\frac{dz^{\\prime}}{\\sqrt{\\Omega_{M}\\left(1+z^{\\prime}\\right)^{3} + \\Omega_{\\kappa}\\left(1+z^{\\prime}\\right)^{2}+ 1-\\Omega_{M}-\\Omega_{\\kappa}}}$$\n", "\n", "Adjust the `lumidistance()` function to fit for curvature. What is the value of $\\Omega_{\\kappa}$ if we perform a new fit to the data? What do you think this is implying? \n", "\n", "Enter your answer as a number with precision 1e-2.\n"]}, {"cell_type": "code", "execution_count": null, "id": "6ddb6375", "metadata": {"tags": ["draft", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>EXERCISE: L6.8.1\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "def hubble_curve(z,Om,OmK):\n", "    return #YOUR CODE HERE\n", "\n", "def lumidistance_curve(z,h0,Om,OmK):\n", "    return #YOUR CODE HERE\n", "\n", "\n", "model  = lmfit.Model(lumidistance_curve)\n", "p = model.make_params(h0=70,Om=0.2,OmK=0.0)\n", "result = model.fit(data=distance, params=p, z=redshift, weights=1./distance_err)\n", "lmfit.report_fit(result)\n", "result.plot();\n"]}], "metadata": {"celltoolbar": "Tags", "kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}