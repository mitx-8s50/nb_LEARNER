{"cells": [{"cell_type": "markdown", "id": "f44134fe", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<hr style=\"height: 1px;\">\n", "<i>This notebook was authored by the 8.S50x Course Team, Copyright 2022 MIT All Rights Reserved.</i>\n", "<hr style=\"height: 1px;\">\n", "<br>\n", "\n", "<h1>Lesson 7: Correlations</h1>\n"]}, {"cell_type": "markdown", "id": "7da993b2", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_7_0'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L7.0 Overview</h2>\n"]}, {"cell_type": "markdown", "id": "5bf91bc0", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<h3>Navigation</h3>\n", "\n", "<table style=\"width:100%\">\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_7_1\">L7.1 Understanding Best Fit (Revisited)</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_7_1\">L7.1 Exercises</a></td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_7_2\">L7.2 Minimizing on a Surface (1D Scan)</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_7_2\">L7.2 Exercises</a></td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_7_3\">L7.3 Minimizing on a Surface (2D Scan)</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_7_3\">L7.3 Exercises</a></td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_7_4\">L7.4 Correlations Between Fit Parameters: Part 1</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_7_4\">L7.4 Exercises</a></td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_7_5\">L7.5 Correlations Between Fit Parameters: Part 2</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_7_5\">L7.5 Exercises</a></td>\n", "    </tr>\n", "</table>\n", "\n"]}, {"cell_type": "markdown", "id": "ebd13d85", "metadata": {"tags": ["learner", "catsoop_00", "md"]}, "source": ["<h3>Learning Objectives</h3>\n", "\n", "In this lecture, we wil explore the nature of correlated variables and how they impact our interpretation of results. Furthermore, we will understand how to go away from correlations to see how we can reduce their impact. Additionally, we will start to scan the parameters of our fit beyond just finding the minimum. By scanning, we start to learn the subtleties of each of these setups, and we start to get deep insight into the nature of what we are minimizing and how to interpret it. \n"]}, {"cell_type": "markdown", "id": "4105ab2e", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<h3>Importing Data (Colab Only)</h3>\n", "\n", "If you are in a Google Colab environment, run the cell below to import the data for this notebook. Otherwise, if you have downloaded the course repository, you do not have to run the cell below.\n", "\n", "See the source and attribution information below:\n", "\n", ">data: data/L05/events_a4_1space.dat, data/L05/events_a8_1space.dat <br>\n", ">source: https://www.auger.org/index.php/science/data <br>\n", ">source : https://arxiv.org/abs/1709.07321 <br>\n", ">attribution: Pierre Auger Collaboration, arXiv:1709.07321v1 <br>\n", ">license type: https://creativecommons.org/licenses/by-sa/4.0/ "]}, {"cell_type": "code", "execution_count": null, "id": "65a5b9ff", "metadata": {"tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L7.0-runcell00\n", "\n", "#importing data from git repository\n", "\n", "!git init\n", "!git remote add -f origin https://github.com/mitx-8s50/nb_LEARNER/\n", "!git config core.sparseCheckout true\n", "!echo 'data/L05' >> .git/info/sparse-checkout\n", "!git pull origin main"]}, {"cell_type": "markdown", "id": "07ca5a93", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Importing Libraries</h3>\n", "\n", "Before beginning, run the cells below to import the relevant libraries for this notebook."]}, {"cell_type": "code", "execution_count": null, "id": "7b8fed76", "metadata": {"tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L7.0-runcell01\n", "\n", "# for Colab users\n", "!pip install lmfit"]}, {"cell_type": "code", "execution_count": null, "id": "33156765", "metadata": {"tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L7.0-runcell02\n", "\n", "import numpy as np                 #https://numpy.org/doc/stable/\n", "from scipy import optimize as opt  #https://docs.scipy.org/doc/scipy/reference/optimize.html\n", "import matplotlib.pyplot as plt    #https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.html\n", "import math                        #https://docs.python.org/3/library/math.html\n", "import csv                         #https://docs.python.org/3/library/csv.html\n", "import lmfit                       #https://lmfit.github.io/lmfit-py/ \n", "import scipy.stats as stats        #https://docs.scipy.org/doc/scipy/reference/stats.html\n", "from scipy.optimize.zeros import RootResults        #https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.RootResults.html\n", "from scipy.optimize.optimize import OptimizeResult  # https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.OptimizeResult.html\n", "from typing import Callable, Tuple #https://docs.python.org/3/library/typing.html#typing.Callable\n", "                                   #https://docs.python.org/3/library/typing.html\n", "import numpy.linalg as la          #https://docs.scipy.org/doc/scipy/reference/linalg.html\n", "from matplotlib.patches import Ellipse  # https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.patches.Ellipse.html\n"]}, {"cell_type": "markdown", "id": "4b676a02", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Setting Default Figure Parameters</h3>\n", "\n", "The following code cell sets default values for figure parameters."]}, {"cell_type": "code", "execution_count": null, "id": "67d4684e", "metadata": {"tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L7.0-runcell03\n", "\n", "#set plot resolution\n", "%config InlineBackend.figure_format = 'retina'\n", "\n", "#set default figure parameters\n", "plt.rcParams['figure.figsize'] = (9,6)\n", "\n", "medium_size = 12\n", "large_size = 15\n", "\n", "plt.rc('font', size=medium_size)          # default text sizes\n", "plt.rc('xtick', labelsize=medium_size)    # xtick labels\n", "plt.rc('ytick', labelsize=medium_size)    # ytick labels\n", "plt.rc('legend', fontsize=medium_size)    # legend\n", "plt.rc('axes', titlesize=large_size)      # axes title\n", "plt.rc('axes', labelsize=large_size)      # x and y labels\n", "plt.rc('figure', titlesize=large_size)    # figure title\n"]}, {"cell_type": "markdown", "id": "0ad89b66", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_7_1'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L7.1 Understanding Best Fit (Revisited)</h2>  \n", "\n", "| [Top](#section_7_0) | [Previous Section](#section_7_0) | [Exercises](#exercises_7_1) | [Next Section](#section_7_2) |\n"]}, {"cell_type": "markdown", "id": "aacea1b1", "metadata": {"tags": ["learner", "md", "8S50x"]}, "source": ["*The material in this section is discussed in the video **<a href=\"https://courses.mitxonline.mit.edu/learn/course/course-v1:MITxT+8.S50.1x+3T2022/block-v1:MITxT+8.S50.1x+3T2022+type@sequential+block@seq_LS7/block-v1:MITxT+8.S50.1x+3T2022+type@vertical+block@vert_LS7_vid1\" target=\"_blank\">HERE</a>.** You are encouraged to watch that video and use this notebook concurrently.*"]}, {"cell_type": "markdown", "id": "7aa6dba5", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Slides</h3>\n", "\n", "Run the code below to view the slides for this section, which are discussed in the related video. You can also open the slides in a separate window <a href=\"https://mitx-8s50.github.io/slides/L07/slides_L07_01.html\" target=\"_blank\">HERE</a>."]}, {"cell_type": "code", "execution_count": null, "id": "1c381a19", "metadata": {"tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L7.1-slides\n", "\n", "from IPython.display import IFrame\n", "IFrame(src='https://mitx-8s50.github.io/slides/L07/slides_L07_01.html', width=970, height=550)"]}, {"cell_type": "markdown", "id": "701f3174", "metadata": {"tags": ["learner", "md", "lect_01"]}, "source": ["<h3> What is the Chi-squared-value from a p-value? </h3>\n", "\n", "So what is the importance of this fit? This is just equal to a $\\chi^{2}$ distribution, as we have seen before. Before we delve into the details of the importance of the fit, let's just remind ourselves how to compute the p-value from a gaussian and the $\\chi^{2}$ value from the p-value or a sigma value assuming a Gaussian. \n"]}, {"cell_type": "code", "execution_count": null, "id": "b9d7a68b", "metadata": {"tags": ["learner", "py", "lect_01", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L7.1-runcell01\n", "\n", "import scipy.stats as stats\n", "\n", "sigma = 1\n", "print(stats.norm.cdf(sigma)-stats.norm.cdf(-sigma))\n", "\n", "def pval(iVal):\n", "    return stats.norm.cdf(iVal)-stats.norm.cdf(-iVal)\n", "\n", "def chi2Val(iGausSigma,iNDOF):\n", "    val=stats.chi2.ppf(pval(iGausSigma),iNDOF)\n", "    return val\n", "\n", "print(chi2Val(1,2))\n", "#print(chi2Val(1,1))\n", "#print(chi2Val(2,1))\n", "#print(chi2Val(3,1))\n", "#print(chi2Val(4,1))"]}, {"cell_type": "markdown", "id": "6b30ab91", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='exercises_7_1'></a>     \n", "\n", "| [Top](#section_7_0) | [Restart Section](#section_7_1) | [Next Section](#section_7_2) |\n"]}, {"cell_type": "markdown", "id": "c1b07b38", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-7.1.1</span>\n", "\n", "Compute the Chi-square value for 2$\\sigma$ and 2 degrees of freedom. Enter your answer as a number with precision 1e-2. \n"]}, {"cell_type": "code", "execution_count": null, "id": "a9a30c37", "metadata": {"tags": ["draft", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>EXERCISE: L7.1.1\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n"]}, {"cell_type": "markdown", "id": "9fed2264", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_7_2'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L7.2 Minimizing on a Surface (1D Scan)</h2>  \n", "\n", "| [Top](#section_7_0) | [Previous Section](#section_7_1) | [Exercises](#exercises_7_2) | [Next Section](#section_7_3) |\n"]}, {"cell_type": "markdown", "id": "1bf19be1", "metadata": {"tags": ["learner", "md", "8S50x"]}, "source": ["*The material in this section is discussed in the video **<a href=\"https://courses.mitxonline.mit.edu/learn/course/course-v1:MITxT+8.S50.1x+3T2022/block-v1:MITxT+8.S50.1x+3T2022+type@sequential+block@seq_LS7/block-v1:MITxT+8.S50.1x+3T2022+type@vertical+block@vert_LS7_vid2\" target=\"_blank\">HERE</a>.** You are encouraged to watch that video and use this notebook concurrently.*"]}, {"cell_type": "markdown", "id": "3ed2de98", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Slides</h3>\n", "\n", "Run the code below to view the slides for this section, which are discussed in the related video. You can also open the slides in a separate window <a href=\"https://mitx-8s50.github.io/slides/L07/slides_L07_02.html\" target=\"_blank\">HERE</a>."]}, {"cell_type": "code", "execution_count": null, "id": "5b99381c", "metadata": {"tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L7.2-slides\n", "\n", "from IPython.display import IFrame\n", "IFrame(src='https://mitx-8s50.github.io/slides/L07/slides_L07_02.html', width=970, height=550)"]}, {"cell_type": "markdown", "id": "49307bc7", "metadata": {"tags": ["learner", "md", "lect_02"]}, "source": ["<h3>Uncertainty on more than one parameter</h3>\n", "\n", "Previously, we explored fitting more than one parameter. That is, we minimized some error metric for more than one parameter. Let's take a closer look. As before, we'll fit the function \n", "\n", "$$\n", "\\begin{equation}\n", " f(x,a,b) = a + b \\sin(x)\n", "\\end{equation}\n", "$$\n", "\n", "to the Auger data, i.e. finding optimal choices of parameters $a$ and $b$ given some set of sample observations $y_i$ and their sample uncertainties $\\sigma_i$. One way to choose optimal parameter values is by optimizing our likelihood over both $a$ and $b$. As a reminder, likelihood defines  how probable our observed data is as a function of parameter values and a hypothesized form. At extrema, we'd expect our likelihood is minimized or \n", "\n", "$$\n", "\\begin{equation}\n", " \\frac{\\partial \\mathcal{L}}{\\partial a} = 0 \\\\\n", " \\frac{\\partial \\mathcal{L}}{\\partial b} = 0 \\\\\n", "\\end{equation}\n", "$$\n", "\n", "As we saw in L5.7, maxima of the likelihood function correspond to minima of the relevant $\\chi^2$ statistic of the form\n", "\n", "$$\n", "\\begin{equation}\n", "  \\chi^{2}(x|a,b) = \\sum_{i=1}^{N} \\frac{(y_{i}-f(x_{i},a,b))^2}{\\sigma_i^2}\n", "\\end{equation}\n", "$$\n", "\n", "where we will make the assumption that $\\sigma_i = \\sqrt{f(x_{i},a,b)}$. At the extrema,\n", "\n", "$$\n", "\\begin{eqnarray}\n", "\\frac{\\partial \\chi^{2}(x|a,b) }{\\partial a} = 0 \\\\\n", "\\frac{\\partial \\chi^{2}(x|a,b) }{\\partial b} = 0\n", "\\end{eqnarray}\n", "$$\n", "\n", "In a little bit, we're going to use this to manually write code to fit the function using the generic optimizer in `scipy`. But first, let's review how things look using `lmfit`. "]}, {"cell_type": "code", "execution_count": null, "id": "c88f8011", "metadata": {"scrolled": true, "tags": ["learner", "py", "learner_chopped", "lect_02"]}, "outputs": [], "source": ["#>>>RUN: L7.2-runcell01\n", "\n", "############ This is all code from a previous lesson\n", "import numpy as np\n", "import csv\n", "import math\n", "import lmfit\n", "import matplotlib.pyplot as plt\n", "from scipy import optimize as opt\n", "\n", "def rad(iTheta):\n", "    return iTheta/180. * math.pi\n", "\n", "def rad1(iTheta):\n", "    return iTheta/180. * math.pi-math.pi\n", "\n", "def load(label):\n", "    dec=np.array([])\n", "    ra=np.array([])\n", "    az=np.array([])\n", "    with open(label,'r') as csvfile:\n", "        plots = csv.reader(csvfile,delimiter=' ')\n", "        for pRow in plots:\n", "            if '#' in pRow[0] or pRow[0]=='':\n", "                continue\n", "            dec = np.append(dec,rad(float(pRow[2])))\n", "            ra  = np.append(ra,rad1(float(pRow[3])))\n", "            az  = np.append(az,rad(float(pRow[4])))\n", "    return dec,ra,az\n", "\n", "def prephist(iRA):\n", "    y0, bin_edges = np.histogram(iRA, bins=30)\n", "    x0 = 0.5*(bin_edges[1:] + bin_edges[:-1])\n", "    y0 = y0.astype('float')\n", "    return x0,y0,1./(y0**0.5)\n", "\n", "label8='data/L05/events_a8_1space.dat'\n", "\n", "dec,ra8,az=load(label8)\n", "xhist,yhist,xweights=prephist(ra8)\n", "\n", "\n", "########## Tlast fit code\n", "\n", "def fnew(x,a,b):\n", "    pVal=b*np.sin(x)\n", "    return a+pVal\n", "\n", "model  = lmfit.Model(fnew)\n", "p = model.make_params(a=1000,b=10)\n", "result = model.fit(data=yhist,x=xhist, params=p, weights=xweights)\n", "lmfit.report_fit(result)\n", "plt.figure()\n", "result.plot()\n", "plt.show()\n"]}, {"cell_type": "markdown", "id": "5d85bf1a", "metadata": {"tags": ["learner", "md", "lect_02"]}, "source": ["<h3>Re-run the Fit</h3>\n", "\n", "Now, what we are going to do is re-run the fit. Here we'll go through the $\\chi^2$ minimization process semi-manually, using `scipy.opt`. The code is explained in the Lesson video.\n", "\n", "Strategically, what we are trying to do is write the $\\chi^{2}$ value and use our normal minimizer tools to find the minimum parameters. \n"]}, {"cell_type": "code", "execution_count": null, "id": "4b5fc9a6", "metadata": {"tags": ["learner", "py", "lect_02", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L7.2-runcell02\n", "\n", "#This is the new code\n", "def chi2(iX):\n", "    #Note that this function depends on xhist and yhist being defined already.\n", "    #This is bad practice in general, but we do it here for convenience.\n", "    #After all, this code isn't reused elsewhere.\n", "    #MAKE SURE YOU RUN prephist(ra8) to define yhist\n", "    assert len(iX) == 2\n", "    lTot=0\n", "    for val in range(len(yhist)):\n", "        xtest=fnew(xhist[val],iX[0],iX[1])\n", "        lTot += (1./(xtest+1e-5))*(yhist[val]-xtest)**2\n", "    return lTot\n", "\n", "#First we minimize\n", "x0 = np.array([1000,10]) # initial conditions\n", "ps = [x0]\n", "sol=opt.minimize(chi2, x0)\n", "print(sol)"]}, {"cell_type": "markdown", "id": "64747250", "metadata": {"tags": ["learner", "md", "lect_02"]}, "source": ["<h3>Visualizing the Uncertainty</h3>\n", "\n", "Now, given that we have found the minimum, let's look at how the $\\chi^{2}$ value changes from the minimum. The important thing is to remember that from <a href=\"https://en.wikipedia.org/wiki/Wilks%27_theorem\" target=\"_blank\">Wilk's Thereom</a>, the uncertainty of a parameter can be obtained by taking $\\Delta \\chi^{2}$ from the minimum value to be equal to 1. Let's go ahead and plot the minimum and look at what $\\Delta \\chi^{2}=1$ looks like. \n", "\n", "Note, where the red and blue lines cross corresponds to the one $\\sigma$ fluctuation up and down from the minimum."]}, {"cell_type": "code", "execution_count": null, "id": "26616a72", "metadata": {"tags": ["learner", "py", "lect_02", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L7.2-runcell03\n", "\n", "#Scan near the minimum of each value\n", "x = np.linspace(sol.x[0]*0.99,sol.x[0]*1.01, 100)\n", "y = np.linspace(sol.x[1]*0.5,sol.x[1]*1.5, 100)\n", "\n", "#Now let's fix one parameter at the minimum, and profile the other\n", "plt.plot(x, chi2([x,sol.x[1]]),label='chi2');\n", "plt.axhline(sol.fun+1, c='red')\n", "plt.xlabel(\"a-value\")\n", "plt.ylabel(\"$\\chi^{2}$\")\n", "plt.show()\n", "\n", "#Now for the other parameter\n", "plt.plot(y, chi2([sol.x[0],y]),label='chi2');\n", "plt.axhline(sol.fun+1, c='red')\n", "plt.xlabel(\"b-value\")\n", "plt.ylabel(\"$\\chi^{2}$\")\n", "plt.show()"]}, {"cell_type": "markdown", "id": "8cd63491", "metadata": {"tags": ["learner", "md", "lect_02"]}, "source": ["<h3>Computing the Best Fit and Uncertainty</h3>\n", "\n", "We can code up this algorithm and compute the best fit and the uncertainty on the best fit parameters. Let's go ahead and code this up. "]}, {"cell_type": "code", "execution_count": null, "id": "b991443a", "metadata": {"tags": ["learner", "py", "lect_02", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L7.2-runcell04\n", "\n", "######## Now let's use a numerical solver to find the points at which a function crosses zero (root solver)\n", "def chi2minX(xval, delta_chi2=1):\n", "    val=chi2([xval,sol.x[1]])\n", "    minval=chi2(sol.x) + delta_chi2\n", "    return val-minval\n", "\n", "def chi2minY(yval, delta_chi2=1):\n", "    val=chi2([sol.x[0],yval])\n", "    minval=chi2(sol.x) + delta_chi2\n", "    return val-minval\n", "\n", "def chi2uncX(sol):\n", "    solX1=opt.root_scalar(chi2minX,bracket=[sol.x[0], sol.x[0]*1.02],method='brentq')\n", "    solX2=opt.root_scalar(chi2minX,bracket=[sol.x[0]*0.98, sol.x[0]],method='brentq')\n", "    print(\"a:\",sol.x[0],\"+/-\",abs(solX2.root-solX1.root)/2.)\n", "    print(\"Reminder the Poisson uncertainty would be:\",math.sqrt(sol.x[0]/40))\n", "    return solX1, solX2\n", "\n", "def chi2uncY(sol):\n", "    solY1=opt.root_scalar(chi2minY,bracket=[sol.x[1],    sol.x[1]*1.2],method='brentq')\n", "    solY2=opt.root_scalar(chi2minY,bracket=[sol.x[1]*0.8, sol.x[1]],method='brentq')\n", "    print(\"b:\",sol.x[1],\"+/-\",abs(solY2.root-solY1.root)/2.)\n", "    return solY1, solY2\n", "\n", "solX1, solX2 = chi2uncX(sol)\n", "solY1, solY2 = chi2uncY(sol)"]}, {"cell_type": "markdown", "id": "0d4afdb6", "metadata": {"tags": ["learner", "md", "lect_02"]}, "source": ["<h3>So what have we done?</h3>\n", "\n", "We've used the optimizer to minimize the $\\chi^{2}$ values in 2D, and we have obtained an uncertainty by looking at $\\Delta \\chi^{2}$. You can see that our semi-manual manipulation got us to the same parameter estimates as the `lmfit` example. Additionally, by looking at slices of $\\chi^{2}$ in each of the parameters, we obtained the same uncertainties. \n", "\n", "As a side note, our final fitted uncertainty is a little bit larger than the Poisson uncertainty. In fact, for all fits there is a rule that our uncertainties on any parameter have to be larger than a certain number. This bound is known as the Cram\u00e9r-Rao bound. I won't derive it or go into in detail, but the Cram\u00e9r-Rao bound states.\n", "\n", "$$\n", "\\begin{equation}\n", "\\mathrm{Var}(\\theta|\\hat{\\theta}) \\geq \\frac{1}{\\mathcal{I}(\\theta)} \\\\\n", "\\mathcal{I}(\\theta) = E_{p(X|\\theta)}\\left[-\\frac{\\partial^{2}}{\\partial\\theta^{2}}\\log\\left(p\\left(x|\\theta\\right)\\right)\\right]\n", "\\end{equation}\n", "$$\n", "\n", "Where $\\mathrm{Var}(\\theta|\\hat{\\theta})$ is the variance on our fitted parameter and we call $\\mathcal{I}(\\theta)$ the <a href=\"https://en.wikipedia.org/wiki/Fisher_information\" target=\"_blank\">Fisher information</a>, which generalizes the variance over binomial sampled distributions and, in its simplest form, is equal the inverse of the variance of a binomial distribution, which in turn is the more precise description of a poisson distribution. Hence the fact that our uncertainty is larger than the Poisson uncertainty. While I don't want to go into this more, this result is powerful because it means that there is limit to when you should stop searching for a best fit. "]}, {"cell_type": "markdown", "id": "290142a4", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='exercises_7_2'></a>     \n", "\n", "| [Top](#section_7_0) | [Restart Section](#section_7_2) | [Next Section](#section_7_3) |\n"]}, {"cell_type": "markdown", "id": "db70b48a", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-7.2.1</span>\n", "\n", "What are the 2$\\sigma$ bounds (that is, 95.45% confidence interval values) of $a$ and $b$ in the fit above? Enter your answer as a list of numbers with precision 1 (the nearest whole number): `[a_lower, a_upper, b_lower, b_upper]`"]}, {"cell_type": "code", "execution_count": null, "id": "725bd600", "metadata": {"tags": ["draft", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>EXERCISE: L7.2.1\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "#For a given parameter, assume everything else is determined, so we're\n", "# working with 1 degree of freedom.\n", "# First, we want to know the chi-square value at which 2 standard deivations (95.45% of the probability)\n", "# in the chi-square distribution is for values less extreme than that value.\n", "ndof=1\n", "pval=0.9545\n", "val = stats.chi2.ppf(pval,ndof)\n", "print(\"Delta Chi-square:\", val)\n", "\n", "def minXfunc(x):\n", "    return chi2minX(x, delta_chi2=val)\n", "\n", "def minYfunc(x):\n", "    return chi2minY(x, delta_chi2=val)\n", "\n", "def chi2unc(sol, sol_index, min_func, unc_prop_guess):\n", "     #insert code here\n", "    return valmin,valmax\n", "\n", "a1, a2 = chi2unc(sol, 0, minXfunc, 0.08)\n", "b1, b2 = chi2unc(sol, 1, minYfunc, 0.4)\n", "\n", "print(f\"a bounds: [{a1}, {a2}]\")\n", "print(f\"b bounds: [{b1}, {b2}]\")"]}, {"cell_type": "markdown", "id": "14c0d887", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_7_3'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L7.3 Minimizing on a Surface (2D Scan)</h2>  \n", "\n", "| [Top](#section_7_0) | [Previous Section](#section_7_2) | [Exercises](#exercises_7_3) | [Next Section](#section_7_4) |\n"]}, {"cell_type": "markdown", "id": "15b7ff77", "metadata": {"tags": ["learner", "md", "8S50x"]}, "source": ["*The material in this section is discussed in the video **<a href=\"https://courses.mitxonline.mit.edu/learn/course/course-v1:MITxT+8.S50.1x+3T2022/block-v1:MITxT+8.S50.1x+3T2022+type@sequential+block@seq_LS7/block-v1:MITxT+8.S50.1x+3T2022+type@vertical+block@vert_LS7_vid3\" target=\"_blank\">HERE</a>.** You are encouraged to watch that video and use this notebook concurrently.*"]}, {"cell_type": "markdown", "id": "a3569b6a", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Slides</h3>\n", "\n", "Run the code below to view the slides for this section, which are discussed in the related video. You can also open the slides in a separate window <a href=\"https://mitx-8s50.github.io/slides/L07/slides_L07_03.html\" target=\"_blank\">HERE</a>."]}, {"cell_type": "code", "execution_count": null, "id": "71725fcc", "metadata": {"tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L7.3-slides\n", "\n", "from IPython.display import IFrame\n", "IFrame(src='https://mitx-8s50.github.io/slides/L07/slides_L07_03.html', width=970, height=550)"]}, {"cell_type": "markdown", "id": "e1cc1ae9", "metadata": {"tags": ["learner", "md", "lect_03"]}, "source": ["<h3>Overview</h3>\n", "    \n", "Now given our minimization worked in 1D for both variables $a$ and $b$, what about if we scan both variables simultaneously and then look at how the minimum behaves. To do this, we can start by first scaning around the best fit values of $a$ and $b$ and just plotting the value of our $\\chi^{2}$ minimization. From this space, we can start to relate this back to the 2D version of Wilk's theorem in the slides above. \n", "\n", "To really visualize the whole thing let's make one more plot: how the $\\chi^2$ value depends on both parameters, i.e., in 2D. We can do this using the `meshgrid` function and plotting it. In addition, we will add some color full lines for a $\\Delta \\chi^{2}$ given by various values, we will revisit what these values are in a little bit."]}, {"cell_type": "code", "execution_count": null, "id": "c3feb0e5", "metadata": {"tags": ["learner", "py", "lect_03", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L7.3-runcell01\n", "\n", "#define the 2D X and Y grid\n", "x = np.linspace(sol.x[0]*0.99,sol.x[0]*1.01, 100) #grid in x\n", "y = np.linspace(sol.x[1]*0.5,sol.x[1]*1.5, 100)#grid in y\n", "X, Y = np.meshgrid(x, y) #2d grid\n", "\n", "# For z coordinate, evaluate chi2 at each x,y point in the grid.\n", "# note understanding this one-liner itself isn't too important\n", "Z = np.array([chi2([x,y]) for (x,y) in zip(X.ravel(), Y.ravel())]).reshape(X.shape)\n", "\n", "#and plot\n", "def plotColorsAndContours(X,Y,Z):\n", "    fig, ax = plt.subplots(1, 1)\n", "    c = ax.pcolor(X,Y,Z,cmap='RdBu')\n", "    cb=fig.colorbar(c, ax=ax)\n", "    plt.xlabel(\"a\")\n", "    plt.ylabel(\"b\")\n", "    cb.set_label(\"$\\chi^{2}$\")\n", "    #Now let's plot the contours of Delta chi^2\n", "    levels = [0.1,1,2.3,4,6.18,9, 16, 25, 36, 49, 64, 81, 100]\n", "    for i0 in range(len(levels)):\n", "        levels[i0] = levels[i0]+sol.fun\n", "    c = plt.contour(X, Y, Z, levels,colors=['red', 'blue', 'yellow','green'])\n", "    #plt.show()\n", "    \n", "plotColorsAndContours(X,Y,Z)"]}, {"cell_type": "markdown", "id": "5ee97882", "metadata": {"tags": ["learner", "md", "lect_03"]}, "source": ["In the plot above we've shown curves representing different $\\chi^2$ increases from the minimum. In the last section, we obtained uncertainties for our fit parameters by allowing just one of them to vary, and then observing the $\\Delta\\chi^2$. We may ask, what is the uncertainty profile when we are letting two parameters float simultaneously?\n", "\n", "Let's go back to the Taylor expansion result in the Lesson slides, which led to the relation of Wilk's theorem, but write the expansion in terms of all variables $\\vec{\\theta}$:  \n", "\n", "\n", "\n", "$$\n", "\\begin{equation}\n", "\\chi^{2}(x_{i},\\vec{\\theta})=\\chi^{2}_{min}(x_{i},\\vec{\\theta})+\\frac{1}{2}(\\theta_{i}-\\theta_{j})^{T}\\frac{\\partial^{2}}{\\partial \\theta_{i}\\theta_{0}}\\chi^{2}_{min}(x_{i},\\vec{\\theta}_{0})(\\theta_{j}-\\theta_{0})\n", "\\end{equation}\n", "$$\n", "\n", "We can write this out in 2D as:\n", "\n", "$$\n", "\\begin{equation}\n", "\\chi^{2}(x,\\vec{\\theta})=\\chi_{min}^{2}(x,\\vec{\\theta})+\\frac{1}{2}\\left(\\begin{array}{cc}\n", "\\theta_{a}-\\theta_{a-min} & \\theta_{b}-\\theta_{b-min}\\end{array}\\right)\\left(\\begin{array}{cc}\n", "\\frac{\\partial^{2}\\chi^{2}}{\\partial\\theta_{a}^{2}} & \\frac{\\partial^{2}\\chi^{2}}{\\partial\\theta_{a}\\partial\\theta_{b}}\\\\\n", "\\frac{\\partial^{2}\\chi^{2}}{\\partial\\theta_{a}\\partial\\theta_{b}} & \\frac{\\partial^{2}\\chi^{2}}{\\partial\\theta_{b}^{2}}\n", "\\end{array}\\right)\\left(\\begin{array}{c}\n", "\\theta_{a}-\\theta_{a-min}\\\\\n", "\\theta_{b}-\\theta_{b-min}\n", "\\end{array}\\right)\n", "\\end{equation}\n", "$$\n", "\n", "In the case where $\\frac{\\partial^{2}\\chi^{2}}{\\partial\\theta_{a}\\partial\\theta_{b}}\\approx0$ we can simplify this approximation a lot:\n", "\n", "$$\n", "\\begin{equation}\n", "\\chi^{2}(x,\\vec{\\theta})=\\chi_{min}^{2}(x,\\vec{\\theta})+\\frac{1}{2}\\left(\\begin{array}{cc}\n", "\\Delta\\theta_{a} & \\Delta\\theta_{b}\\end{array}\\right)\\left(\\begin{array}{cc}\n", "\\frac{\\partial^{2}\\chi^{2}}{\\partial\\theta_{a}^{2}} & 0\\\\\n", "0 & \\frac{\\partial^{2}\\chi^{2}}{\\partial\\theta_{b}^{2}}\n", "\\end{array}\\right)\\left(\\begin{array}{c}\n", "\\Delta\\theta_{a}\\\\\n", "\\Delta\\theta_{b}\n", "\\end{array}\\right)\n", "\\end{equation}\n", "$$\n", "\n", "This all becomes a 2D quadratic equation:\n", "\n", "$$\n", "\\begin{align}\n", "\\chi^{2}(x,\\vec{\\theta}) & =\\chi_{min}^{2}(x,\\vec{\\theta})+\\frac{1}{2}\\left(\\begin{array}{cc}\n", "\\Delta\\theta_{a}^{2}\\frac{\\partial^{2}\\chi^{2}}{\\partial\\theta_{a}^{2}}+ & \\Delta\\theta_{b}^{2}\\frac{\\partial^{2}\\chi^{2}}{\\partial\\theta_{b}^{2}}\\end{array}\\right)\\\\\n", " & =\\chi_{min}^{2}(x,\\vec{\\theta})+\\left(\\begin{array}{cc}\n", "\\frac{\\Delta\\theta_{a}^{2}}{\\sigma_{\\theta_{a}}^{2}}+ & \\frac{\\Delta\\theta_{b}^{2}}{\\sigma_{\\theta_{b}}^{2}}\\end{array}\\right)\n", "\\end{align}\n", "$$\n", "\n", "I want to point out, we are now profiling two parameters at once in this 2D plot, which means the contribution to $\\chi^{2}$ involves 2 degrees of freedom. You can see this from the above equation, since its the sum of 2 gaussian distributed variables with width $1$ and mean $0$. The 1 $\\sigma$ confidence interval for 2 degrees of freedom is computed by taking $\\Delta \\chi^2(x,\\nu=2)=x~\\mathrm{where~}\\mathrm{cdf}\\left(\\chi^{2}(x,2)=0.683\\right)\\approx2.3$. This is the yellow contour. Contrast this with the case of 1 $\\sigma$ confidence on 1 degree of freedom, where $\\Delta\\chi^2 = 1$. \n", "\n", "Now, we can go one step further, and note that the above formula  $ a x^2 + b y^2 = c$ is just the form of an ellipse with a specific major and minor axis. As a result, we can define a 3D funciton given by \n", "\n", "$$ f(x,y) = \\left(\\frac{x-\\bar{a}}{\\sigma_{a}}\\right)^{2} + \\left(\\frac{y-\\bar{b}}{\\sigma_{b}}\\right)^{2} $$\n", "\n", "Anyway, let's plot the ellipses and compare it to our minimum contours. \n"]}, {"cell_type": "code", "execution_count": null, "id": "a3d5ff96", "metadata": {"tags": ["learner", "py", "lect_03", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L7.3-runcell02\n", "\n", "#Let's plot the uncertainties  from hess_inv\n", "print(np.sqrt(2*sol.hess_inv))\n", "#the diagonals are approximately the errors\n", "\n", "#Make a the expression in the above equation x and x0 are 2 vectors\n", "def quadratic2D(x,x0,sigma0):\n", "    lVals=x-x0\n", "    lVals=(lVals**2)/(sigma0)/sigma0\n", "    return np.sum(lVals)\n", "\n", "plotColorsAndContours(X,Y,Z)\n", "\n", "#Now plot the ellipse in 3D\n", "def plotEllipse(sigx,sigy):\n", "    levels = [0.1,1,2.3,4,6.18,9, 16, 25, 36, 49, 64, 81, 100]\n", "    ZQ = np.array([quadratic2D([x,y],sol.x,[sigx,sigy]) for (x,y) in zip(X.ravel(), Y.ravel())]).reshape(X.shape)\n", "    c = plt.contour(X, Y, ZQ, levels,colors=['red', 'blue', 'yellow','green'],linestyles='dashed')\n", "\n", "sigx=(solX2.root-solX1.root)/2.\n", "sigy=(solY2.root-solY1.root)/2.\n", "plotEllipse(sigx,sigy)\n", "plt.show()"]}, {"cell_type": "markdown", "id": "1de87697", "metadata": {"tags": ["learner", "md", "lect_03"]}, "source": ["Remarkably, our space, give by the formula above, yields the solid lines, which matches very closely to the dashed contours obtained by just writing the $\\chi^{2}$ value. This is a profound statement, it really is. \n", "\n", "This gives us the same yellow line for our 2-variable 1$\\sigma$ confidence ellipse. If you look very closely, you will see a small difference. This observation raises the question of what happens when $\\frac{\\partial^{2}\\chi^{2}}{\\partial \\theta_{a}\\theta_{b}}\\neq0$. We'll see an example of that when we look at correlations in fitting a different set of parameters.\n"]}, {"cell_type": "markdown", "id": "cb948a8a", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='exercises_7_3'></a>     \n", "\n", "| [Top](#section_7_0) | [Restart Section](#section_7_3) | [Next Section](#section_7_4) |\n"]}, {"cell_type": "markdown", "id": "52737170", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-7.3.1</span>\n", "\n", "When we allow 2 parameters to vary, for a given confidence level (e.g. 1$\\sigma$) we end up with a confidence ellipse containing parameter values outside the confidence intervals on the individual parameters alone. Compute the 1$\\sigma$ (68.27%) confidence interval bounds for the parameter $a$, based on this ellipse from floating both $a$ and $b$.\n", "\n", "Enter your answer as a list of number with precision 1 (nearest whole number): `[a_lower, a_upper]`\n", "\n", "Why do we typically quote the uncertainty from the 1D variation?"]}, {"cell_type": "code", "execution_count": null, "id": "39befb5a", "metadata": {"tags": ["draft", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>EXERCISE: L7.3.1\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "# We can follow the same procedure for finding the confidence interval based on 1D\n", "# but for 1-sigma we now use 2.3 \n", "# Use the code from the solution to Ex. 7.2.1,\n", "ndof=#insert value here\n", "pval=0.6827#1 sigma p-value\n", "val = stats.chi2.ppf(pval,ndof) # 2 DoF this time!\n", "print(\"Delta Chi-square:\", val)\n", "\n", "def minfunc(x):\n", "    return chi2minX(x, delta_chi2=val)\n", "\n", "a1, a2 = #YOUR CODE HERE\n", "\n", "print(f\"a bounds: [{a1}, {a2}]\")"]}, {"cell_type": "markdown", "id": "510af4b6", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_7_4'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L7.4 Correlations Between Fit Parameters: Part 1</h2>  \n", "\n", "| [Top](#section_7_0) | [Previous Section](#section_7_3) | [Exercises](#exercises_7_4) | [Next Section](#section_7_5) |\n"]}, {"cell_type": "markdown", "id": "445d4680", "metadata": {"tags": ["learner", "md", "8S50x"]}, "source": ["*The material in this section is discussed in the video **<a href=\"https://courses.mitxonline.mit.edu/learn/course/course-v1:MITxT+8.S50.1x+3T2022/block-v1:MITxT+8.S50.1x+3T2022+type@sequential+block@seq_LS7/block-v1:MITxT+8.S50.1x+3T2022+type@vertical+block@vert_LS7_vid4\" target=\"_blank\">HERE</a>.** You are encouraged to watch that video and use this notebook concurrently.*"]}, {"cell_type": "markdown", "id": "c0bfe601", "metadata": {"tags": ["learner", "md"]}, "source": [" <h3>Slides</h3>\n", "\n", "Run the code below to view the slides for this section, which are discussed in the related video. You can also open the slides in a separate window <a href=\"https://mitx-8s50.github.io/slides/L07/slides_L07_04.html\" target=\"_blank\">HERE</a>."]}, {"cell_type": "code", "execution_count": null, "id": "e64965cf", "metadata": {"tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L7.4-slides\n", "\n", "from IPython.display import IFrame\n", "IFrame(src='https://mitx-8s50.github.io/slides/L07/slides_L07_04.html', width=970, height=550)"]}, {"cell_type": "markdown", "id": "f9bfd331", "metadata": {"tags": ["learner", "md", "lect_04"]}, "source": ["<h3>Overview</h3>\n", "    \n", "Ok, so now let's try to understand what happens when our off-diagonal terms are off. Let's go back to Wilk's theorem and look at the Hessian for the $\\chi^{2}$, but now with non-zero off-diagonal parameters.\n", "\n", "To understand the point of this, what we are going to do is make the 2D contour plot like the one above with the same contours as before, but now we will use a new function, that is just a reparametrization of the old one. This function will be\n", "\n", "$$\n", "\\begin{equation}\n", " f(x) = a x + b (1-x)\n", "\\end{equation}\n", "$$\n", "\n", "This means that the values of $a$ and $b$ now have different meaning, but this is just a linear fit like before and it should give the same overall $\\chi^{2}$ value as before. \n"]}, {"cell_type": "code", "execution_count": null, "id": "c89f20c6", "metadata": {"tags": ["learner", "py", "lect_04", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L7.4-runcell01\n", "\n", "def fnew(x,a,b):\n", "    pVal=b*(1-x)\n", "    return a*x+pVal\n", "\n", "model  = lmfit.Model(fnew)\n", "p = model.make_params(a=1000,b=1000)\n", "\n", "result = model.fit(data=yhist,x=xhist, params=p, weights=xweights)\n", "lmfit.report_fit(result)\n", "plt.figure()\n", "result.plot()\n", "plt.show()\n", "\n", "x0 = np.array([1000,1000])\n", "ps = [x0]\n", "sol=opt.minimize(chi2, x0)\n", "print(sol)\n", "\n", "x = np.linspace(sol.x[0]*0.99,sol.x[0]*1.01, 100)\n", "y = np.linspace(sol.x[1]*0.99,sol.x[1]*1.01, 100)\n", "X, Y = np.meshgrid(x, y)\n", "Z = np.array([chi2([x,y]) for (x,y) in zip(X.ravel(), Y.ravel())]).reshape(X.shape)\n", "plotColorsAndContours(X,Y,Z)\n", "plt.show()"]}, {"cell_type": "markdown", "id": "2d7a22d3", "metadata": {"tags": ["learner", "md", "lect_04"]}, "source": ["Now critically, let's now make the same ellipse plot with our new best fit values for $a$ and $b$, and our new uncertainties. Additionally, we can also do the 1D uncertainty scans like we did above again, to see what uncertainties we get on $a$ and $b$. "]}, {"cell_type": "code", "execution_count": null, "id": "24bc5b47", "metadata": {"tags": ["learner", "lect_04", "learner_chopped", "py"]}, "outputs": [], "source": ["#>>>RUN: L7.4-runcell02\n", "\n", "plotColorsAndContours(X,Y,Z)\n", "solX1, solX2 = chi2uncX(sol)\n", "solY1, solY2 = chi2uncY(sol)\n", "sigx=(solX2.root-solX1.root)/2.\n", "sigy=(solY2.root-solY1.root)/2.\n", "plotEllipse(sigx,sigy)\n", "plt.show()\n"]}, {"cell_type": "markdown", "id": "c8427941", "metadata": {"tags": ["learner", "md", "lect_04"]}, "source": ["What we can see is that our ellipses differ pretty dramatically from above. Also, we see that our 1D uncertainties are very mis-estimated from what lmfit gives us. The reason for this is that `lmfit` takes into account the full correlation of the variables, whereas just doing a 1D scan is equivalent to just moving on a single line along the $y$ and $x$ axes. "]}, {"cell_type": "markdown", "id": "aa61b45e", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='exercises_7_4'></a>     \n", "\n", "| [Top](#section_7_0) | [Restart Section](#section_7_4) | [Next Section](#section_7_5) |\n"]}, {"cell_type": "markdown", "id": "600b5811", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-7.4.1</span>\n", "\n", "Run the previous fit with just a regular linear fit given by $f(x) = ax + b$. How does the $\\chi^{2}$ change? How do the correlations between the variables change? Choose the best option from the following:\n", "\n", "- $\\chi^{2}$ increases and the correlations increase\n", "- $\\chi^{2}$ decreases and the correlations increase\n", "- $\\chi^{2}$ stays approximately the same and the correlations increase\n", "- $\\chi^{2}$ increases and the correlations decrease\n", "- $\\chi^{2}$ decreases and the correlations decrease\n", "- $\\chi^{2}$ stays approximately the same and the correlations decrease\n", "- $\\chi^{2}$ increases and the correlations stay approximately the same\n", "- $\\chi^{2}$ decreases and the correlations stay approximately the same\n", "- $\\chi^{2}$ stays approximately the same and the correlations stay approximately the same"]}, {"cell_type": "code", "execution_count": null, "id": "ff980996", "metadata": {"tags": ["draft", "py"]}, "outputs": [], "source": ["#>>>EXERCISE: L7.4.1\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n"]}, {"cell_type": "markdown", "id": "1118b3c4", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_7_5'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L7.5 Correlations Between Fit Parameters: Part 2</h2>     \n", "\n", "| [Top](#section_7_0) | [Previous Section](#section_7_4) | [Exercises](#exercises_7_5) |\n"]}, {"cell_type": "markdown", "id": "460fc4a9", "metadata": {"tags": ["learner", "md", "8S50x"]}, "source": ["*The material in this section is discussed in the video **<a href=\"https://courses.mitxonline.mit.edu/learn/course/course-v1:MITxT+8.S50.1x+3T2022/block-v1:MITxT+8.S50.1x+3T2022+type@sequential+block@seq_LS7/block-v1:MITxT+8.S50.1x+3T2022+type@vertical+block@vert_LS7_vid5\" target=\"_blank\">HERE</a>.** You are encouraged to watch that video and use this notebook concurrently.*"]}, {"cell_type": "markdown", "id": "17ce9139", "metadata": {"tags": ["learner", "md", "lect_05"]}, "source": ["<h3>Overview</h3>\n", "\n", "Now if we go back to our original correlated parameterization, we see that the 1D profile method for obtaining the uncertianties is not the same as the uncertainties we get from the fit.  This is clearly a problem, how can we address this issue? \n", "\n", "<h3>Dealing with correlated uncertainties</h3>\n", "\n", "Looking at our parameters, our uncertainty estimate is smaller than what we observed above. The uncertainties quoted now differ from what we got by varying from the $\\chi^{2}$ minimum. What we are doing is moving up and down the 2D plot and looking at $\\Delta \\chi^{2}$. However, the two parameters are strongly correlated, meaning that the value of one parameter that minimizes the $\\chi^{2}$ changes as the value of the other parameter changes. Because of that, you see that this doesn't capture the true uncertainty in the sense that we can move further along $x$ or $y$ (while changing the value of the other parameter appropriately) and still be within the yellow or even blue ellipse. This is quite clear when you overlay the uncertainty from the quadratic function, which draws circles not ellipses. \n", "\n", "What, then, is the right uncertainty to quote? \n", "\n", "Notice our fit with `lmfit` outputs a parameter $C(a,b)=0.872$. This is, in fact, a measure of the correlation between the two parameters $a$ and $b$. We also see from our optimization function that we get something labelled as the `hess_inv`. This we can write noting the relation of the $\\chi^{2}$ Hessian and uncertainties as:\n", "\n", "$$\n", "\\begin{equation}\n", "\\chi^{2}(x_{i},\\vec{\\theta})=\\chi^{2}_{min}(x_{i},\\vec{\\theta})+\\frac{1}{2}(\\theta_{i}-\\theta_{j})^{T}\\frac{\\partial^{2}}{\\partial \\theta_{i}\\theta_{0}}\\chi^{2}_{min}(x_{i},\\vec{\\theta}_{0})(\\theta_{j}-\\theta_{0})\n", "\\end{equation}\n", "$$\n", "\n", "which allows us to write the uncertainties as:\n", "\n", "$$\n", "\\begin{equation}\n", " \\frac{2}{\\sigma^{2}}=\\frac{\\partial^{2}\\chi^{2}}{\\partial\\theta_{i}\\partial\\theta_{j}} \\\\\n", " \\sigma^{2}    = 2\\left(\\frac{\\partial^{2}\\chi^{2}}{\\partial\\theta_{i}\\partial\\theta_{j}}\\right)^{-1} \n", "\\end{equation}\n", "$$\n", "\n", "Now, in the case where the off-diagonals of the Hessian were zero, the parameters were uncorrelated, so:\n", "\n", "$$\n", "\\begin{equation}\n", "\\left(\\begin{array}{cc}\n", "\\frac{\\partial^{2}\\chi^{2}}{\\partial\\theta_{a}^{2}} & 0\\\\\n", "0 & \\frac{\\partial^{2}\\chi^{2}}{\\partial\\theta_{b}^{2}}\n", "\\end{array}\\right)\\rightarrow\\left(\\begin{array}{cc}\n", "\\frac{2}{\\sigma_{a}^{2}} & 0\\\\\n", "0 & \\frac{2}{\\sigma_{b}^{2}}\n", "\\end{array}\\right)\n", "\\end{equation}\n", "$$\n", "\n", "But here we have something more complicated. However, this is a natural way to define correlations. Let's first verify our intuition for our minimization scheme by taking our $2\\times2$ Hessian matrix and diagonalizing, computing the eigenvectors and the eigenvalues. We can diagonalize the matrix as: \n", "\n", "\n", "$$\n", "\\begin{equation}\n", "A^{-1}=2\\left(\\begin{array}{cc}\n", "\\frac{\\partial^{2}\\chi^{2}}{\\partial\\theta_{a}^{2}} & \\frac{\\partial^{2}\\chi^{2}}{\\partial\\theta_{a}\\partial\\theta_{b}}\\\\\n", "\\frac{\\partial^{2}\\chi^{2}}{\\partial\\theta_{a}\\partial\\theta_{b}} & \\frac{\\partial^{2}\\chi^{2}}{\\partial\\theta_{b}^{2}}\n", "\\end{array}\\right)^{-1}A=\\left(\\begin{array}{cc}\n", "\\sigma_{1}^{2} & 0\\\\\n", "0 & \\sigma_{2}^{2}\n", "\\end{array}\\right)\n", "\\end{equation}\n", "$$\n", "\n", "It is important to note that for any N dimensional Hessian, provided the determinant is not zero, we can find a basis of independent variables that are not correlated. That is to say we can always diagonalize our Hessian, and the eigenvectors of our Hessian are the independent values with variances given by the eigenvalues. These are our true uncertainty values (eigenvalues) and direction of variation (eigenvector).  \n", "\n", "When we run our minimizer, we get the Hessian inverse, which we can play with. Let's go ahead and look at the Hessian from our minimizer. From the Hessian, we can import numpy's linear algebra toolkit and compute the eigen vectors and values. Finally, we can draw these vectors and values on our 2D scan plot above. Let's do it:\n"]}, {"cell_type": "markdown", "id": "618218c9", "metadata": {"tags": ["learner", "md", "lect_05"]}, "source": ["<h3>Rerun correlated fit</h3>\n", "\n", "First, let's rerun the correlated fit."]}, {"cell_type": "code", "execution_count": null, "id": "2c09be2f", "metadata": {"tags": ["learner", "py", "lect_05", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L7.5-runcell01\n", "\n", "def fnew(x,a,b):\n", "    pVal=b*(1-x)\n", "    return a*x+pVal\n", "\n", "model  = lmfit.Model(fnew)\n", "p = model.make_params(a=1000,b=1000)\n", "\n", "result = model.fit(data=yhist,x=xhist, params=p, weights=xweights)\n", "lmfit.report_fit(result)\n", "plt.figure()\n", "result.plot()\n", "plt.show()\n", "\n", "x0 = np.array([1000,1000])\n", "ps = [x0]\n", "sol=opt.minimize(chi2, x0)\n", "print(sol)\n", "\n", "x = np.linspace(sol.x[0]*0.99,sol.x[0]*1.01, 100)\n", "y = np.linspace(sol.x[1]*0.99,sol.x[1]*1.01, 100)\n", "X, Y = np.meshgrid(x, y)\n", "Z = np.array([chi2([x,y]) for (x,y) in zip(X.ravel(), Y.ravel())]).reshape(X.shape)\n", "plotColorsAndContours(X,Y,Z)\n", "plt.show()\n"]}, {"cell_type": "code", "execution_count": null, "id": "12807150", "metadata": {"tags": ["learner", "py", "lect_05", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L7.5-runcell02\n", "\n", "print(np.sqrt(2*sol.hess_inv))\n", "#The diagonals are the uncertainty lmfit quotes\n", "\n", "#Really the best way to do this is to get the eigen values using an linear algebra problem\n", "import numpy.linalg as la\n", "w, v=la.eig(2*sol.hess_inv)\n", "print(\"values\",w,\"vectors\",v)\n", "\n", "#Now let's plot the eigenvectors\n", "from matplotlib.patches import Ellipse\n", "def get_cov_ellipse(cov, centre, nstd, **kwargs):\n", "    \"\"\"\n", "    Return a matplotlib Ellipse patch representing the covariance matrix\n", "    cov centred at centre and scaled by the factor nstd.\n", "\n", "    \"\"\"\n", "    # Find and sort eigenvalues and eigenvectors into descending order\n", "    eigvals, eigvecs = np.linalg.eigh(cov)\n", "    order = eigvals.argsort()[::-1]\n", "    eigvals, eigvecs = eigvals[order], eigvecs[:, order]\n", "\n", "    # The anti-clockwise angle to rotate our ellipse by \n", "    vx, vy = eigvecs[:,0][0], eigvecs[:,0][1]\n", "    theta = np.arctan2(vy, vx)\n", "\n", "    # Width and height of ellipse to draw\n", "    width, height = 2 * nstd * np.sqrt(eigvals)\n", "    return Ellipse(xy=centre, width=width, height=height,angle=np.degrees(theta), **kwargs)\n", "\n", "err_ellipse=get_cov_ellipse(2*sol.hess_inv,sol.x,1)\n", "fig, ax = plt.subplots(1, 1)\n", "c = ax.pcolor(X,Y,Z,cmap='RdBu')\n", "fig.colorbar(c, ax=ax)\n", "levels = [0.1,1,2.3,4,9, 16, 25, 36, 49, 64, 81, 100]\n", "for i0 in range(len(levels)):\n", "    levels[i0] = levels[i0]+sol.fun\n", "c = plt.contour(X, Y, Z, levels,colors=['red', 'blue', 'yellow','green'])\n", "ax.add_artist(err_ellipse)\n", "plt.show()"]}, {"cell_type": "markdown", "id": "0bf07b75", "metadata": {"tags": ["learner", "md", "lect_05"]}, "source": ["Now the filled in circles make a direct correspondance with the uncertainties above. \n", "\n", "From the above, we can now formulate a description of uncertainties in our system. When we had uncorrelated parameters, we had a total uncertainty from our $\\chi^{2}$ given by \n", "\n", "\n", "$$\n", "\\begin{equation}\n", "\\sigma_{tot}^{2} = \\sigma_{a}^2+\\sigma_{b}^2\n", "\\end{equation}\n", "$$\n", "\n", "Now, we have the full ellipse\n", "\n", "$$\n", "\\begin{equation}\n", "\\sigma_{tot}^{2} = \\sigma_{a}^2+\\sigma_{b}^2+2\\sigma_{ab}\n", "\\end{equation}\n", "$$\n", "\n", "where $\\sigma_{ab}=\\rm{COV(a,b)}$. There are a number of names for this variable, but they are all equivalent. Let's write it out. We can write the error matrix in 2D as:\n", "\n", "\n", "$$\n", "\\begin{equation}\n", "\\left(\\begin{array}{cc}\n", "\\sigma_{a}^{2} & {\\rm COV}(a,b)\\\\\n", "{\\rm COV}(a,b) & \\sigma_{b}^{2}\n", "\\end{array}\\right)=\\sum_{i=1}^{N}\\left(\\begin{array}{cc}\n", "\\left(a_{i}-\\bar{a}\\right)^{2} & \\left(a_{i}-\\bar{a}\\right)\\left(b_{i}-\\bar{b}\\right)\\\\\n", "\\left(a_{i}-\\bar{a}\\right)\\left(b_{i}-\\bar{b}\\right) & \\left(b_{i}-\\bar{b}\\right)^{2}\n", "\\end{array}\\right)\n", "\\end{equation}\n", "$$\n", "\n", "where on the right side we have written it in terms of the computation over events. Recall that, for a linear regression, the slope is just the $\\rm{COV(x,y)/VAR(x)}$, so the covariance matrix is intricately tied with the slope.  \n", "\n", "We can also write it as the correlation matrix where we normalize by the uncertainties:\n", "\n", "$$\n", "\\begin{equation}\n", "\\rho=\\left(\\frac{1}{\\sigma_{a}}  \\frac{1}{\\sigma_{b}} \\right)^{T}\\left(\\begin{array}{cc}\n", "1 & \\frac{{\\rm COV}(a,b)}{\\sigma_{a}\\sigma_{b}}\\\\\n", "\\frac{{\\rm COV}(a,b)}{\\sigma_{a}\\sigma_{b}} & 1\n", "\\end{array}\\right)\\left(\\frac{1}{\\sigma_{a}}   \\frac{1}{\\sigma_{b}} \\right) \n", "\\end{equation}\n", "$$\n", "\n", "Recall that the covariance is what we originally used to compute the linear slope of the points, this is exactly the same here. In fact, instead of scanning the likelihood analytically we could have sampled the points, and done a linear regression. The resulting slope and line can be related to our eigenvectors. One last thing to mention is that if variables are correlated, we can use the correlation to propagate the uncertainties. \n", "\n", "$$\n", "\\begin{equation}\n", "\\sigma_{f}^{2} = \\left(\\frac{\\partial f}{\\partial x}\\right)^2\\sigma_{x}^2 + \\left(\\frac{\\partial f}{\\partial y}\\right)^{2}+\\left(\\frac{\\partial f}{\\partial x}\\right)\\left(\\frac{\\partial f}{\\partial y}\\right)\\sigma_{xy}\n", "\\end{equation}\n", "$$\n", "\n", "Now let's check we can get the corelation matrix."]}, {"cell_type": "code", "execution_count": null, "id": "6719aad5", "metadata": {"tags": ["learner", "py", "lect_05", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L7.5-runcell03\n", "\n", "#Now let's get the correlation C(a,b) (see below)\n", "w, v=np.linalg.eig(2*sol.hess_inv)\n", "print(\"c(a,b)\",v[0,1]/v[0,0])\n", "print(\"A deceptively wrong way to get correlation: since its not normalized\",sol.hess_inv[0,1]/sol.hess_inv[0,0])"]}, {"cell_type": "markdown", "id": "48cc041c", "metadata": {"tags": ["learner", "md", "lect_05"]}, "source": ["Finally, we are going to run our fit "]}, {"cell_type": "code", "execution_count": null, "id": "6033110d", "metadata": {"tags": ["learner", "py", "lect_05", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L7.5-runcell04\n", "\n", "import lmfit\n", "\n", "def fnew(x,a,b):\n", "    pVal=b*(1-x)\n", "    return a*x+pVal\n", "\n", "#Randomly sample points in the above range\n", "def maketoy(iy):\n", "    toy=np.array([])\n", "    #go through the y-values and Poisson fluctuate\n", "    for i0 in range(len(iy)):\n", "        pVal = np.random.normal (iy[i0],np.sqrt([iy[i0]]))\n", "        toy = np.append(toy,float(pVal))\n", "    return toy\n", "\n", "def fittoy(ibin,iy):\n", "    #generate toy\n", "    toy=maketoy(iy)\n", "    #now fit\n", "    model  = lmfit.Model(fnew)\n", "    p = model.make_params(a=1000,b=10)\n", "    xweights=np.array([])\n", "    #setup poison weight\n", "    for i0 in range(len(toy)):\n", "        xweights = np.append(xweights,1./math.sqrt(toy[i0]))\n", "    result = model.fit(data=toy,x=ibin, params=p, weights=xweights)\n", "    return result.params[\"a\"].value,result.params[\"b\"].value\n", "\n", "ntoys=1000\n", "lAs=np.array([])\n", "lBs=np.array([])\n", "for i0 in range(ntoys):\n", "    pA,pB=fittoy(xhist,yhist)\n", "    lAs = np.append(lAs,pA)\n", "    lBs = np.append(lBs,pB)\n", "\n", "err_ellipse=get_cov_ellipse(2*sol.hess_inv,sol.x, 1)\n", "fig, ax = plt.subplots(1, 1)\n", "c = ax.pcolor(X,Y,Z,cmap='RdBu')\n", "fig.colorbar(c, ax=ax)\n", "c = plt.contour(X, Y, Z, levels,colors=['red', 'blue', 'yellow','green'])\n", "ax.add_artist(err_ellipse)\n", "plt.plot(lAs,lBs,c='black',marker='.',linestyle = 'None')\n", "plt.show()"]}, {"cell_type": "markdown", "id": "d810c8fa", "metadata": {"tags": ["learner", "md", "lect_05"]}, "source": ["You can see that if we randomly sample from our distributions, then we get the ellipse variation in $A$ and $B$. The toys have a lot of useful elements. We can use it to get the variation in our parameters.  What we can also do is look at the variance of our parameters and compare it to the Hessian that we can diagonalize to get the fit. "]}, {"cell_type": "code", "execution_count": null, "id": "a5ae9db2", "metadata": {"tags": ["learner", "py", "lect_05", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L7.5-runcell05\n", "\n", "#Now let's run a. linear regression\n", "def variance(isamples):\n", "    mean=isamples.mean()\n", "    n=len(isamples)\n", "    tot=0\n", "    for pVal in isamples:\n", "        tot+=(pVal-mean)**2\n", "    return tot/n\n", "\n", "def covariance(ixs,iys):\n", "    meanx=ixs.mean()\n", "    meany=iys.mean()\n", "    n=len(ixs)\n", "    tot=0\n", "    for i0 in range(len(ixs)):\n", "        tot+=(ixs[i0]-meanx)*(iys[i0]-meany)\n", "    return tot/n\n", "\n", "print(\"A:\",lAs.mean(),\"+/-\",lAs.std())\n", "print(\"B:\",lBs.mean(),\"+/-\",lBs.std())\n", "print(\"Cov:\",covariance(lAs,lBs),\"A Variance:\",variance(lAs),\"B Variance:\",variance(lBs))\n", "print(\"Check with Hessian:\",2*sol.hess_inv)\n", "print(\"Cor:\",covariance(lAs,lBs)/math.sqrt(variance(lAs)*variance(lBs)),\"A Variance:\",1.,\"B Variance:\",1.)"]}, {"cell_type": "markdown", "id": "a4251144", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='exercises_7_5'></a>   \n", "\n", "| [Top](#section_7_0) | [Restart Section](#section_7_5) |\n"]}, {"cell_type": "markdown", "id": "0b0b7275", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-7.5.1</span>\n", "\n", "Repeat the previous analysis using the uncorrelated fit, $f(x) = a x + b$. Specifically, plot the ellipse and compute the uncertainties in $a$ and $b$? Do they correspond with lmfit? \n"]}, {"cell_type": "code", "execution_count": null, "id": "9a896c73", "metadata": {"tags": ["draft", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>EXERCISE: L7.5.1\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n"]}], "metadata": {"celltoolbar": "Tags", "kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}