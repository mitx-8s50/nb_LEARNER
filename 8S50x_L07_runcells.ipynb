{"cells": [{"cell_type": "markdown", "id": "f44134fe", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<hr style=\"height: 1px;\">\n", "<i>This notebook was authored by the 8.S50x Course Team, Copyright 2022 MIT All Rights Reserved.</i>\n", "<hr style=\"height: 1px;\">\n", "<br>\n", "\n", "<h1>Lesson 7: Correlations</h1>\n"]}, {"cell_type": "markdown", "id": "7da993b2", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_7_0'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L7.0 Overview</h2>\n"]}, {"cell_type": "markdown", "id": "5bf91bc0", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<h3>Navigation</h3>\n", "\n", "<table style=\"width:100%\">\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_7_1\">L7.1 Understanding Best Fit (Revisited)</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_7_1\">L7.1 Exercises</a></td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_7_2\">L7.2 Minimizing on a Surface (1D Scan)</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_7_2\">L7.2 Exercises</a></td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_7_3\">L7.3 Minimizing on a Surface (2D Scan)</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_7_3\">L7.3 Exercises</a></td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_7_4\">L7.4 Correlations Between Fit Parameters: Part 1</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_7_4\">L7.4 Exercises</a></td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_7_5\">L7.5 Correlations Between Fit Parameters: Part 2</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_7_5\">L7.5 Exercises</a></td>\n", "    </tr>\n", "</table>\n", "\n"]}, {"cell_type": "code", "execution_count": null, "id": "65a5b9ff", "metadata": {"tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L7.0-runcell00\n", "\n", "#importing data from git repository\n", "\n", "!git init\n", "!git remote add -f origin https://github.com/mitx-8s50/nb_LEARNER/\n", "!git config core.sparseCheckout true\n", "!echo 'data/L05' >> .git/info/sparse-checkout\n", "!git pull origin main"]}, {"cell_type": "code", "execution_count": null, "id": "7b8fed76", "metadata": {"tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L7.0-runcell01\n", "\n", "# for Colab users\n", "!pip install lmfit"]}, {"cell_type": "code", "execution_count": null, "id": "33156765", "metadata": {"tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L7.0-runcell02\n", "\n", "import numpy as np                 #https://numpy.org/doc/stable/\n", "from scipy import optimize as opt  #https://docs.scipy.org/doc/scipy/reference/optimize.html\n", "import matplotlib.pyplot as plt    #https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.html\n", "import math                        #https://docs.python.org/3/library/math.html\n", "import csv                         #https://docs.python.org/3/library/csv.html\n", "import lmfit                       #https://lmfit.github.io/lmfit-py/ \n", "import scipy.stats as stats        #https://docs.scipy.org/doc/scipy/reference/stats.html\n", "from scipy.optimize.zeros import RootResults        #https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.RootResults.html\n", "from scipy.optimize.optimize import OptimizeResult  # https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.OptimizeResult.html\n", "from typing import Callable, Tuple #https://docs.python.org/3/library/typing.html#typing.Callable\n", "                                   #https://docs.python.org/3/library/typing.html\n", "import numpy.linalg as la          #https://docs.scipy.org/doc/scipy/reference/linalg.html\n", "from matplotlib.patches import Ellipse  # https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.patches.Ellipse.html\n"]}, {"cell_type": "code", "execution_count": null, "id": "67d4684e", "metadata": {"tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L7.0-runcell03\n", "\n", "#set plot resolution\n", "%config InlineBackend.figure_format = 'retina'\n", "\n", "#set default figure parameters\n", "plt.rcParams['figure.figsize'] = (9,6)\n", "\n", "medium_size = 12\n", "large_size = 15\n", "\n", "plt.rc('font', size=medium_size)          # default text sizes\n", "plt.rc('xtick', labelsize=medium_size)    # xtick labels\n", "plt.rc('ytick', labelsize=medium_size)    # ytick labels\n", "plt.rc('legend', fontsize=medium_size)    # legend\n", "plt.rc('axes', titlesize=large_size)      # axes title\n", "plt.rc('axes', labelsize=large_size)      # x and y labels\n", "plt.rc('figure', titlesize=large_size)    # figure title\n"]}, {"cell_type": "markdown", "id": "0ad89b66", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_7_1'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L7.1 Understanding Best Fit (Revisited)</h2>  \n", "\n", "| [Top](#section_7_0) | [Previous Section](#section_7_0) | [Exercises](#exercises_7_1) | [Next Section](#section_7_2) |\n"]}, {"cell_type": "code", "execution_count": null, "id": "1c381a19", "metadata": {"tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L7.1-slides\n", "\n", "from IPython.display import IFrame\n", "IFrame(src='https://mitx-8s50.github.io/slides/L07/slides_L07_01.html', width=970, height=550)"]}, {"cell_type": "code", "execution_count": null, "id": "b9d7a68b", "metadata": {"tags": ["learner", "py", "lect_01", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L7.1-runcell01\n", "\n", "import scipy.stats as stats\n", "\n", "sigma = 1\n", "print(stats.norm.cdf(sigma)-stats.norm.cdf(-sigma))\n", "\n", "def pval(iVal):\n", "    return stats.norm.cdf(iVal)-stats.norm.cdf(-iVal)\n", "\n", "def chi2Val(iGausSigma,iNDOF):\n", "    val=stats.chi2.ppf(pval(iGausSigma),iNDOF)\n", "    return val\n", "\n", "print(chi2Val(1,2))\n", "#print(chi2Val(1,1))\n", "#print(chi2Val(2,1))\n", "#print(chi2Val(3,1))\n", "#print(chi2Val(4,1))"]}, {"cell_type": "markdown", "id": "6b30ab91", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='exercises_7_1'></a>     \n", "\n", "| [Top](#section_7_0) | [Restart Section](#section_7_1) | [Next Section](#section_7_2) |\n"]}, {"cell_type": "markdown", "id": "c1b07b38", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-7.1.1</span>\n", "\n", "Compute the Chi-square value for 2$\\sigma$ and 2 degrees of freedom. Enter your answer as a number with precision 1e-2. \n"]}, {"cell_type": "code", "execution_count": null, "id": "a9a30c37", "metadata": {"tags": ["draft", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>EXERCISE: L7.1.1\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n"]}, {"cell_type": "markdown", "id": "9fed2264", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_7_2'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L7.2 Minimizing on a Surface (1D Scan)</h2>  \n", "\n", "| [Top](#section_7_0) | [Previous Section](#section_7_1) | [Exercises](#exercises_7_2) | [Next Section](#section_7_3) |\n"]}, {"cell_type": "code", "execution_count": null, "id": "5b99381c", "metadata": {"tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L7.2-slides\n", "\n", "from IPython.display import IFrame\n", "IFrame(src='https://mitx-8s50.github.io/slides/L07/slides_L07_02.html', width=970, height=550)"]}, {"cell_type": "code", "execution_count": null, "id": "c88f8011", "metadata": {"scrolled": true, "tags": ["learner", "py", "lect_01", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L7.2-runcell01\n", "\n", "############ This is all code from a previous lesson\n", "import numpy as np\n", "import csv\n", "import math\n", "import lmfit\n", "import matplotlib.pyplot as plt\n", "from scipy import optimize as opt\n", "\n", "def rad(iTheta):\n", "    return iTheta/180. * math.pi\n", "\n", "def rad1(iTheta):\n", "    return iTheta/180. * math.pi-math.pi\n", "\n", "def load(label):\n", "    dec=np.array([])\n", "    ra=np.array([])\n", "    az=np.array([])\n", "    with open(label,'r') as csvfile:\n", "        plots = csv.reader(csvfile,delimiter=' ')\n", "        for pRow in plots:\n", "            if '#' in pRow[0] or pRow[0]=='':\n", "                continue\n", "            dec = np.append(dec,rad(float(pRow[2])))\n", "            ra  = np.append(ra,rad1(float(pRow[3])))\n", "            az  = np.append(az,rad(float(pRow[4])))\n", "    return dec,ra,az\n", "\n", "def prephist(iRA):\n", "    y0, bin_edges = np.histogram(iRA, bins=30)\n", "    x0 = 0.5*(bin_edges[1:] + bin_edges[:-1])\n", "    y0 = y0.astype('float')\n", "    return x0,y0,1./(y0**0.5)\n", "\n", "label8='data/L05/events_a8_1space.dat'\n", "\n", "dec,ra8,az=load(label8)\n", "xhist,yhist,xweights=prephist(ra8)\n", "\n", "\n", "########## Tlast fit code\n", "\n", "def fnew(x,a,b):\n", "    pVal=b*np.sin(x)\n", "    return a+pVal\n", "\n", "model  = lmfit.Model(fnew)\n", "p = model.make_params(a=1000,b=10)\n", "result = model.fit(data=yhist,x=xhist, params=p, weights=xweights)\n", "lmfit.report_fit(result)\n", "plt.figure()\n", "result.plot()\n", "plt.show()\n"]}, {"cell_type": "code", "execution_count": null, "id": "4b5fc9a6", "metadata": {"tags": ["learner", "py", "lect_02", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L7.2-runcell02\n", "\n", "#This is the new code\n", "def chi2(iX):\n", "    #Note that this function depends on xhist and yhist being defined already.\n", "    #This is bad practice in general, but we do it here for convenience.\n", "    #After all, this code isn't reused elsewhere.\n", "    #MAKE SURE YOU RUN prephist(ra8) to define yhist\n", "    assert len(iX) == 2\n", "    lTot=0\n", "    for val in range(len(yhist)):\n", "        xtest=fnew(xhist[val],iX[0],iX[1])\n", "        lTot += (1./(xtest+1e-5))*(yhist[val]-xtest)**2\n", "    return lTot\n", "\n", "#First we minimize\n", "x0 = np.array([1000,10]) # initial conditions\n", "ps = [x0]\n", "sol=opt.minimize(chi2, x0)\n", "print(sol)"]}, {"cell_type": "code", "execution_count": null, "id": "26616a72", "metadata": {"tags": ["learner", "py", "lect_02", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L7.2-runcell03\n", "\n", "#Scan near the minimum of each value\n", "x = np.linspace(sol.x[0]*0.99,sol.x[0]*1.01, 100)\n", "y = np.linspace(sol.x[1]*0.5,sol.x[1]*1.5, 100)\n", "\n", "#Now let's fix one parameter at the minimum, and profile the other\n", "plt.plot(x, chi2([x,sol.x[1]]),label='chi2');\n", "plt.axhline(sol.fun+1, c='red')\n", "plt.xlabel(\"a-value\")\n", "plt.ylabel(\"$\\chi^{2}$\")\n", "plt.show()\n", "\n", "#Now for the other parameter\n", "plt.plot(y, chi2([sol.x[0],y]),label='chi2');\n", "plt.axhline(sol.fun+1, c='red')\n", "plt.xlabel(\"b-value\")\n", "plt.ylabel(\"$\\chi^{2}$\")\n", "plt.show()"]}, {"cell_type": "code", "execution_count": null, "id": "b991443a", "metadata": {"tags": ["learner", "py", "lect_02", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L7.2-runcell04\n", "\n", "######## Now let's use a numerical solver to find the points at which a function crosses zero (root solver)\n", "def chi2minX(xval, delta_chi2=1):\n", "    val=chi2([xval,sol.x[1]])\n", "    minval=chi2(sol.x) + delta_chi2\n", "    return val-minval\n", "\n", "def chi2minY(yval, delta_chi2=1):\n", "    val=chi2([sol.x[0],yval])\n", "    minval=chi2(sol.x) + delta_chi2\n", "    return val-minval\n", "\n", "def chi2uncX(sol):\n", "    solX1=opt.root_scalar(chi2minX,bracket=[sol.x[0], sol.x[0]*1.02],method='brentq')\n", "    solX2=opt.root_scalar(chi2minX,bracket=[sol.x[0]*0.98, sol.x[0]],method='brentq')\n", "    print(\"a:\",sol.x[0],\"+/-\",abs(solX2.root-solX1.root)/2.)\n", "    print(\"Reminder the Poisson uncertainty would be:\",math.sqrt(sol.x[0]/40))\n", "    return solX1, solX2\n", "\n", "def chi2uncY(sol):\n", "    solY1=opt.root_scalar(chi2minY,bracket=[sol.x[1],    sol.x[1]*1.2],method='brentq')\n", "    solY2=opt.root_scalar(chi2minY,bracket=[sol.x[1]*0.8, sol.x[1]],method='brentq')\n", "    print(\"b:\",sol.x[1],\"+/-\",abs(solY2.root-solY1.root)/2.)\n", "    return solY1, solY2\n", "\n", "solX1, solX2 = chi2uncX(sol)\n", "solY1, solY2 = chi2uncY(sol)"]}, {"cell_type": "markdown", "id": "290142a4", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='exercises_7_2'></a>     \n", "\n", "| [Top](#section_7_0) | [Restart Section](#section_7_2) | [Next Section](#section_7_3) |\n"]}, {"cell_type": "markdown", "id": "db70b48a", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-7.2.1</span>\n", "\n", "What are the 2$\\sigma$ bounds (that is, 95.45% confidence interval values) of $a$ and $b$ in the fit above? Enter your answer as a list of numbers with precision 1 (the nearest whole number): `[a_lower, a_upper, b_lower, b_upper]`"]}, {"cell_type": "code", "execution_count": null, "id": "725bd600", "metadata": {"tags": ["draft", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>EXERCISE: L7.2.1\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n"]}, {"cell_type": "markdown", "id": "14c0d887", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_7_3'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L7.3 Minimizing on a Surface (2D Scan)</h2>  \n", "\n", "| [Top](#section_7_0) | [Previous Section](#section_7_2) | [Exercises](#exercises_7_3) | [Next Section](#section_7_4) |\n"]}, {"cell_type": "code", "execution_count": null, "id": "71725fcc", "metadata": {"tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L7.3-slides\n", "\n", "from IPython.display import IFrame\n", "IFrame(src='https://mitx-8s50.github.io/slides/L07/slides_L07_03.html', width=970, height=550)"]}, {"cell_type": "code", "execution_count": null, "id": "c3feb0e5", "metadata": {"tags": ["learner", "py", "lect_03", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L7.3-runcell01\n", "\n", "#define the 2D X and Y grid\n", "x = np.linspace(sol.x[0]*0.99,sol.x[0]*1.01, 100) #grid in x\n", "y = np.linspace(sol.x[1]*0.5,sol.x[1]*1.5, 100)#grid in y\n", "X, Y = np.meshgrid(x, y) #2d grid\n", "\n", "# For z coordinate, evaluate chi2 at each x,y point in the grid.\n", "# note understanding this one-liner itself isn't too important\n", "Z = np.array([chi2([x,y]) for (x,y) in zip(X.ravel(), Y.ravel())]).reshape(X.shape)\n", "\n", "#and plot\n", "def plotColorsAndContours(X,Y,Z):\n", "    fig, ax = plt.subplots(1, 1)\n", "    c = ax.pcolor(X,Y,Z,cmap='RdBu')\n", "    cb=fig.colorbar(c, ax=ax)\n", "    plt.xlabel(\"a\")\n", "    plt.ylabel(\"b\")\n", "    cb.set_label(\"$\\chi^{2}$\")\n", "    #Now let's plot the contours of Delta chi^2\n", "    levels = [0.1,1,2.3,4,6.18,9, 16, 25, 36, 49, 64, 81, 100]\n", "    for i0 in range(len(levels)):\n", "        levels[i0] = levels[i0]+sol.fun\n", "    c = plt.contour(X, Y, Z, levels,colors=['red', 'blue', 'yellow','green'])\n", "    #plt.show()\n", "    \n", "plotColorsAndContours(X,Y,Z)"]}, {"cell_type": "code", "execution_count": null, "id": "a3d5ff96", "metadata": {"tags": ["learner", "py", "lect_03", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L7.3-runcell02\n", "\n", "#Let's plot the uncertainties  from hess_inv\n", "print(np.sqrt(2*sol.hess_inv))\n", "#the diagonals are approximately the errors\n", "\n", "#Make a the expression in the above equation x and x0 are 2 vectors\n", "def quadratic2D(x,x0,sigma0):\n", "    lVals=x-x0\n", "    lVals=(lVals**2)/(sigma0)/sigma0\n", "    return np.sum(lVals)\n", "\n", "plotColorsAndContours(X,Y,Z)\n", "\n", "#Now plot the ellipse in 3D\n", "def plotEllipse(sigx,sigy):\n", "    levels = [0.1,1,2.3,4,6.18,9, 16, 25, 36, 49, 64, 81, 100]\n", "    ZQ = np.array([quadratic2D([x,y],sol.x,[sigx,sigy]) for (x,y) in zip(X.ravel(), Y.ravel())]).reshape(X.shape)\n", "    c = plt.contour(X, Y, ZQ, levels,colors=['red', 'blue', 'yellow','green'],linestyles='dashed')\n", "\n", "sigx=(solX2.root-solX1.root)/2.\n", "sigy=(solY2.root-solY1.root)/2.\n", "plotEllipse(sigx,sigy)\n", "plt.show()"]}, {"cell_type": "markdown", "id": "cb948a8a", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='exercises_7_3'></a>     \n", "\n", "| [Top](#section_7_0) | [Restart Section](#section_7_3) | [Next Section](#section_7_4) |\n"]}, {"cell_type": "markdown", "id": "52737170", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-7.3.1</span>\n", "\n", "When we allow 2 parameters to vary, for a given confidence level (e.g. 1$\\sigma$) we end up with a confidence ellipse containing parameter values outside the confidence intervals on the individual parameters alone. Compute the 1$\\sigma$ (68.27%) confidence interval bounds for the parameter $a$, based on this ellipse from floating both $a$ and $b$.\n", "\n", "Enter your answer as a list of number with precision 1 (nearest whole number): `[a_lower, a_upper]`\n", "\n", "Why do we typically quote the uncertainty from the 1D variation?"]}, {"cell_type": "code", "execution_count": null, "id": "39befb5a", "metadata": {"tags": ["draft", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>EXERCISE: L7.3.1\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n"]}, {"cell_type": "markdown", "id": "510af4b6", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_7_4'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L7.4 Correlations Between Fit Parameters: Part 1</h2>  \n", "\n", "| [Top](#section_7_0) | [Previous Section](#section_7_3) | [Exercises](#exercises_7_4) | [Next Section](#section_7_5) |\n"]}, {"cell_type": "code", "execution_count": null, "id": "e64965cf", "metadata": {"tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L7.4-slides\n", "\n", "from IPython.display import IFrame\n", "IFrame(src='https://mitx-8s50.github.io/slides/L07/slides_L07_04.html', width=970, height=550)"]}, {"cell_type": "code", "execution_count": null, "id": "c89f20c6", "metadata": {"tags": ["learner", "py", "lect_04", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L7.4-runcell01\n", "\n", "def fnew(x,a,b):\n", "    pVal=b*(1-x)\n", "    return a*x+pVal\n", "\n", "model  = lmfit.Model(fnew)\n", "p = model.make_params(a=1000,b=1000)\n", "\n", "result = model.fit(data=yhist,x=xhist, params=p, weights=xweights)\n", "lmfit.report_fit(result)\n", "plt.figure()\n", "result.plot()\n", "plt.show()\n", "\n", "x0 = np.array([1000,1000])\n", "ps = [x0]\n", "sol=opt.minimize(chi2, x0)\n", "print(sol)\n", "\n", "x = np.linspace(sol.x[0]*0.99,sol.x[0]*1.01, 100)\n", "y = np.linspace(sol.x[1]*0.99,sol.x[1]*1.01, 100)\n", "X, Y = np.meshgrid(x, y)\n", "Z = np.array([chi2([x,y]) for (x,y) in zip(X.ravel(), Y.ravel())]).reshape(X.shape)\n", "plotColorsAndContours(X,Y,Z)\n", "plt.show()"]}, {"cell_type": "code", "execution_count": null, "id": "24bc5b47", "metadata": {"tags": ["learner", "lect_04", "learner_chopped", "py"]}, "outputs": [], "source": ["#>>>RUN: L7.4-runcell02\n", "\n", "plotColorsAndContours(X,Y,Z)\n", "solX1, solX2 = chi2uncX(sol)\n", "solY1, solY2 = chi2uncY(sol)\n", "sigx=(solX2.root-solX1.root)/2.\n", "sigy=(solY2.root-solY1.root)/2.\n", "plotEllipse(sigx,sigy)\n", "plt.show()\n"]}, {"cell_type": "markdown", "id": "aa61b45e", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='exercises_7_4'></a>     \n", "\n", "| [Top](#section_7_0) | [Restart Section](#section_7_4) | [Next Section](#section_7_5) |\n"]}, {"cell_type": "markdown", "id": "600b5811", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-7.4.1</span>\n", "\n", "Run the previous fit with just a regular linear fit given by $f(x) = ax + b$. How does the $\\chi^{2}$ change? How do the correlations between the variables change? Choose the best option from the following:\n", "\n", "- $\\chi^{2}$ increases and the correlations increase\n", "- $\\chi^{2}$ decreases and the correlations increase\n", "- $\\chi^{2}$ stays approximately the same and the correlations increase\n", "- $\\chi^{2}$ increases and the correlations decrease\n", "- $\\chi^{2}$ decreases and the correlations decrease\n", "- $\\chi^{2}$ stays approximately the same and the correlations decrease\n", "- $\\chi^{2}$ increases and the correlations stay approximately the same\n", "- $\\chi^{2}$ decreases and the correlations stay approximately the same\n", "- $\\chi^{2}$ stays approximately the same and the correlations stay approximately the same"]}, {"cell_type": "code", "execution_count": null, "id": "ff980996", "metadata": {"tags": ["draft", "py"]}, "outputs": [], "source": ["#>>>EXERCISE: L7.4.1\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n"]}, {"cell_type": "markdown", "id": "1118b3c4", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_7_5'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L7.5 Correlations Between Fit Parameters: Part 2</h2>     \n", "\n", "| [Top](#section_7_0) | [Previous Section](#section_7_4) | [Exercises](#exercises_7_5) |\n"]}, {"cell_type": "code", "execution_count": null, "id": "2c09be2f", "metadata": {"tags": ["learner", "py", "lect_05", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L7.5-runcell01\n", "\n", "def fnew(x,a,b):\n", "    pVal=b*(1-x)\n", "    return a*x+pVal\n", "\n", "model  = lmfit.Model(fnew)\n", "p = model.make_params(a=1000,b=1000)\n", "\n", "result = model.fit(data=yhist,x=xhist, params=p, weights=xweights)\n", "lmfit.report_fit(result)\n", "plt.figure()\n", "result.plot()\n", "plt.show()\n", "\n", "x0 = np.array([1000,1000])\n", "ps = [x0]\n", "sol=opt.minimize(chi2, x0)\n", "print(sol)\n", "\n", "x = np.linspace(sol.x[0]*0.99,sol.x[0]*1.01, 100)\n", "y = np.linspace(sol.x[1]*0.99,sol.x[1]*1.01, 100)\n", "X, Y = np.meshgrid(x, y)\n", "Z = np.array([chi2([x,y]) for (x,y) in zip(X.ravel(), Y.ravel())]).reshape(X.shape)\n", "plotColorsAndContours(X,Y,Z)\n", "plt.show()\n"]}, {"cell_type": "code", "execution_count": null, "id": "12807150", "metadata": {"tags": ["learner", "py", "lect_05", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L7.5-runcell02\n", "\n", "print(np.sqrt(2*sol.hess_inv))\n", "#The diagonals are the uncertainty lmfit quotes\n", "\n", "#Really the best way to do this is to get the eigen values using an linear algebra problem\n", "import numpy.linalg as la\n", "w, v=la.eig(2*sol.hess_inv)\n", "print(\"values\",w,\"vectors\",v)\n", "\n", "#Now let's plot the eigenvectors\n", "from matplotlib.patches import Ellipse\n", "def get_cov_ellipse(cov, centre, nstd, **kwargs):\n", "    \"\"\"\n", "    Return a matplotlib Ellipse patch representing the covariance matrix\n", "    cov centred at centre and scaled by the factor nstd.\n", "\n", "    \"\"\"\n", "    # Find and sort eigenvalues and eigenvectors into descending order\n", "    eigvals, eigvecs = np.linalg.eigh(cov)\n", "    order = eigvals.argsort()[::-1]\n", "    eigvals, eigvecs = eigvals[order], eigvecs[:, order]\n", "\n", "    # The anti-clockwise angle to rotate our ellipse by \n", "    vx, vy = eigvecs[:,0][0], eigvecs[:,0][1]\n", "    theta = np.arctan2(vy, vx)\n", "\n", "    # Width and height of ellipse to draw\n", "    width, height = 2 * nstd * np.sqrt(eigvals)\n", "    return Ellipse(xy=centre, width=width, height=height,angle=np.degrees(theta), **kwargs)\n", "\n", "err_ellipse=get_cov_ellipse(2*sol.hess_inv,sol.x,1)\n", "fig, ax = plt.subplots(1, 1)\n", "c = ax.pcolor(X,Y,Z,cmap='RdBu')\n", "fig.colorbar(c, ax=ax)\n", "levels = [0.1,1,2.3,4,9, 16, 25, 36, 49, 64, 81, 100]\n", "for i0 in range(len(levels)):\n", "    levels[i0] = levels[i0]+sol.fun\n", "c = plt.contour(X, Y, Z, levels,colors=['red', 'blue', 'yellow','green'])\n", "ax.add_artist(err_ellipse)\n", "plt.show()"]}, {"cell_type": "code", "execution_count": null, "id": "6719aad5", "metadata": {"tags": ["learner", "py", "lect_05", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L7.5-runcell03\n", "\n", "#Now let's get the correlation C(a,b) (see below)\n", "w, v=np.linalg.eig(2*sol.hess_inv)\n", "print(\"c(a,b)\",v[0,1]/v[0,0])\n", "print(\"A deceptively wrong way to get correlation: since its not normalized\",sol.hess_inv[0,1]/sol.hess_inv[0,0])"]}, {"cell_type": "code", "execution_count": null, "id": "6033110d", "metadata": {"tags": ["learner", "py", "lect_05", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L7.5-runcell04\n", "\n", "import lmfit\n", "\n", "def fnew(x,a,b):\n", "    pVal=b*(1-x)\n", "    return a*x+pVal\n", "\n", "#Randomly sample points in the above range\n", "def maketoy(iy):\n", "    toy=np.array([])\n", "    #go through the y-values and Poisson fluctuate\n", "    for i0 in range(len(iy)):\n", "        pVal = np.random.normal (iy[i0],np.sqrt([iy[i0]]))\n", "        toy = np.append(toy,float(pVal))\n", "    return toy\n", "\n", "def fittoy(ibin,iy):\n", "    #generate toy\n", "    toy=maketoy(iy)\n", "    #now fit\n", "    model  = lmfit.Model(fnew)\n", "    p = model.make_params(a=1000,b=10)\n", "    xweights=np.array([])\n", "    #setup poison weight\n", "    for i0 in range(len(toy)):\n", "        xweights = np.append(xweights,1./math.sqrt(toy[i0]))\n", "    result = model.fit(data=toy,x=ibin, params=p, weights=xweights)\n", "    return result.params[\"a\"].value,result.params[\"b\"].value\n", "\n", "ntoys=1000\n", "lAs=np.array([])\n", "lBs=np.array([])\n", "for i0 in range(ntoys):\n", "    pA,pB=fittoy(xhist,yhist)\n", "    lAs = np.append(lAs,pA)\n", "    lBs = np.append(lBs,pB)\n", "\n", "err_ellipse=get_cov_ellipse(2*sol.hess_inv,sol.x, 1)\n", "fig, ax = plt.subplots(1, 1)\n", "c = ax.pcolor(X,Y,Z,cmap='RdBu')\n", "fig.colorbar(c, ax=ax)\n", "c = plt.contour(X, Y, Z, levels,colors=['red', 'blue', 'yellow','green'])\n", "ax.add_artist(err_ellipse)\n", "plt.plot(lAs,lBs,c='black',marker='.',linestyle = 'None')\n", "plt.show()"]}, {"cell_type": "code", "execution_count": null, "id": "a5ae9db2", "metadata": {"tags": ["learner", "py", "lect_05", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L7.5-runcell05\n", "\n", "#Now let's run a. linear regression\n", "def variance(isamples):\n", "    mean=isamples.mean()\n", "    n=len(isamples)\n", "    tot=0\n", "    for pVal in isamples:\n", "        tot+=(pVal-mean)**2\n", "    return tot/n\n", "\n", "def covariance(ixs,iys):\n", "    meanx=ixs.mean()\n", "    meany=iys.mean()\n", "    n=len(ixs)\n", "    tot=0\n", "    for i0 in range(len(ixs)):\n", "        tot+=(ixs[i0]-meanx)*(iys[i0]-meany)\n", "    return tot/n\n", "\n", "print(\"A:\",lAs.mean(),\"+/-\",lAs.std())\n", "print(\"B:\",lBs.mean(),\"+/-\",lBs.std())\n", "print(\"Cov:\",covariance(lAs,lBs),\"A Variance:\",variance(lAs),\"B Variance:\",variance(lBs))\n", "print(\"Check with Hessian:\",2*sol.hess_inv)\n", "print(\"Cor:\",covariance(lAs,lBs)/math.sqrt(variance(lAs)*variance(lBs)),\"A Variance:\",1.,\"B Variance:\",1.)"]}, {"cell_type": "markdown", "id": "a4251144", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='exercises_7_5'></a>   \n", "\n", "| [Top](#section_7_0) | [Restart Section](#section_7_5) |\n"]}, {"cell_type": "markdown", "id": "0b0b7275", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-7.5.1</span>\n", "\n", "Repeat the previous analysis using the uncorrelated fit, $f(x) = a x + b$. Specifically, plot the ellipse and compute the uncertainties in $a$ and $b$? Do they correspond with lmfit? \n"]}, {"cell_type": "code", "execution_count": null, "id": "9a896c73", "metadata": {"tags": ["draft", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>EXERCISE: L7.5.1\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n"]}], "metadata": {"celltoolbar": "Tags", "kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}