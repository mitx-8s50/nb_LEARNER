{"cells": [{"cell_type": "markdown", "id": "802238db", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<hr style=\"height: 1px;\">\n", "<i>This notebook was authored by the 8.S50x Course Team, Copyright 2022 MIT All Rights Reserved.</i>\n", "<hr style=\"height: 1px;\">\n", "<br>\n", "\n", "<h1>Lesson 8: Fitting Neutrino Data</h1>\n"]}, {"cell_type": "markdown", "id": "7348d65b", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_8_0'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L8.0 Overview</h2>\n"]}, {"cell_type": "markdown", "id": "3e2dfdd9", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<h3>Navigation</h3>\n", "\n", "<table style=\"width:100%\">\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_8_1\">L8.1 Neutrino Oscillations</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_8_1\">L8.1 Exercises</a></td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_8_2\">L8.2 Loading the Data</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_8_2\">L8.2 Exercises</a></td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_8_3\">L8.3 Fitting the Master Function to the Data</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_8_3\">L8.3 Exercises</a></td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_8_4\">L8.4 Principal Component Analysis</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_8_4\">L8.4 Exercises</a></td>\n", "    </tr>\n", "</table>\n", "\n"]}, {"cell_type": "markdown", "id": "548823b4", "metadata": {"tags": ["learner", "catsoop_00", "md"]}, "source": ["<h3>Learning Objectives</h3>\n", "\n", "In this Lesson we will consider the following questions:\n", "\n", "- Why Do Neutrinos Oscillate?\n", "- What does this mean in terms of mesurement?\n", "\n", "We will also briefly explore the topic of principal component analysis.\n"]}, {"cell_type": "markdown", "id": "ab603bc7", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<h3>Importing Data (Colab Only)</h3>\n", "\n", "If you are in a Google Colab environment, run the cell below to import the data for this notebook. Otherwise, if you have downloaded the course repository, you do not have to run the cell below.\n", "\n", "See the source and attribution information below:\n", "\n", ">data: data/L08/NOvA_2020_data_histograms.root, data/L08/NOvA_2020_data_release_predictions_with_systs_all_hists.root <br>\n", ">source: https://publicdocs.fnal.gov/cgi-bin/ShowDocument?docid=17, https://doi.org/10.1103/PhysRevD.106.032004 <br>\n", ">attribution: M.\u2009A. Acero et al. (The NOvA Collaboration), Phys. Rev. D 106, 032004 <br>\n", ">license type: Public document"]}, {"cell_type": "code", "execution_count": null, "id": "1b451aa5", "metadata": {"tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L8.0-runcell00\n", "\n", "#importing data from git repository\n", "\n", "!git init\n", "!git remote add -f origin https://github.com/mitx-8s50/nb_LEARNER/\n", "!git config core.sparseCheckout true\n", "!echo 'data/L08' >> .git/info/sparse-checkout\n", "!git pull origin main"]}, {"cell_type": "markdown", "id": "3e877a00", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Importing Libraries</h3>\n", "\n", "Before beginning, run the cells below to import the relevant libraries for this notebook."]}, {"cell_type": "code", "execution_count": null, "id": "7b11924b", "metadata": {"tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L8.0-runcell01\n", "\n", "# for Colab users\n", "!pip install lmfit\n", "!pip install uproot\n", "!pip install astroML"]}, {"cell_type": "code", "execution_count": null, "id": "444b6f96", "metadata": {"tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L8.0-runcell02\n", "\n", "import numpy as np                 #https://numpy.org/doc/stable/\n", "from scipy import optimize as opt  #https://docs.scipy.org/doc/scipy/reference/optimize.html\n", "import matplotlib.pyplot as plt    #https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.html\n", "import lmfit                       #https://lmfit.github.io/lmfit-py/ \n", "import scipy.stats as stats        #https://docs.scipy.org/doc/scipy/reference/stats.html\n", "import uproot                      #https://uproot.readthedocs.io/en/latest/\n", "from sklearn.decomposition import PCA                   #https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\n", "from sklearn.datasets import fetch_lfw_people           #https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_lfw_people.html\n", "from sklearn.decomposition import PCA as RandomizedPCA  \n", "from astroML.datasets import sdss_corrected_spectra"]}, {"cell_type": "markdown", "id": "9dc917bb", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Setting Default Figure Parameters</h3>\n", "\n", "The following code cell sets default values for figure parameters."]}, {"cell_type": "code", "execution_count": null, "id": "c176308a", "metadata": {"tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L8.0-runcell03\n", "\n", "#set plot resolution\n", "%config InlineBackend.figure_format = 'retina'\n", "\n", "#set default figure parameters\n", "plt.rcParams['figure.figsize'] = (9,6)\n", "\n", "medium_size = 12\n", "large_size = 15\n", "\n", "plt.rc('font', size=medium_size)          # default text sizes\n", "plt.rc('xtick', labelsize=medium_size)    # xtick labels\n", "plt.rc('ytick', labelsize=medium_size)    # ytick labels\n", "plt.rc('legend', fontsize=medium_size)    # legend\n", "plt.rc('axes', titlesize=large_size)      # axes title\n", "plt.rc('axes', labelsize=large_size)      # x and y labels\n", "plt.rc('figure', titlesize=large_size)    # figure title\n"]}, {"cell_type": "markdown", "id": "d98847c2", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_8_1'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L8.1 Neutrino Oscillations</h2>  \n", "\n", "| [Top](#section_8_0) | [Previous Section](#section_8_0) | [Exercises](#exercises_8_1) | [Next Section](#section_8_2) |\n"]}, {"cell_type": "markdown", "id": "c5e525bc", "metadata": {"tags": ["learner", "md", "8S50x"]}, "source": ["*The material in this section is discussed in the video **<a href=\"https://courses.mitxonline.mit.edu/learn/course/course-v1:MITxT+8.S50.1x+3T2022/block-v1:MITxT+8.S50.1x+3T2022+type@sequential+block@seq_LS8/block-v1:MITxT+8.S50.1x+3T2022+type@vertical+block@vert_LS8_vid1\" target=\"_blank\">HERE</a>.** You are encouraged to watch that video and use this notebook concurrently.*"]}, {"cell_type": "markdown", "id": "236193f6", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Slides</h3>\n", "\n", "Run the code below to view the slides for this section, which are discussed in the related video. You can also open the slides in a separate window <a href=\"https://mitx-8s50.github.io/slides/L08/slides_L08_01.html\" target=\"_blank\">HERE</a>."]}, {"cell_type": "code", "execution_count": null, "id": "55fd95cf", "metadata": {"tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L8.1-slides\n", "\n", "from IPython.display import IFrame\n", "IFrame(src='https://mitx-8s50.github.io/slides/L08/slides_L08_01.html', width=970, height=550)"]}, {"cell_type": "markdown", "id": "91526c49", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='exercises_8_1'></a>     \n", "\n", "| [Top](#section_8_0) | [Restart Section](#section_8_1) | [Next Section](#section_8_2) |\n"]}, {"cell_type": "markdown", "id": "ff58d903", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-8.1.1</span>\n", "\n", "For a neutrino beam that has a detector 1000km away, like is present with the DUNE neutrino experiment, what is the optimal energy (in GeV) to observe muon neutrino disappearance given a neutrino energy > 0.5 GeV?\n", "\n", "Use the master formula in the code cell below, which outputs the probability of oscillating from a muon neutrino to a muon neutrino, as a function of energy (in units of GeV). Find the minimum of the function to determine where the muon neutrino disappears.\n", "\n", "Enter your answer as a number with precision 1e-2. "]}, {"cell_type": "code", "execution_count": null, "id": "d911ae4b", "metadata": {"tags": ["draft", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>EXERCISE: L8.1.1\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "def master_formula(E):\n", "    deltam=1*1e-3\n", "    L=1000\n", "    sin2theta23=0.57\n", "    xval=1.27*deltam*scale1*L/E\n", "    #val=1-4*scale2*sin2theta23*(1-scale2*sin2theta23)*np.sin(xval)**2\n", "    val=1-4*sin2theta23*(1-sin2theta23)*(np.sin(xval)**2)\n", "    return val\n", "\n"]}, {"cell_type": "markdown", "id": "7ff65cc6", "metadata": {"tags": ["learner", "md"]}, "source": [">#### Follow-up 8.1.1a (ungraded)\n", ">\n", ">What other parameters of this experiment could we vary to observe the neutrino disappearance?"]}, {"cell_type": "markdown", "id": "1b727f3a", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_8_2'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L8.2 Loading the Data</h2>  \n", "\n", "| [Top](#section_8_0) | [Previous Section](#section_8_1) | [Exercises](#exercises_8_2) | [Next Section](#section_8_3) |\n"]}, {"cell_type": "markdown", "id": "ecb21f38", "metadata": {"tags": ["learner", "md", "8S50x"]}, "source": ["*The material in this section is discussed in the video **<a href=\"https://courses.mitxonline.mit.edu/learn/course/course-v1:MITxT+8.S50.1x+3T2022/block-v1:MITxT+8.S50.1x+3T2022+type@sequential+block@seq_LS8/block-v1:MITxT+8.S50.1x+3T2022+type@vertical+block@vert_LS8_vid2\" target=\"_blank\">HERE</a>.** You are encouraged to watch that video and use this notebook concurrently.*"]}, {"cell_type": "markdown", "id": "6f02585e", "metadata": {"tags": ["learner", "md", "lect_02"]}, "source": ["<h3>Fitting Neutrino data</h3>\n", "\n", "In this part of the lecture, I would like to fit data from one of the recent neutrino experiments. The data consists of events at various energies that are observed from neutrino-matter interactions in the NO$\\nu$A experiment in Minnesota. Details about this experiment can be found <a href=\"https://inspirehep.net/files/0a3cd74d55753d242b2a364ce70a5e0e\" target=\"_blank\">here</a>.\n", "\n", "There are 3 type of neutrinos, the electron, muon and $\\tau$ neutrino. These neutrinos all interact in roughly the same way, through the weak interaction. Additionally, these neutrinos are all known to be very light. Lastly, it is found that these neutrinos are capable of changing from one type to another over time. What that means is that an electron neutrino can oscillate into a muon neutrino or a $\\tau$ neutrino, and so on. The fact that they oscillate is a bit of a mystery, but what we do know is that this means the way mass is generated for the neutrinos is a different mechanism to the way it interacts.  \n", "\n", "To understand the data, we need to consider the key components of this experiment, which is that we first create a beam of neutrinos at Fermilab in Illinois, and we then fire this beam at the NO$\\nu$A experiment in Minnesota. At NO$\\nu$A we check to see what we observe. Since neutrinos interact very weakly, we do this by counting interactions of muon neutrinos in two separate detectors, one at Fermilab and the other at NO$\\nu$A. Between Fermilab and NO$\\nu$A, some of these muon neutrinos will oscillate into other types of neutrino through quantum mechanical mixing. This is a great way to test the properties of quantum mechanics. You can read more about that <a href=\"https://arxiv.org/abs/1602.00041\" target=\"_blank\">here</a>.\n", "\n", "That being said, what we expect to compare is the shape of the observed events from the input beam, with the shape of the output beam. Since neutrinos interact very weakly, the way we perform this is we put a large detector near the input beam, and we measure the rate of muon neutrinos, and then we put an even larger detector at the output beam, and we measure the rate. Let's take a look at this data. \n", "\n", "The data is in root format, like the project. We will use uproot to load the data and see what it is like. "]}, {"cell_type": "code", "execution_count": null, "id": "fd7586f0", "metadata": {"tags": ["learner", "py", "lect_02", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L8.2-runcell01\n", "\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "from scipy import stats\n", "import uproot\n", "\n", "file = uproot.open(\"data/L08/NOvA_2020_data_histograms.root\")\n", "\n", "#print(file.classnames())\n", "\n", "def plot(iLabel,iFile,iColor):\n", "    bin_edges = iFile[iLabel].axis().edges()\n", "    bin_centers = 0.5*(bin_edges[1:] + bin_edges[:-1])\n", "    plt.xlabel(\"E (GeV)\")\n", "    plt.ylabel(\"$N_{events}$\")\n", "    plt.errorbar(bin_centers,iFile[iLabel].values(),yerr=iFile[iLabel].errors(),marker='.',linestyle = '', color = iColor,label=iLabel)    \n", "    \n", "plot(\"neutrino_mode_numu_quartile1\",file,'black')\n", "plot(\"antineutrino_mode_numu_quartile1\",file,'red')\n", "plt.legend()\n", "plt.show()\n", "\n", "    "]}, {"cell_type": "markdown", "id": "d793458a", "metadata": {"tags": ["learner", "md", "lect_02"]}, "source": ["So, we see two neutrino samples with four quartiles. The quartiles turn out to be different quality selections on the data. Quartile 1 is the most sensitive quartile, whereas Quartile 2, 3, and 4 are progressively less sensitive. How these quartiles are chosen depends on the beam, detector performance, and quality of the reconstruction. For our measurement, we can sum them all up and treat them as one measurement. \n", "\n", "The other label we see is the anti-neutrino and neutrino labels for the type of beam. The beam at Fermilab can be run in two different modes. One mode is neutrino mode. In this mode, particles are fired into the beam that mostly decay into regular neutrinos. The other mode is anti-neutrino mode, in that scenario particles are fired into the beam that decay into anti-neutrinos. \n", "\n", "Suffice it to say there is no guarantee that anti-neutrinos and neutrinos oscillate in the same way, so we keep these samples separate. We can look at the separate quartiles, but let's do that later. \n", "\n", "Instead, let's look at another root file that has the predictions for what we expect the neutrino beam to look like. "]}, {"cell_type": "code", "execution_count": null, "id": "9fad2131", "metadata": {"tags": ["learner", "py", "lect_02", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L8.2-runcell02\n", "\n", "filePred = uproot.open(\"data/L08/NOvA_2020_data_release_predictions_with_systs_all_hists.root\")\n", "\n", "#print(filePred.classnames())\n", "plot(\"prediction_components_numu_fhc_Quartile1/NoOscillations_Total_pred\",filePred,'black')\n", "plot(\"prediction_components_numu_rhc_Quartile1/NoOscillations_Total_pred\",filePred,'red')\n", "plt.legend()\n", "plt.show()"]}, {"cell_type": "markdown", "id": "960998ae", "metadata": {"tags": ["learner", "md", "lect_02"]}, "source": ["This file is messy. I won't go into the details but let me put some labels here, to reconcile things. First of all, we see prediction_components_rhc_Quartile. Quartile is the same as before. RHC, and its counterpart FHC standard for \"Reverse Horn Current\" (RHC) and \"Foward Horn Current\".   The FHC configuration focuses charged particles with positive polarity (pions, $\\pi^{+}$ and Kaons, $K^{+}$) which decay to give a neutrino beam ($\\nu_{\\mu}$) whereas, the RHC configuration focuses charged particles with negative polarity (pions,$\\pi^{-}$ and Kaons, $K^{-}$) that decay to give an anti-neutrino enhanced beam ($\\bar{\\nu}_{\\mu}$). \n", "\n", "Furthermore, the predictions are done under the assumption that there are no oscillations. Hence, the \"NoOscillations\" label."]}, {"cell_type": "markdown", "id": "6731562c", "metadata": {"tags": ["learner", "md", "lect_02"]}, "source": ["<h3>Neutrino Oscillations</h3>\n", "\n", "To understand how to fit this data, we follow from the master formula for neutrino oscillations. For those familiar with quantum mechanics, let's write out what the neutrino particle eigen-state is:\n", "\n", "$$\n", "\\begin{equation}\n", " |\\nu_{\\mu}\\rangle = U^{*}_{\\mu 1}|\\nu_{1}\\rangle + U^{*}_{\\mu 2}|\\nu_{2}\\rangle + U^{*}_{\\mu 3}|\\nu_{3}\\rangle\n", "\\end{equation}\n", "$$\n", "\n", "Where $U_{\\mu i}$ is the muon row of the oscillation matrix. When you evolve each piece $i$ of this state over time $t$ and allow the neutrino of energy $E$ and mass $m_{i}$ to move forward a length $L$, you will get that (skipping some steps) \n", "\n", "$$\n", "\\begin{eqnarray}\n", " |\\nu_{i}(L)\\rangle = e^{-iEt-\\vec{p}\\cdot\\vec{x}}|\\nu_{i}\\rangle \\\\\n", "               \\approx e^{-i\\frac{Lm^{2}_{i}}{2E}}|\\nu_{i}(L=0)\\rangle\n", "\\end{eqnarray}\n", "$$\n", "\n", "and, thus, even for identical energies $E$, separate mass eigenstates $\\nu_{1}$, $\\nu_{2}$ and $\\nu_{3}$ will evolve at different rates because of the $m_{i}$ term. What that means is that the probability for neutrinos to still be there can be written by the following master formula: \n", "\n", "$$\n", "\\begin{eqnarray}\n", " P_{\\mu\\rightarrow\\mu} & = &  \\left|\\langle\\nu_{\\mu}(L)|\\nu_{\\mu}(0)\\rangle\\right|^{2} \\\\\n", "                       & \\approx & 1-\\sin^{2}\\theta_{23}\\sin^{2}\\left(\\frac{1.27\\Delta m^{2}_{23}}{E} L\\right)  \n", "\\end{eqnarray}\n", "$$\n", "\n", "where $\\sin^{2}\\theta_{23}$ is the parameter that describes the rate of oscillation between muon neutrinos and $\\tau$ neutrinos, and $m^{2}_{23}=m_{3}^2-m_{2}^2$ is the mass difference between the $\\tau$ and muon neutrino. You may ask, why is the electron neutrino not involved. It turns out that its rate of oscillations is too small to impact this measurement. \n", "\n", "Given that, what we can do then is take our original data, divide it by our no oscillation expectation and fit it. In this case, what we would like to extract is not just one parameter, but two parameters $\\theta_{23}$ and $m^{2}_{23}$. Let's see if we can get them. \n", "\n", "First, let's prepare our ratio data, starting by constructing the ratio for each quantile.\n"]}, {"cell_type": "code", "execution_count": null, "id": "2506d164", "metadata": {"tags": ["learner", "py", "lect_02", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L8.2-runcell03\n", "\n", "nquartiles=4\n", "label=\"neutrino_mode_numu_quartile\"\n", "predlabel0=\"prediction_components_numu_fhc_Quartile\"\n", "predlabel1=\"/NoOscillations_Total_pred\"\n", "bin_edges=file[label+\"1\"].axis().edges()\n", "x = 0.5*(bin_edges[1:] + bin_edges[:-1])\n", "def ratio(iQuartile,iPlot=False):\n", "    ytop=file[label+str(i0+1)].values()\n", "    ytop_err=file[label+str(i0+1)].values()\n", "    ybot=filePred[predlabel0+str(i0+1)+predlabel1].values()\n", "    #ybot_err=file[label+str(i0+1)].values() we will skip this since the error is much smaller\n", "    y = ytop/ybot\n", "    y_err = ytop_err/ybot #we will ignore the ybot error since it is tiny\n", "    if iPlot:\n", "        plt.errorbar(x,y,yerr=y_err,marker='.',linestyle = '',label=\"Quartile \"+str(i0+1))\n", "    return y,y_err\n", "    \n", "for i0 in range(nquartiles):\n", "    ratio(i0,True)\n", "    \n", "plt.xlabel(\"E(GeV)\")\n", "plt.ylabel(\"Ratio\")\n", "plt.legend()\n", "plt.show()"]}, {"cell_type": "markdown", "id": "a6cf8051", "metadata": {"tags": ["learner", "md", "lect_02"]}, "source": ["Now, what we would like to do is combine these ratios together. However, we need to do an average weighted by their uncertainties. To do that, we will define for the i-th bin in the ratio $r_{i}$ for the j-th quartile, the <a href=\"https://en.wikipedia.org/wiki/Weighted_arithmetic_mean\" target=\"_blank\">weighted mean</a>\n", "\n", "$$\n", "\\begin{equation}\n", " \\bar{r}_{i} = \\frac{1}{\\sum_{j=1}^{4} \\frac{1}{\\sigma^{2}_{ij}} }\\sum_{j=1}^{4} \\frac{1}{\\sigma^{2}_{ij}} r_{ij}\n", "\\end{equation}\n", "$$\n", "\n", "This is the maximum likelihood mean for normally distributed independent variables (see above). The weighted variance is then given by propagation of errors as\n", "\n", "$$\n", "\\begin{equation}\n", " \\sigma^{2}_{i} = \\left(\\frac{1}{\\sum_{j=1}^{4} \\frac{1}{\\sigma^{2}_{ij}} }\\right)^{2}\\sum_{j=1}^{4} \\frac{1}{\\sigma^{4}_{ij}} \\sigma^{2}_{ij} \\\\\n", "  \\sigma^{2}_{i} = \\left(\\frac{1}{\\sum_{j=1}^{4} \\frac{1}{\\sigma^{2}_{ij}} }\\right)^{2}\\sum_{j=1}^{4} \\frac{1}{\\sigma^{2}_{ij}} \\\\\n", "\\sigma^{2}_{i} = \\left(\\frac{1}{\\sum_{j=1}^{4} \\frac{1}{\\sigma^{2}_{ij}} }\\right)\n", "\\end{equation}\n", "$$\n", "\n", "Let's go ahead and combine them."]}, {"cell_type": "code", "execution_count": null, "id": "63cd4ba8", "metadata": {"tags": ["learner", "py", "lect_02", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L8.2-runcell04\n", "\n", "def combinedRatio():\n", "    y,y_err = ratio(0,False)\n", "    y_arrs=np.array([y])\n", "    weight_arrs=np.array([y_err])\n", "    for i0 in range(nquartiles-1):\n", "        y,y_err = ratio(i0,False)\n", "        y_arrs=np.vstack([y_arrs,y])\n", "        weights=1./(y_err**2)\n", "        weights[weights == np.inf] = 0.1\n", "        weight_arrs = np.vstack([weight_arrs,weights])\n", "    #Now do the weighted \n", "    yout=np.average(y_arrs,weights=weight_arrs,axis=0)\n", "    weights=np.sum(weight_arrs,axis=0)\n", "    return yout,1/weights**0.5,weights**0.5\n", "\n", "label=\"neutrino_mode_numu_quartile\"\n", "predlabel0=\"prediction_components_numu_fhc_Quartile\"\n", "y,yerr,weights=combinedRatio()    \n", "\n", "label=\"antineutrino_mode_numu_quartile\"\n", "predlabel0=\"prediction_components_numu_rhc_Quartile\"\n", "y_anti,yerr_anti,weights_anti=combinedRatio()    \n", "\n", "plt.errorbar(x,y,yerr=yerr,marker='.',linestyle = '',label=\"neutrino\")\n", "plt.errorbar(x,y_anti,yerr=yerr_anti,marker='.',linestyle = '',label=\"anti-neutrino\")\n", "plt.xlabel(\"E(GeV)\")\n", "plt.ylabel(\"Ratio\")\n", "plt.legend()\n", "plt.ylim(0,1.5)\n", "plt.show()"]}, {"cell_type": "markdown", "id": "e8542f60", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='exercises_8_2'></a>     \n", "\n", "| [Top](#section_8_0) | [Restart Section](#section_8_2) | [Next Section](#section_8_3) |\n"]}, {"cell_type": "markdown", "id": "8d6e8f87", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-8.2.1</span>\n", "\n", "In this problem, we want to show that the weighted average with weight given by $\\frac{1}{\\sigma^2}$ minmizes the uncertainty. For this case, consider a weighted average of two numbers $x$ and $y$. We can define the weighted average $\\bar{x}$ as \n", "\n", "$$\\bar{x}=f x + (1-f) y$$\n", "\n", "where $0 \\geq f \\geq 1$ is our weight factor. The uncertainty on $\\bar{x}$ can be written as \n", "\n", "\n", "$$\\sigma^2_{\\bar{x}}=f^2 \\sigma_{x}^2 + (1-f)^2 \\sigma_{y}^2$$ \n", "\n", "To minimize the uncertainty all we need to do is \n", "\n", "$$ \\frac{d\\sigma_{\\bar{x}}^2}{df} = 0$$\n", "\n", "What is the value of $f$ that minimizes the uncertainty? Express your answer using `sigmax` for $\\sigma_{x}$ and `sigmay` for $\\sigma_{y}$.\n"]}, {"cell_type": "markdown", "id": "77205726", "metadata": {"tags": ["md", "learner"]}, "source": [">#### Follow-up 8.2.1a (ungraded)\n", ">\n", ">Show that this yields the weighted average formula that we previously derived. Try on your own, or see the solution to the problem above.\n", ">\n", ">Hint: You might have to multiply your final result by:\n", ">\n", ">$$\\frac{\\frac{1}{\\sigma_{x}^2\\sigma_{y}^2}}{\\frac{1}{\\sigma_{x}^2\\sigma_{y}^2}}$$"]}, {"cell_type": "markdown", "id": "51599a46", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_8_3'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L8.3 Fitting the Master Function to the Data</h2>  \n", "\n", "| [Top](#section_8_0) | [Previous Section](#section_8_2) | [Exercises](#exercises_8_3) | [Next Section](#section_8_4) |\n"]}, {"cell_type": "markdown", "id": "040b4e29", "metadata": {"tags": ["learner", "md", "8S50x"]}, "source": ["*The material in this section is discussed in the video **<a href=\"https://courses.mitxonline.mit.edu/learn/course/course-v1:MITxT+8.S50.1x+3T2022/block-v1:MITxT+8.S50.1x+3T2022+type@sequential+block@seq_LS8/block-v1:MITxT+8.S50.1x+3T2022+type@vertical+block@vert_LS8_vid3\" target=\"_blank\">HERE</a>.** You are encouraged to watch that video and use this notebook concurrently.*"]}, {"cell_type": "markdown", "id": "811a7e8f", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Slides</h3>\n", "\n", "Run the code below to view the slides for this section, which are discussed in the related video. You can also open the slides in a separate window <a href=\"https://mitx-8s50.github.io/slides/L08/slides_L08_03.html\" target=\"_blank\">HERE</a>."]}, {"cell_type": "code", "execution_count": null, "id": "7147b619", "metadata": {"tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L8.3-slides\n", "\n", "from IPython.display import IFrame\n", "IFrame(src='https://mitx-8s50.github.io/slides/L08/slides_L08_03.html', width=970, height=550)"]}, {"cell_type": "markdown", "id": "6f4c14be", "metadata": {"tags": ["learner", "md", "lect_03"]}, "source": ["Ok, now that we have the points, let's finally fit the data.\n", "\n", "Note, the points shown by L8.2-runcell04 that have very large error bars are not shown (or fit) by this code due to the selection `[iY > 0]`. "]}, {"cell_type": "code", "execution_count": null, "id": "789833e8", "metadata": {"tags": ["learner", "py", "lect_03", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L8.3-runcell01\n", "\n", "import lmfit\n", "deltam=1*1e-3\n", "L=810\n", "sin2theta23=1.0\n", "def func(x,scale1,scale2):\n", "    xval=1.27*deltam*scale1*L/x\n", "    #val=1-4*scale2*sin2theta23*(1-scale2*sin2theta23)*np.sin(xval)**2\n", "    val=1-4*scale2*(1-scale2)*(np.sin(xval)**2)\n", "    return val\n", "\n", "def fit(iX,iY,iWeight):\n", "    model  = lmfit.Model(func)\n", "    p = model.make_params(scale1=1.0,scale2=0.6)\n", "    result = model.fit(x=iX[iY > 0],data=iY[iY > 0], params=p, weights=iWeight[iY > 0])\n", "    lmfit.report_fit(result)\n", "    result.plot()\n", "    print(\"Fit1 chi2 probability: \",stats.chi2.cdf(result.chisqr,result.nfree))\n", "\n", "fit(x,y,weights)\n", "fit(x,y_anti,weights_anti)"]}, {"cell_type": "markdown", "id": "c8e98b81", "metadata": {"tags": ["learner", "md", "lect_03"]}, "source": ["<h3>Profiling Neutrino Parameters</h3>\n", "\n", "So, we see that neutrinos oscillate. However, what if we want to understand how the values of the parameters vary. Let's do a quick scan of the parameters, computing the likelihood for each. \n", "\n", "We can write minus 2 times the log likelihood in terms of the $\\chi^{2}$: \n", "\n", "\n", "$$\n", "\\begin{eqnarray}\n", "\\chi^{2}(x|\\vec{\\theta}) &=& \\sum_{i=1}^{N} \\frac{(x_{i}-f(x_{i}|\\vec{\\theta}))^2}{\\sigma_{i}^{2}} \\\\\n", "-2 \\log\\left(\\mathcal{L}(x|\\vec{\\theta})\\right) &=& \\sum_{i=1}^{N} \\frac{(x_{i}-f(x_{i}|\\vec{\\theta}))^2}{\\sigma_{i}^{2}} \\\\\n", "\\end{eqnarray}\n", "$$"]}, {"cell_type": "code", "execution_count": null, "id": "2fdcfd65", "metadata": {"tags": ["learner", "py", "lect_03", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L8.3-runcell02\n", "\n", "def twoLogLike(var,iX=x,iY=y,iWeights=weights):\n", "    lTot=0\n", "    xtest=func(iX,var[1],var[0])\n", "    lTot = weights*(iY-xtest)\n", "    return np.sum(lTot**2)\n", "\n", "from scipy import optimize as opt\n", "x0 = np.array([1,1])\n", "sol=opt.minimize(twoLogLike, x0)\n", "\n", "def plotScan(sol):\n", "    #Look the same answers, now let's plot the chi2\n", "    xscan = np.linspace(sol.x[0]*0.6,sol.x[0]*2.5, 100)\n", "    yscan = np.linspace(sol.x[1]*0.6,sol.x[1]*2.0, 100)\n", "    X, Y = np.meshgrid(xscan, yscan)\n", "    levels = [0.1,1,2.3,4,9, 16, 25, 36, 49, 64, 81, 100]\n", "    for i0 in range(len(levels)):\n", "        levels[i0] = levels[i0]+sol.fun\n", "    Z = np.array([twoLogLike([xscan,yscan]) for (xscan,yscan) in zip(X.ravel(), Y.ravel())]).reshape(X.shape)\n", "    fig, ax = plt.subplots(1, 1)\n", "    c = ax.pcolor(X,Y,Z,cmap='RdBu')\n", "    fig.colorbar(c, ax=ax)\n", "    c = plt.contour(X, Y, Z, levels,colors=['red', 'blue', 'yellow','green'])\n", "    plt.xlabel(\"$\\sin^{2}\\Theta_{23}$\")\n", "    plt.ylabel(\"$\\Delta m^{2}_{23}$\")\n", "    plt.show()\n", "\n", "plotScan(sol)"]}, {"cell_type": "markdown", "id": "d1fc31a0", "metadata": {"tags": ["learner", "md", "lect_03"]}, "source": ["Now we see two circles. What exactly does this mean? Let's profile one variable at a time. First we can look at a profile of $\\sin^{2}\\theta_{23}$ for the case where we mix $\\Delta m^{2}_{23}$ to the best fit value. Then we can do the opposite, fixing $\\sin^{2}\\theta_{23}$ to one of the two minima and profiling  $\\Delta m^{2}_{23}$.\n"]}, {"cell_type": "code", "execution_count": null, "id": "619e57ac", "metadata": {"tags": ["learner", "py", "lect_03", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L8.3-runcell03\n", "\n", "#Now let's fix one parameter at the minimum, and profile the other\n", "def scanAxes(sol):\n", "    xscan = np.linspace(sol.x[0]*0.8,sol.x[0]*2.2, 100)\n", "    yscan = np.linspace(sol.x[1]*0.8,sol.x[1]*1.2, 100)\n", "\n", "    xLog = np.array([])\n", "    for pX in xscan:\n", "        xLog = np.append(xLog,twoLogLike(var=[pX,sol.x[1]]))\n", "\n", "    yLog = np.array([])\n", "    for pY in yscan:\n", "        yLog = np.append(yLog,twoLogLike(var=[sol.x[0],pY]))\n", "\n", "    plt.plot(xscan, xLog,label='loglike');\n", "    plt.axhline(sol.fun+1, c='red')\n", "    plt.xlabel(\"$\\sin^{2}\\Theta_{23}$\")\n", "    plt.ylabel(\"2$\\Delta$LL\")\n", "    plt.show()\n", "\n", "    #Now for the other parameter\n", "    plt.plot(yscan,yLog,label='LL');\n", "    plt.axhline(sol.fun+1, c='red')\n", "    plt.xlabel(\"$\\Delta m^{2}_{23}$\")\n", "    plt.ylabel(\"2$\\Delta$LL\")\n", "    plt.show()\n", "    \n", "scanAxes(sol)"]}, {"cell_type": "markdown", "id": "528d02c3", "metadata": {"tags": ["learner", "md", "lect_03"]}, "source": ["So, for $\\sin^{2}(\\theta_{23})$ there are actually two minima that have the same minimal value. This is a degeneracy in the ways neutrinos oscillate that this data cannot resolve. "]}, {"cell_type": "markdown", "id": "4f36f87f", "metadata": {"tags": ["learner", "md", "lect_03"]}, "source": ["This is a complex fit that is hard to  intepret. Let's do the scan for anti-neutrino to see if a difference in parameters between anti and regular neutrino.  A difference in the parameters would mean that anti particles behave differently that regular particles. This is known as <a href=\"https://en.wikipedia.org/wiki/CP_violation\" target=\"_blank\">CP-violation</a> and can possibly explain why the universe is made of predominatly matter!"]}, {"cell_type": "code", "execution_count": null, "id": "37605587", "metadata": {"tags": ["learner", "py", "lect_03", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L8.3-runcell04\n", "\n", "#answer\n", "def twoLogLike(var,iX=x,iY=y_anti,iWeights=weights_anti):\n", "    lTot=0\n", "    xtest=func(iX,var[1],var[0])\n", "    lTot = weights*(iY-xtest)\n", "    return np.sum(lTot**2)\n", "\n", "x0 = np.array([1,1])\n", "sol=opt.minimize(twoLogLike, x0)\n", "plotScan(sol)\n", "scanAxes(sol)"]}, {"cell_type": "markdown", "id": "7abc357f", "metadata": {"tags": ["learner", "md", "lect_03"]}, "source": ["These look almost exactly like the regular neutrino, so sadly we don't see any CP-violation."]}, {"cell_type": "markdown", "id": "d69c8fbe", "metadata": {"tags": ["learner", "md", "lect_03"]}, "source": ["<h3>Combining measurements with constraints from the world</h3>\n", "\n", "Now, let's say we want to combine this measurement with another measurement. The simplest way to imagine this is that we are minimizing our fit with an additional bin, which is the likelihood that our measurement has deviated from the world average. Our likelihood now will be the product of the probabilities of the best fit parameters, with the new results from NO$\\nu$A.  We can write this as\n", "\n", "\n", "$$\n", "\\begin{equation}\n", "2 \\log\\left(\\mathcal{L}(x|\\vec{\\theta})\\right) = 2 \\log\\left(\\mathcal{L}(x|\\vec{\\theta})\\right)_{\\rm original} + \n", "\\frac{\\left(\\sin \\theta_{23} - \\sin \\theta_{23}^{\\rm best}\\right)^{2}}{\\sigma^{2}_{\\sin \\theta_{23}}} + \\frac{\\left(\\Delta m^{2}_{23} - \\Delta m^{2~\\rm{best}}_{23}\\right)^{2}} {\\sigma^{2~\\rm{best}}_{\\Delta m^{2}_{12}}}\n", "\\end{equation}\n", "$$\n", "\n", "This is just equivalent to multiplying the p-values of our fit with a gaussian about the best fit parameters.\n", "\n", "<a href=\"https://pdg.lbl.gov/2020/listings/rpp2020-list-neutrino-mixing.pdf\" target=\"_blank\">Here</a> is a link to the world's average measurements of these parameters, used in the code below."]}, {"cell_type": "code", "execution_count": null, "id": "78ad2eca", "metadata": {"tags": ["learner", "py", "lect_03", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L8.3-runcell05\n", "\n", "#Now what if we try to add the world's measurement of these parameters into our fit\n", "#https://pdg.lbl.gov/2020/listings/rpp2020-list-neutrino-mixing.pdf\n", "def twoLogLike(var,iX=x,iY=y,iWeights=weights):\n", "    lTot=0\n", "    xtest=func(iX,var[1],var[0])\n", "    lTot = weights*(iY-xtest)\n", "    lTot = np.sum(lTot**2)\n", "    sin2worldavg=0.547\n", "    sin2uncavg=0.021\n", "    constraintsin2=((var[0]-sin2worldavg)**2)/(sin2uncavg**2)\n", "    deltamworldavg=2.453\n", "    deltamuncavg=0.034\n", "    constraintdeltam=((var[1]-deltamworldavg)**2)/(deltamuncavg**2)\n", "    return lTot+constraintsin2+constraintdeltam\n", "\n", "x0 = np.array([1,1])\n", "sol=opt.minimize(twoLogLike, x0)\n", "plotScan(sol)\n", "scanAxes(sol)"]}, {"cell_type": "markdown", "id": "9049b24b", "metadata": {"tags": ["learner", "md", "lect_03"]}, "source": ["Now you can see that the degneracy is resolved and the best fit parameters are tightly constrained to a very specific set of parameters. Now, we can go ahead and zoom in our best fit set of parameters. "]}, {"cell_type": "code", "execution_count": null, "id": "23420b84", "metadata": {"tags": ["learner", "py", "lect_03", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L8.3-runcell06\n", "\n", "def scanAxes(sol):\n", "    xscan = np.linspace(sol.x[0]*0.9,sol.x[0]*1.2, 100)\n", "    yscan = np.linspace(sol.x[1]*0.9,sol.x[1]*1.1, 100)\n", "\n", "    xLog = np.array([])\n", "    for pX in xscan:\n", "        xLog = np.append(xLog,twoLogLike(var=[pX,sol.x[1]]))\n", "\n", "    yLog = np.array([])\n", "    for pY in yscan:\n", "        yLog = np.append(yLog,twoLogLike(var=[sol.x[0],pY]))\n", "\n", "    plt.plot(xscan, xLog,label='loglike');\n", "    plt.axhline(sol.fun+1, c='red')\n", "    plt.xlabel(\"$\\sin^{2}\\Theta_{23}$\")\n", "    plt.ylabel(\"2$\\Delta$LL\")\n", "    plt.show()\n", "\n", "    #Now for the other parameter\n", "    plt.plot(yscan,yLog,label='LL');\n", "    plt.axhline(sol.fun+1, c='red')\n", "    plt.xlabel(\"$\\Delta m^{2}_{23}$\")\n", "    plt.ylabel(\"2$\\Delta$LL\")\n", "    plt.show()\n", "    \n", "scanAxes(sol)"]}, {"cell_type": "markdown", "id": "21acadc6", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='exercises_8_3'></a>     \n", "\n", "| [Top](#section_8_0) | [Restart Section](#section_8_3) | [Next Section](#section_8_4) |\n"]}, {"cell_type": "markdown", "id": "7948684e", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-8.3.1</span>\n", "\n", "Looking at the world's best fit values, does the NOvA data improve the best fit and, if so, by how much? Does this improvement make sense? Choose from the options below (optionally draft a computational solution):\n", "\n", "- The uncertainty gets better in m23 and sin2theta23. \n", "- The uncertainty gets better in m23, but worse in sin2theta23. \n", "- The uncertainty gets worse in m23, but better in sin2theta23. \n", "- The uncertainty gets worse in m23 and sin2theta23. "]}, {"cell_type": "code", "execution_count": null, "id": "a2651a68", "metadata": {"tags": ["draft", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>EXERCISE: L8.3.1\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "pass\n"]}, {"cell_type": "markdown", "id": "94c73eca", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_8_4'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L8.4 Principal Component Analysis</h2>     \n", "\n", "| [Top](#section_8_0) | [Previous Section](#section_8_3) | [Exercises](#exercises_8_4) |\n"]}, {"cell_type": "markdown", "id": "6f323cf8", "metadata": {"tags": ["learner", "md", "8S50x"]}, "source": ["*The material in this section is discussed in the video **<a href=\"https://courses.mitxonline.mit.edu/learn/course/course-v1:MITxT+8.S50.1x+3T2022/block-v1:MITxT+8.S50.1x+3T2022+type@sequential+block@seq_LS8/block-v1:MITxT+8.S50.1x+3T2022+type@vertical+block@vert_LS8_vid4\" target=\"_blank\">HERE</a>.** You are encouraged to watch that video and use this notebook concurrently.*"]}, {"cell_type": "markdown", "id": "818dc421", "metadata": {"tags": ["learner", "md", "lect_04"]}, "source": ["<h3>Overview</h3>\n", "\n", "Finally, I would like to say that this method of finding the ellipse is our first deep learning method.\n", "This procedure of computing the covariance matrix, and finding the eigenvectors is known as  principal component analysis or PCA. Let's run it on our example and look from <a href=\"https://jakevdp.github.io/PythonDataScienceHandbook/05.09-principal-component-analysis.html\" target=\"_blank\">Python Data Science Handbook by Jake VanderPlas</a>.\n", "\n", "First, what we can do is look at our old correlated fit. All this will do is get the eigenvectors and values for our 2D plot."]}, {"cell_type": "code", "execution_count": null, "id": "bdd59fa6", "metadata": {"tags": ["learner", "py", "lect_04", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L8.4-runcell01\n", "from sklearn.decomposition import PCA\n", "import numpy.linalg as la\n", "#make some toy data\n", "lAs = np.random.normal(0,1,1000)\n", "lBs = np.random.normal(0,1,1000)+0.5*lAs\n", "cov = np.cov([lAs,lBs])\n", "#eigen cov\n", "w, v=la.eig(cov)\n", "\n", "\n", "plt.plot(lAs,lBs,\".\")\n", "plt.xlabel(\"x\")\n", "plt.ylabel(\"y\")\n", "plt.show()\n", "\n", "X=(np.vstack([lAs,lBs])).T\n", "pca = PCA(n_components=2)\n", "pca.fit(X)\n", "print(\"PCA vectors\")\n", "print(pca.components_)\n", "print(\"PCA values\")\n", "print(pca.explained_variance_)\n", "print(\"Old Eigen\",\"vectors\",w,\"values\",v)\n", "\n", "def draw_vector(v0, v1, ax=None):\n", "    ax = ax or plt.gca()\n", "    arrowprops=dict(arrowstyle='->',linewidth=2,shrinkA=0, shrinkB=0)\n", "    ax.annotate('', v1, v0, arrowprops=arrowprops)\n", "\n", "# plot data\n", "plt.scatter(lAs, lBs, alpha=0.2)\n", "for length, vector in zip(pca.explained_variance_, pca.components_):\n", "    v = vector * 3 * np.sqrt(length)\n", "    draw_vector(pca.mean_, pca.mean_ + v)\n", "plt.axis('equal');"]}, {"cell_type": "markdown", "id": "3fac5e41", "metadata": {"tags": ["learner", "md", "lect_04"]}, "source": ["Finally, we can try this on an ML dataset. Let's take images with many pixels and treat each pixel as a separate dimension. We can then run the decomposition on the image by decomposing the n-pixel by n-pixel correlation matrix. What we will do is take an image that is 62x47=2914 Pixels. From that we can compute the covariance matrix and we can start to diagonlize this. Since this is a large matrix (2914x2914), we have to use sophisticated eigen decomposition strategies. In this case, we will use something called RandomizedPCA. \n", "\n", "What we are going to do is run PCA on the images, and take the top 200 eigenvectors from that. Then we are going to plot the top few of these eigen vectors and the cumulative explained variance, which is a metric for how much information can be gained by adding dimensions. "]}, {"cell_type": "code", "execution_count": null, "id": "05851adf", "metadata": {"tags": ["learner", "py", "lect_04", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L8.4-runcell02\n", "\n", "#Now let's do it ML style for fun\n", "#Load some images of faces\n", "from sklearn.datasets import fetch_lfw_people\n", "faces = fetch_lfw_people(min_faces_per_person=60)\n", "\n", "fig, axes = plt.subplots(3, 8, figsize=(9, 4),subplot_kw={'xticks':[], 'yticks':[]},gridspec_kw=dict(hspace=0.1, wspace=0.1))\n", "#Let's plot the eigenvectors\n", "for i, ax in enumerate(axes.flat):\n", "    ax.imshow(faces.data[i].reshape(62, 47), cmap='bone')\n", "    \n", "#Fit them to PCA \n", "from sklearn.decomposition import PCA as RandomizedPCA\n", "pca = RandomizedPCA(200)\n", "pca.fit(faces.data)\n", "fig, axes = plt.subplots(3, 8, figsize=(9, 4),subplot_kw={'xticks':[], 'yticks':[]},gridspec_kw=dict(hspace=0.1, wspace=0.1))\n", "#Let's plot the eigenvectors\n", "for i, ax in enumerate(axes.flat):\n", "    ax.imshow(pca.components_[i].reshape(62, 47), cmap='bone')\n", "plt.show()\n", "    \n", "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n", "plt.xlabel('number of components')\n", "plt.ylabel('cumulative explained variance');\n", "plt.show()\n"]}, {"cell_type": "markdown", "id": "dcb0f6fe", "metadata": {"tags": ["learner", "md", "lect_04"]}, "source": ["What you can see above is that the top eigen vectors capture the shape of the face. You can see the shape variations between the different eigenvectors as you go through. You can see also that about 80% of the total info in the first 200 eigenvectors is captured in the first 25 components. \n", "\n", "Finally, what we can do is plot our world leaders just by taking the first 80 eigenvectors of our sample. What we have effectively done is compress our original image into just 80 values, thats all!"]}, {"cell_type": "code", "execution_count": null, "id": "cfb7d1e7", "metadata": {"tags": ["learner", "py", "lect_04", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L8.4-runcell03\n", "\n", "# Compute the components and projected faces\n", "pca = RandomizedPCA(80).fit(faces.data)\n", "components = pca.transform(faces.data)\n", "projected = pca.inverse_transform(components)\n", "\n", "# Plot the results\n", "fig, ax = plt.subplots(2, 10, figsize=(10, 2.5),subplot_kw={'xticks':[], 'yticks':[]},gridspec_kw=dict(hspace=0.1, wspace=0.1))\n", "for i in range(10):\n", "    ax[0, i].imshow(faces.data[i].reshape(62, 47), cmap='binary_r')\n", "    ax[1, i].imshow(projected[i].reshape(62, 47), cmap='binary_r')\n", "    \n", "ax[0, 0].set_ylabel('full-dim\\ninput')\n", "ax[1, 0].set_ylabel('80-dim\\nreconstruction');\n"]}, {"cell_type": "markdown", "id": "bb371895", "metadata": {"tags": ["learner", "md", "lect_04"]}, "source": ["<h3>Exercise using stellar spectra</h3>\n", "\n", "As a final exercise, we are going to isolate features from stellar spectra of the Sloan Digital Sky Survey. To do that, let's first load data from the SDSS and try to understand what this looks like. We will use the `astroML` package, that allows us to pull astro data quickly. \n", "\n", "From this data, we will look at redshift-corrected galaxy spectra. The correction allows us to look at the large population of galaxies in this dataset and try to find the dominant features. Let's take a quick look at the data. "]}, {"cell_type": "code", "execution_count": null, "id": "83deda61", "metadata": {"tags": ["learner", "py", "lect_04", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L8.4-runcell04\n", "\n", "from astroML.datasets import sdss_corrected_spectra\n", "\n", "data = sdss_corrected_spectra.fetch_sdss_corrected_spectra()\n", "wavelengths = sdss_corrected_spectra.compute_wavelengths(data)\n", "spectra_raw = data['spectra']\n", "count=-1\n", "#let's plot this guy\n", "fig, ax = plt.subplots(2, 10, figsize=(20, 5.5),subplot_kw={'yticks':[]},gridspec_kw=dict(hspace=0.1, wspace=0.1))\n", "nrows = 2; ncols = 10\n", "for i in range(ncols):\n", "    for j in range(nrows):\n", "        count=count+1\n", "        ax[j,i].plot(wavelengths,spectra_raw[count], '-k', lw=1)\n", "        ax[j,i].set_xlim(3100, 7999)\n", "        if j < nrows - 1:\n", "            ax[j,i].xaxis.set_major_formatter(plt.NullFormatter())\n", "        else:\n", "            ax[j,i].set_xlabel(r'wavelength $(\\AA)$')\n", "            \n", "plt.show()\n"]}, {"cell_type": "markdown", "id": "fd2abffb", "metadata": {"tags": ["learner", "md", "lect_04"]}, "source": ["What you see is that the spectra vary, but there are a few lines from the galaxies that are particularly bright. Look at the one above 6000 Angstroms. That's the Hydrogen, $\\beta$ emission line. You can see the whole table of lines <a href=\"http://astronomy.nmsu.edu/drewski/tableofemissionlines.html\" target=\"_blank\">here</a>.\n", "\n", "In the following problem, we will look closely at the dominant eigenvector and identify any spectral lines that it might include."]}, {"cell_type": "markdown", "id": "8039b9bd", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='exercises_8_4'></a>   \n", "\n", "| [Top](#section_8_0) | [Restart Section](#section_8_4) |\n"]}, {"cell_type": "markdown", "id": "290a15f0", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-8.4.1</span>\n", "\n", "Your task is to run PCA, look at the dominant eigenvector, and find the top two spectral lines. What do they correspond to (refer to the table <a href=\"http://astronomy.nmsu.edu/drewski/tableofemissionlines.html\" target=\"_blank\">here</a>.)?\n", "\n", "Select two spectral lines from the options below (in units of angstroms):\n", "\n", "- He ~4200\n", "- H$\\gamma$ ~4350\n", "- H$\\beta$ ~4860\n", "- O III ~4950\n", "- O III ~5006\n", "- H$\\alpha$ ~6560\n", "- N II ~6580\n", "- O I ~7000\n", "\n", "Does this make sense?\n"]}, {"cell_type": "code", "execution_count": null, "id": "9f135894", "metadata": {"tags": ["draft", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>EXERCISE: L8.4.1\n", "\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "from astroML.datasets import sdss_corrected_spectra\n", "\n", "data = sdss_corrected_spectra.fetch_sdss_corrected_spectra()\n", "wavelengths = sdss_corrected_spectra.compute_wavelengths(data)\n", "spectra_raw = data['spectra']\n", "\n", "#select only the first 850 bins to avoid noise features\n", "spectra_raw = spectra_raw[:,0:850]\n", "wavelengths = wavelengths[0:850]\n", "pca = RandomizedPCA()\n", "\n", "\n", "#YOUR CODE HERE\n", "#fit the pca to the spectra_raw data \n", "#plot the first eigenvector, pca.components_[0] over the full range of wavelengths\n", "#optionally constrain the wavelength range to zoom in on spectral lines"]}, {"cell_type": "markdown", "id": "1f2fa5ae", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": [">#### Follow-up 8.4.1a (ungraded)\n", ">\n", ">Plot all of the eigenvectors. What are the dominant features of each?"]}], "metadata": {"celltoolbar": "Tags", "kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}