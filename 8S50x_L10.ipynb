{"cells": [{"cell_type": "markdown", "id": "648c5691", "metadata": {"id": "648c5691", "tags": ["learner", "md", "learner_chopped"]}, "source": ["<hr style=\"height: 1px;\">\n", "<i>This notebook was authored by the 8.S50x Course Team, Copyright 2022 MIT All Rights Reserved.</i>\n", "<hr style=\"height: 1px;\">\n", "<br>\n", "\n", "<h1>Lesson 10: Bayesian Statistics and Likelihood</h1>\n"]}, {"cell_type": "markdown", "id": "11639ba2", "metadata": {"id": "11639ba2", "tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_10_0'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L10.0 Overview</h2>\n"]}, {"cell_type": "markdown", "id": "02859d6d", "metadata": {"id": "02859d6d", "tags": ["learner", "md", "learner_chopped"]}, "source": ["<h3>Navigation</h3>\n", "\n", "<table style=\"width:100%\">\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_10_1\">L10.1 Definition of Convolution</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_10_1\">L10.1 Exercises</a></td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_10_2\">L10.2 Example of Convolutions with Different Functions</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_10_2\">L10.2 Exercises</a></td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_10_3\">L10.3 Prior and Posterior Probabilities and Bayes Theorem</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_10_3\">L10.3 Exercises</a></td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_10_4\">L10.4 Bayesian vs. Frequentist and Likelihood</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_10_4\">L10.4 Exercises</a></td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_10_5\">L10.5 Bayesian vs. Frequentist Fitting Example</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_10_5\">L10.5 Exercises</a></td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_10_6\">L10.6 Maximum Likelihood</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_10_6\">L10.6 Exercises</a></td>\n", "    </tr>\n", "</table>\n", "\n"]}, {"cell_type": "markdown", "id": "7f1e40d3", "metadata": {"id": "7f1e40d3", "tags": ["learner", "catsoop_00", "md"]}, "source": ["<h3>Learning Objectives</h3>\n", "\n", "This lesson covers several fundamental concepts in probability theory, including convolution, prior and posterior probabilities, Bayes theorem, Bayesian vs. frequentist approaches to statistics, and maximum likelihood.\n", "\n", "Convolution is defined as the integration of two functions to produce a third function which describes what happens when the two functions combine to produce a modified output. This lesson presents examples of convolutions using different functions and explains how the convolution theorem can simplify the calculation of convolutions.\n", "\n", "We then discuss the concepts of prior and posterior probabilities and the Bayes theorem, which is a fundamental concept in Bayesian statistics. The differences between Bayesian and frequentist approaches to statistics are explained, along with the role of likelihood in Bayesian inference.\n", "\n", "Finally, we introduce the relationship between likelihood and probability."]}, {"cell_type": "markdown", "id": "10e7f86a", "metadata": {"id": "10e7f86a", "tags": ["learner", "md"]}, "source": ["<h3>Importing Libraries</h3>\n", "\n", "Before beginning, run the cell below to import the relevant libraries for this notebook.\n"]}, {"cell_type": "code", "execution_count": null, "id": "af1805bb", "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "af1805bb", "outputId": "0d16a67c-ccfe-41b3-ab9e-1a5a490167b6", "tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L10.0-runcell00\n", "\n", "#install lmfit if you have not done so\n", "!pip install lmfit"]}, {"cell_type": "code", "execution_count": null, "id": "7c34dd39", "metadata": {"id": "7c34dd39", "tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L10.0-runcell01\n", "\n", "import numpy as np                #https://numpy.org/doc/stable/ \n", "import lmfit                      #https://lmfit.github.io/lmfit-py/ \n", "import matplotlib.pyplot as plt   #https://matplotlib.org/3.5.3/api/_as_gen/matplotlib.pyplot.html\n", "from scipy import stats           #https://docs.scipy.org/doc/scipy/reference/stats.html"]}, {"cell_type": "markdown", "id": "12f94e87", "metadata": {"id": "12f94e87", "tags": ["learner", "md"]}, "source": ["<h3>Setting Default Figure Parameters</h3>\n", "\n", "The following code cell sets default values for figure parameters.\n"]}, {"cell_type": "code", "execution_count": null, "id": "0afd3a79", "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 240}, "id": "0afd3a79", "outputId": "698acf56-fff9-432e-c4c2-92ef76759f3b", "tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L10.0-runcell02\n", "\n", "#set plot resolution\n", "%config InlineBackend.figure_format = 'retina'\n", "\n", "#set default figure parameters\n", "plt.rcParams['figure.figsize'] = (9,6)\n", "\n", "medium_size = 12\n", "large_size = 15\n", "\n", "plt.rc('font', size=medium_size)          # default text sizes\n", "plt.rc('xtick', labelsize=medium_size)    # xtick labels\n", "plt.rc('ytick', labelsize=medium_size)    # ytick labels\n", "plt.rc('legend', fontsize=medium_size)    # legend\n", "plt.rc('axes', titlesize=large_size)      # axes title\n", "plt.rc('axes', labelsize=large_size)      # x and y labels\n", "plt.rc('figure', titlesize=large_size)    # figure title\n"]}, {"cell_type": "markdown", "id": "0856101b", "metadata": {"id": "0856101b", "tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_10_1'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L10.1 Definition of Convolution</h2>  \n", "\n", "| [Top](#section_10_0) | [Previous Section](#section_10_0) | [Exercises](#exercises_10_1) | [Next Section](#section_10_2) |\n"]}, {"cell_type": "markdown", "id": "aca067bd", "metadata": {"id": "aca067bd", "tags": ["learner", "md"]}, "source": ["*The material in this section is discussed in the video **<a href=\"https://courses.mitxonline.mit.edu/learn/course/course-v1:MITxT+8.S50.2x+1T2025/block-v1:MITxT+8.S50.2x+1T2025+type@sequential+block@seq_LS10/block-v1:MITxT+8.S50.2x+1T2025+type@vertical+block@vert_LS10_vid1\" target=\"_blank\">HERE</a>.** You are encouraged to watch that video and use this notebook concurrently.*"]}, {"cell_type": "markdown", "id": "3ed08e30", "metadata": {"id": "3ed08e30", "tags": ["learner", "md", "lect_01"]}, "source": ["<h3>Overview</h3>\n", "\n", "Convolutions are a critical component of every good statistical analysis. They are a way to merge distributions together to determine what happens when there are two or more different effects combining to give a final result. Let's build convolutions up by scratch and then go from there.  \n", "\n", "The core concept of a convolution is that you are effectively multiplying distributions. Given two functions $f(x)$ and $g(x)$, we can define their convolution by:\n", "\n", "\n", "$$\n", "\\begin{eqnarray}\n", "(f*g)(z) &=& \\int^{\\infty}_{-\\infty} f(z-t)g(t)dt\n", "\\end{eqnarray}\n", "$$\n", "\n", "For data analysis, we usually think about this in the context of probability distributions $g$ and $f$. From these two, we construct a new probability distribution $(f*g)$. Let's take a look at how it works. \n", "\n", "To do this, let's first define some functions to convolve. "]}, {"cell_type": "code", "execution_count": null, "id": "a82359e2", "metadata": {"id": "a82359e2", "tags": ["learner", "py", "lect_01", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L10.1-runcell01\n", "\n", "#First let's define a triangular distribution\n", "def triangle(x,mean=5):\n", "    Norm=mean*mean\n", "    val=np.where(x <= mean,np.maximum(x,np.zeros(len(x))), np.maximum(2*mean-x,np.zeros(len(x))))\n", "    return val/Norm\n", "\n", "#Now define the gaussian\n", "def gaussian(x,mean=0,sigma=1):\n", "    return 1./(sigma * np.sqrt(2 * np.pi)) * np.exp( - (x - mean)**2 / (2 * sigma**2)) \n"]}, {"cell_type": "code", "execution_count": null, "id": "a70e216d", "metadata": {"id": "a70e216d", "tags": ["learner", "py", "lect_01", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L10.1-runcell02\n", "\n", "#Now let's do a convolution by hand\n", "def convolve(f1,f2,x,sigma=1,iMin=-10,iMax=10,iN=2000):\n", "    step=(iMax-iMin)/iN\n", "    pInt=0\n", "    for i0 in range(iN):\n", "            pX   = np.repeat(i0*step+iMin,len(x))\n", "            pVal = f1(x-pX,sigma=sigma)*f2(pX)\n", "            pInt += pVal*step\n", "    return pInt\n", "\n", "#You could consider how the choice of iMin, iMax, and iN affect the output"]}, {"cell_type": "markdown", "id": "b87770a6", "metadata": {"id": "3ed08e30", "tags": ["learner", "md", "lect_01"]}, "source": ["The plots generated with the code below show convolutions of the same triangular function with two different Gaussian functions, one of which has a width that is 5 times larger than the other one. Only the narrower Gaussian is shown in the figure.\n", "\n", "As you can see, the convolution with the narrower Gaussian makes only minor modifications to the triangular shape, while the convolution with a wide Gaussian shows little evidence of any influence from the triangular component."]}, {"cell_type": "code", "execution_count": null, "id": "5e8e08b8", "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 294}, "id": "5e8e08b8", "outputId": "96d63fc8-1b97-4c30-be4f-2a19277a42c9", "tags": ["learner", "py", "lect_01", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L10.1-runcell03\n", "\n", "#now let's plot\n", "fig, ax = plt.subplots()\n", "x_in=np.linspace(-10, 15, 100)\n", "tri_out=triangle(x_in)\n", "gaus_out=gaussian(x_in)\n", "conv1_out=convolve(gaussian,triangle,x_in)\n", "conv2_out=convolve(gaussian,triangle,x_in,sigma=5)\n", "\n", "ax.plot(x_in,tri_out,label='triangle')\n", "ax.plot(x_in,gaus_out,label='gaussian')\n", "ax.plot(x_in,conv1_out,label='convolved')\n", "ax.plot(x_in,conv2_out,label='convolved($\\sigma=5$)')\n", "ax.set(xlabel='x(t)', ylabel='y(t)',title='Convolutions')\n", "ax.grid()\n", "plt.legend()\n", "plt.show()"]}, {"cell_type": "markdown", "id": "bcaee76a", "metadata": {"id": "3ed08e30", "tags": ["learner", "md", "lect_01"]}, "source": ["Now, let's compare the original wide Gaussian with the convolution. Adding the triangle does make a difference, but the overall shape is clearly dominated by the wide Gaussian. \n", "\n", "These two examples show how the influence of the different components used in a convolution can vary depending on their shapes, especially their relative widths."]}, {"cell_type": "code", "execution_count": null, "id": "1d9bf38b", "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 294}, "id": "5e8e08b8", "outputId": "96d63fc8-1b97-4c30-be4f-2a19277a42c9", "tags": ["learner", "py", "lect_01", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L10.1-runcell04\n", "\n", "#now let's plot the convolved and original wide Gaussian\n", "fig, ax = plt.subplots()\n", "x_in=np.linspace(-10, 15, 100)\n", "tri_out=triangle(x_in)\n", "gaus_out=gaussian(x_in,5,5)\n", "conv2_out=convolve(gaussian,triangle,x_in,sigma=5)\n", "\n", "ax.plot(x_in,tri_out,label='triangle')\n", "ax.plot(x_in,gaus_out,label='gaussian')\n", "ax.plot(x_in,conv2_out,label='convolved($\\sigma=5$)')\n", "ax.set(xlabel='x(t)', ylabel='y(t)',title='Convolutions')\n", "ax.grid()\n", "plt.legend()\n", "plt.show()"]}, {"cell_type": "markdown", "id": "a766a4c3", "metadata": {"id": "a766a4c3", "tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='exercises_10_1'></a>     \n", "\n", "| [Top](#section_10_0) | [Restart Section](#section_10_1) | [Next Section](#section_10_2) |\n"]}, {"cell_type": "markdown", "id": "52cd47a6", "metadata": {"id": "52cd47a6", "tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-10.1.1</span>\n", "\n", "What happens if you convolve a Gaussian with a straight line: $y=x$? Complete the code below to define convolutions for two different values of sigma, using the previously defined functions 'gaussian' and 'convolve'. Then plot in your notebook.\n", "\n", "Afterwards, consider the following: What happens to the value of the convolved function at $x=0$? What happens if you increase the width of the Gaussian?"]}, {"cell_type": "code", "execution_count": null, "id": "f2b5bf2c", "metadata": {"id": "f2b5bf2c", "outputId": "86daa2d9-a5e4-4d00-b863-acb25d85077d", "tags": ["draft", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>EXERCISE: L10.1.1\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "#defining a linear function\n", "def line(x):\n", "    #YOUR CODE HERE\n", "    return\n", "\n", "#creating the convolution\n", "x_in=np.linspace(-10, 15, 100) #MUST USE THIS INPUT ARRAY FOR ANSWER CHECKER\n", "conv1_out = pass #YOUR CODE HERE, use a gaussian with default mean and sigma\n", "conv2_out = pass #YOUR CODE HERE, use a gaussian with default mean and sigma=5\n", "\n", "\n", "#creating plots and comparing to the original functions\n", "fig, ax = plt.subplots()\n", "\n", "line_out=line(x_in)\n", "gaus1_out=gaussian(x_in)\n", "\n", "ax.plot(x_in,line_out,label='line')\n", "ax.plot(x_in,gaus1_out,label='gaussian')\n", "ax.plot(x_in,conv1_out,label='convolved')\n", "ax.plot(x_in,conv2_out,label='convolved($\\sigma=5$)')\n", "ax.set(xlabel='x(t)', ylabel='y(t)',title='Convolutions')\n", "ax.grid()\n", "plt.legend()\n", "plt.show()"]}, {"cell_type": "markdown", "id": "-m6w2k5PPKoP", "metadata": {"id": "-m6w2k5PPKoP", "tags": ["md", "learner", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-10.1.2</span>\n", "\n", "What happens if you convolve a Gaussian with a Gaussian? Complete the code below to define convolutions for two different values of sigma, again using the previously defined functions 'gaussian' and 'convolve'. Then plot in your notebook.\n", "\n", "As an extra, consider the following questions:\n", "\n", "- What is the functional form of the convolution?\n", "- What happens to the value of the convolved function at $x=0$?\n", "- What happens if you change the width of one or both of the Gaussians?"]}, {"cell_type": "code", "execution_count": null, "id": "55XJ7UfqPJhe", "metadata": {"id": "55XJ7UfqPJhe", "tags": ["py", "draft", "learner_chopped"]}, "outputs": [], "source": ["#>>>EXERCISE: L10.1.2\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "#creating the convolution\n", "x_in=np.linspace(-10, 15, 100) #MUST USE THIS INPUT ARRAY FOR ANSWER CHECKER\n", "conv1_out = pass #YOUR CODE HERE, use a gaussian with default mean and sigma\n", "conv2_out = pass #YOUR CODE HERE, use a gaussian with default mean and sigma=5\n", "\n", "\n", "#creating plots and comparing to the original functions\n", "fig, ax = plt.subplots()\n", "\n", "gaus1_out=gaussian(x_in)\n", "\n", "ax.plot(x_in,gaus1_out,label='gaussian')\n", "ax.plot(x_in,conv1_out,label='convolved')\n", "ax.plot(x_in,conv2_out,label='convolved($\\sigma=5$)')\n", "ax.set(xlabel='x(t)', ylabel='y(t)',title='Convolutions')\n", "ax.grid()\n", "plt.legend()\n", "plt.show()"]}, {"cell_type": "markdown", "id": "a2fe3a84", "metadata": {"id": "a2fe3a84", "tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_10_2'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L10.2 Example of Convolutions with Different Functions</h2>  \n", "\n", "| [Top](#section_10_0) | [Previous Section](#section_10_1) | [Exercises](#exercises_10_2) | [Next Section](#section_10_3) |\n"]}, {"cell_type": "markdown", "id": "eb292ba6", "metadata": {"id": "eb292ba6", "tags": ["learner", "md"]}, "source": ["*The material in this section is discussed in the video **<a href=\"https://courses.mitxonline.mit.edu/learn/course/course-v1:MITxT+8.S50.2x+1T2025/block-v1:MITxT+8.S50.2x+1T2025+type@sequential+block@seq_LS10/block-v1:MITxT+8.S50.2x+1T2025+type@vertical+block@vert_LS10_vid2\" target=\"_blank\">HERE</a>.** You are encouraged to watch that video and use this notebook concurrently.*"]}, {"cell_type": "markdown", "id": "33c37f93", "metadata": {"id": "33c37f93", "tags": ["learner", "md", "lect_02"]}, "source": ["### Challenge question (worked example)\n", "\n", "What does $f(x)=\\sin(x)$ convolved with a Gaussian look like? How about $f(x)=\\sin(x)x$? How do they change with the width of the Gaussian?"]}, {"cell_type": "code", "execution_count": null, "id": "88ec3946", "metadata": {"id": "88ec3946", "outputId": "beebc0f8-b24a-456f-a5b3-3f497b4e794c", "tags": ["learner", "py", "lect_02", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L10.2-runcell01\n", "\n", "def func_1(x):\n", "    return np.sin(x)\n", "\n", "#this plots functions with two gaussians of different width\n", "def plot_convolutions_with_gaussian(func_in,x,func_name):\n", "    func_out=func_in(x)\n", "    gaus_out=gaussian(x)\n", "    conv1_out=convolve(gaussian,func_in,x)\n", "    conv2_out=convolve(gaussian,func_in,x,sigma=2)\n", "\n", "    fig, ax = plt.subplots()\n", "    ax.plot(x,func_out,label=func_name)\n", "    ax.plot(x,gaus_out,label='gaussian')\n", "    ax.plot(x,conv1_out,label='convolved')\n", "    ax.plot(x,conv2_out,label='convolved($\\sigma=2$)')\n", "    ax.set(xlabel='x(t)', ylabel='y(t)',title='Convolutions')\n", "    ax.grid()\n", "    plt.legend()\n", "    plt.show()\n", "    \n", "    \n", "#now let's plot\n", "x_in=np.linspace(-10, 15, 100)\n", "plot_convolutions_with_gaussian(func_1,x_in,'sin(x)')"]}, {"cell_type": "code", "execution_count": null, "id": "ed4c9fa2", "metadata": {"id": "ed4c9fa2", "outputId": "f7ef457a-27ed-4b14-ed7b-aad86dadabe3", "tags": ["learner", "py", "lect_02", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L10.2-runcell02\n", "\n", "def func_2(x):\n", "    return np.sin(x)*x\n", "\n", "\n", "#now let's plot\n", "x_in=np.linspace(-10, 15, 100)\n", "plot_convolutions_with_gaussian(func_2,x_in,'sin(x)*x')"]}, {"cell_type": "markdown", "id": "cfff427c", "metadata": {"id": "33c37f93", "tags": ["learner", "md", "lect_02"]}, "source": ["### Other Examples\n", "\n", "Here are some examples of simple mathematical convolutions, which you can try on your own:\n", "\n", "**Rectangular pulse:** The convolution of two rectangular pulses (i.e. functions that have a constant non-zero value only within a certain range in x) is a triangular pulse. This is a common example used to illustrate the concept of convolution.\n", "\n", "**Exponential function:** The convolution of an exponential function with a rectangular pulse is a function that starts at zero, rises to a maximum, and then decays exponentially. This is a common example used in signal processing.\n", "\n", "**Sine and cosine functions:** The convolution of a sine function with a cosine function is a sine function with a phase shift. This is an important result in Fourier analysis.\n", "\n", "**Gaussian function:** The convolution of a Gaussian function with itself is another Gaussian function. This is a property of the Gaussian distribution and is used in many applications, including image processing and signal smoothing."]}, {"cell_type": "markdown", "id": "e2496576", "metadata": {"id": "e2496576", "tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='exercises_10_2'></a>     \n", "\n", "| [Top](#section_10_0) | [Restart Section](#section_10_2) | [Next Section](#section_10_3) |\n"]}, {"cell_type": "markdown", "id": "e19fd655", "metadata": {"id": "e19fd655", "tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-10.2.1</span>\n", "\n", "Often we use convolutions to \"smear\" our distributions analytically, without having to build a simulation. In the code below we sample points from a Gaussian. Take the original function below (a box distribution), add points sampled by a Gaussian, and show that the distribution matches the convolution.\n", "\n", "Specifically, complete the array `smeared_x_in`, which adds `x_in` and `smeared`. Then complete the array `smeared_box_out`, which takes the average of the smeared values from `func_box` evaluated at the `smeared_x_in` points. Only complete tese arrays in the answer checker, then run the code in your notebook to generate plots.\n", "\n", "To clearly see that the distributions match, try changing which of `smeared_box_out` and `conv_out` gets plotted first, or just comment out one and then the other. The two should be identical, which means that you only see the one which is plotted second."]}, {"cell_type": "code", "execution_count": null, "id": "1d171774", "metadata": {"id": "1d171774", "outputId": "3de6dde1-7696-4736-e9b4-30def3ef4ff4", "tags": ["draft", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>EXERCISE: L10.2.1\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "np.random.seed(10)\n", "\n", "#define a box function\n", "def func_box(x):\n", "    return 0+0.1*np.where(x < -10,0,1) - 0.1*np.where(x < 10,0,1)\n", "\n", "lNToys=1000\n", "nbins=100\n", "sigma=1\n", "smeared=np.random.normal(0,sigma,(lNToys,nbins))\n", "x_in=np.linspace(-20, 20, nbins)\n", "\n", "#add the smeared signal to x_in, make 1000 example x distributions from above\n", "smeared_x_in=#YOUR CODE HERE\n", "\n", "#average over 1000 sampled distributions\n", "smeared_box_out=#YOUR CODE HERE \n", "\n", "\n", "#PLOT\n", "#-------------\n", "#now let's plot\n", "fig, ax = plt.subplots()\n", "x_in=np.linspace(-20, 20, 100)\n", "\n", "#various functions to add to the final plot\n", "box_out=func(x_in)\n", "gaus_out=gaussian(x_in)\n", "conv_out=convolve(gaussian,func,x_in)\n", "\n", "#make final plot\n", "ax.plot(x_in,box_out,label='box(x)')\n", "ax.plot(x_in,smeared_box_out,label='smeared box(x)')\n", "ax.plot(x_in,gaus_out,label='gaussian')\n", "ax.plot(x_in,conv_out,label='convolved')\n", "ax.set(xlabel='x(t)', ylabel='y(t)',title='Convolutions')\n", "ax.grid()\n", "plt.legend()\n", "plt.show()\n"]}, {"cell_type": "markdown", "id": "81192f8b", "metadata": {"id": "81192f8b", "tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_10_3'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L10.3 Prior and Posterior Probabilities and Bayes Theorem</h2>  \n", "\n", "| [Top](#section_10_0) | [Previous Section](#section_10_2) | [Exercises](#exercises_10_3) | [Next Section](#section_10_4) |\n"]}, {"cell_type": "markdown", "id": "8fd83628", "metadata": {"id": "8fd83628", "tags": ["learner", "md"]}, "source": ["*The material in this section is discussed in the video **<a href=\"https://courses.mitxonline.mit.edu/learn/course/course-v1:MITxT+8.S50.2x+1T2025/block-v1:MITxT+8.S50.2x+1T2025+type@sequential+block@seq_LS10/block-v1:MITxT+8.S50.2x+1T2025+type@vertical+block@vert_LS10_vid3\" target=\"_blank\">HERE</a>.** You are encouraged to watch that video and use this notebook concurrently.*"]}, {"cell_type": "markdown", "id": "99b4a9e8", "metadata": {"id": "99b4a9e8", "tags": ["learner", "md"]}, "source": ["<h3>Slides</h3>\n", "\n", "Run the code below to view the slides for this section, which are discussed in the related video. You can also open the slides in a separate window <a href=\"https://mitx-8s50.github.io/slides/L10/slides_L10_03.html\" target=\"_blank\">HERE</a>."]}, {"cell_type": "code", "execution_count": null, "id": "a7f30871", "metadata": {"id": "a7f30871", "outputId": "10c81a22-9086-4425-8d0e-e03396b40e8c", "tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L10.3-slides\n", "\n", "from IPython.display import IFrame\n", "IFrame(src='https://mitx-8s50.github.io/slides/L10/slides_L10_03.html', width=970, height=550)"]}, {"cell_type": "markdown", "id": "cfa149d2", "metadata": {"id": "cfa149d2", "tags": ["learner", "md", "lect_03"]}, "source": ["<h3>What is a measurement?</h3>\n", "\n", "Let's say we have a measurement of some parameter $x$, and this measurement behaves like a Gaussian about some mean point with width $\\sigma=1$. Let's plot this measurement (in this case, we set the mean to be 0).   "]}, {"cell_type": "code", "execution_count": null, "id": "685db9e5", "metadata": {"id": "685db9e5", "outputId": "563013b0-4daf-4ebd-99c3-68475375215f", "tags": ["learner", "py", "lect_03", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L10.3-runcell01\n", "\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "from scipy import stats\n", "\n", "#Our measurement probability\n", "def gaus(mu=0,sigma=1): \n", "    x = np.arange(-10, 10, 0.001)\n", "    y = stats.norm.pdf(x,mu,sigma)\n", "    return x,y\n", "\n", "#a quick plot of what we expect the measurement to be\n", "def plotgaus():\n", "    x,y=gaus(0,1)\n", "    fig, ax = plt.subplots(figsize=(9,6))\n", "    plt.style.use('fast')\n", "    ax.plot(x,y)\n", "    ax.fill_between(x,y,0, alpha=0.1, color='b')\n", "    ax.set_xlim([-6,6])\n", "    ax.set_xlabel('x')\n", "    ax.set_ylabel('p')\n", "    plt.show()\n", "\n", "plotgaus()"]}, {"cell_type": "markdown", "id": "6175f8d8", "metadata": {"id": "6175f8d8", "tags": ["learner", "md", "lect_03"]}, "source": ["<h4>Prior Distribution</h4>\n", "\n", "This preconceived distribution predicting how the probability of our measurement will behave is known as a *prior*. Now, let's say we perform this measurement, but our observed value is not actually at the mean where we expect it to be since the probability is highest there. What if, for example, our measurement is at $x=2$? What would be the likelihood of this occurring or not occurring?"]}, {"cell_type": "code", "execution_count": null, "id": "002c21ec", "metadata": {"id": "002c21ec", "outputId": "b0aebfca-c9d3-40fb-c802-a91d3d700ba0", "tags": ["learner", "py", "lect_03", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L10.3-runcell02\n", "\n", "def gaus(mu=0,sigma=1,meas=2): \n", "    x = np.arange(-10, 10, 0.001)\n", "    xmeas = np.arange(meas, 10, 0.001)\n", "    y = stats.norm.pdf(x,mu,sigma)\n", "    ymeas = stats.norm.pdf(xmeas,mu,sigma)\n", "    return x,y,xmeas,ymeas\n", "\n", "def plotgaus():\n", "    x,y,xmeas,ymeas=gaus(0,1,2)\n", "    fig, ax = plt.subplots(figsize=(9,6))\n", "    plt.style.use('fast')\n", "    ax.plot(x,y)\n", "    ax.fill_between(x,y,0, alpha=0.1, color='b')\n", "    ax.fill_between(xmeas,ymeas,0, alpha=0.3, color='b')\n", "    ax.set_xlim([-6,6])\n", "    ax.set_xlabel('x')\n", "    ax.set_ylabel('p')\n", "    plt.show()\n", "    \n", "plotgaus()"]}, {"cell_type": "markdown", "id": "a8f35efc", "metadata": {"id": "a8f35efc", "tags": ["learner", "md", "lect_03"]}, "source": ["From the plot generated by the code above, it is clear that this particular measurement of $x=2$ has a large deviation from what we actually expected to observe and, therefore, has a very low probability. The key question that we would like to understand here is: *Is this deviation just a statistical fluctuation, or does this indicate that there is something wrong with our expectation, i.e. our assumed prior was wrong?*\n", "\n", "**Understanding if this observation indicates that there is something wrong with what we expected is the focus of this lecture.**"]}, {"cell_type": "markdown", "id": "bfd3ce1c", "metadata": {"id": "bfd3ce1c", "tags": ["learner", "md", "lect_03"]}, "source": ["Now, imagine that we perform this measurement a number of times, say 10 times, and all the results defy our expectations. What do you think about this? What if it happened 1000 times? \n", "\n", "In the plots drawn by the code below, the *posterior* shows the distribution that was actually used to generate the sample data."]}, {"cell_type": "code", "execution_count": null, "id": "2129dc2b", "metadata": {"id": "2129dc2b", "outputId": "ff16b1bd-60ae-4262-cd6e-58e804e86f0b", "tags": ["learner", "py", "lect_03", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L10.3-runcell03\n", "\n", "np.random.seed(32)\n", "\n", "def gaus(mu=0,sigma=1): \n", "    x = np.arange(-10, 10, 0.001)\n", "    y = stats.norm.pdf(x,mu,sigma)\n", "    return x,y\n", "\n", "def plotGausSample(iZ,iSample):\n", "    plt.style.use('fast')\n", "    fig, ax = plt.subplots(figsize=(9,6))\n", "    #sample\n", "    samples = np.random.normal(iZ,1,iSample)\n", "    x,y=gaus(iZ,1)\n", "    #\n", "    xs,ys=gaus(0,1)\n", "    ax.plot(xs,ys,label='prior')\n", "    ax.plot(x,y,label='posterior')\n", "    count, bins, ignored = plt.hist(samples, 30, density=True, label=str(iSample)+' samples')\n", "    ax.fill_between(xs,ys,0, alpha=0.1)\n", "    ax.set_xlim([-6,6])\n", "    ax.set_xlabel('x')\n", "    ax.set_ylabel('p')\n", "    ax.legend()\n", "    plt.show()\n", "\n", "plotGausSample(2,10)\n", "plotGausSample(2,1000)"]}, {"cell_type": "markdown", "id": "mgJWHG7WdCuS", "metadata": {"id": "mgJWHG7WdCuS", "tags": ["learner", "md", "lect_03"]}, "source": ["So, looking at the distribution with only a few events, we can start to guess that our *prior* is wrong, although the shape of the actual distribution is not very clear.\n", "\n", "If we look at the distribution with many events, we see that there is a glaringly obvious deviation between our prior and the actual measured distribution.  Furthermore, the shape of the correct probability distribution is much clearer. With both of these samples, we are getting the hint that something is wrong, and we need to fix our expectation for the probability distribution. \n", "\n", "In this case, we can easily fix this by just moving our Gaussian to the right. In fact, we could just fit a Gaussian to the data to get the correct shape. That would resolve our issue. \n", "\n", "However, we could take a more positive spin on our observed data, and say that our prior is actually right, and we are just getting a biased sample. In reality, there are many more events, and we just took a subset. This equates to thinking about your prior as being much larger, like in the figures below."]}, {"cell_type": "code", "execution_count": null, "id": "BIR8F_BfdqIF", "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 347}, "id": "BIR8F_BfdqIF", "outputId": "aa78035d-b9ff-4aae-ebba-c152d107e8b7", "tags": ["learner", "py", "lect_03", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L10.3-runcell04\n", "\n", "np.random.seed(32)\n", "\n", "def plotGausSample(iZ,iSample):\n", "    plt.style.use('fast')\n", "    #sample\n", "    samples = np.random.normal(iZ,1,iSample)\n", "    x,y=gaus(iZ,1)\n", "    #noral st\n", "    xs,ys=gaus(0,1)\n", "    ys*=100\n", "    fig, ax = plt.subplots(figsize=(9,6))\n", "    ax.plot(xs,ys,label='prior')\n", "    count, bins, ignored = plt.hist(samples, 30, density=True, label=str(iSample)+' samples')\n", "    ax.plot(x,y,label='posterior')\n", "    ax.fill_between(xs,ys,0, alpha=0.1)\n", "    ax.set_xlim([-6,6])\n", "    ax.set_xlabel('x')\n", "    ax.set_ylabel('p')\n", "    ax.legend(loc=2)\n", "    #plt.ylim(0,0.43) #note: these bounds are used in video\n", "    plt.ylim(0,3.0) #note: these bounds more clearly shows what we are investigating\n", "    plt.show()\n", "    \n", "plotGausSample(2,10)\n", "\n", "plotGausSample(2,1000)\n"]}, {"cell_type": "markdown", "id": "1ce20615", "metadata": {"id": "34f439e6", "tags": ["learner", "md", "lect_03"]}, "source": ["<h3>Bayes Theorem</h3>\n", "\n", "Now we have two results, we have a prior (aka a guess) of what we thought the data would look like, and a posterior for what we actually observe in the data. Bayes theorem connects these results.\n", "\n", "To understand Bayes theorem, let's define a bunch of terms. First, we would like to define the probability of a hypothesis happening. We can write this probability as $P\\left(\\mathcal{H}\\right)$, the probability of the hypothesis happening given a prior for how we expect the distribution to behave. To put a concrete label to this, let's consider that you are on a game show similar to the \"Monty Hall\" game show of the 1960s. You have 3 doors and there is a car behind one of the doors. The car is placed randomly; what is your $P\\left(\\mathcal{H}\\right)$ before opening a door?\n", "\n", "\n", "$$\n", "\\begin{eqnarray}\n", "P(\\mathcal{H}=\\rm{door~1}) & = & \\frac{1}{3} \\\\\n", "P(\\mathcal{H}=\\rm{door~2}) & = & \\frac{1}{3} \\\\\n", "P(\\mathcal{H}=\\rm{door~3}) & = & \\frac{1}{3} \n", "\\end{eqnarray}\n", "$$\n", "\n", "Everything changes when one of the doors is opened. There are two results. The first result is that there is a car behind the door, great! In the second case, you know that the door that was opened is empty. "]}, {"cell_type": "markdown", "id": "8f29ec3a", "metadata": {"id": "34f439e6", "tags": ["learner", "md", "lect_03"]}, "source": ["Let's twist it up a bit. The door that you chose first is not opened right away. Instead, Monty opens one of the remaining two doors, being sure to open a door that does not have a car. He then asks you if you'd like to stay with your original choice, or switch to the other unopened door. What do you do? \n", "\n", "Let's define $P\\left(\\mathcal{D}\\right)$ as the probability of an instance of the data happening, and furthermore defining\n", "\n", "\n", "$$\n", "\\begin{eqnarray}\n", " P\\left(\\mathcal{H} | \\mathcal{D} \\right) & = & \\rm{probability~of~a~hypothesis~given~data} \\\\\n", " P\\left(\\mathcal{D} | \\mathcal{H} \\right) & = & \\rm{probability~of~data~given~a~hypothesis} \\\\\n", "\\end{eqnarray}\n", "$$\n", "\n", "Let's say you choose door 1, and Monty opens door 2. We can write down the probability that Monty will open door 2\n", "\n", "\n", "$$\n", "\\begin{eqnarray}\n", "P(\\rm{open~2}|\\mathcal{H}=\\rm{car~at~door~1}) & = & \\frac{1}{2} \\\\\n", "P(\\rm{open~2}|\\mathcal{H}=\\rm{car~at~door~2}) & = & 0 \\\\\n", "P(\\rm{open~2}|\\mathcal{H}=\\rm{car~at~door~3}) & = & 1 \n", "\\end{eqnarray}\n", "$$\n", "\n", "If you chose correctly the first time (car is, in fact, behind door 1), then Monty has a 50% chance of choosing door 2 or 3, since both doors do not contain a car. If you did not choose correctly the first time (car is, in fact, behind door 2 or 3), then Monty will for sure open a specific door, and in this scenario you know for sure that behind the other door that Monty did not choose is the car. So, given this perspective, do you switch or stay? "]}, {"cell_type": "markdown", "id": "36e479a5", "metadata": {"id": "34f439e6", "tags": ["learner", "md", "lect_03"]}, "source": ["Let's write out the probabilities again. Remember, when you start you have a $\\frac{1}{3}$ chance of choosing correctly, and a $\\frac{2}{3}$ chance of choosing incorrectly. \n", "\n", "$$\n", "\\begin{eqnarray}\n", "P(\\mathcal{H}=\\rm{car~at~door~1}|\\mathcal{D}=\\rm{open~2}) & = & \\frac{1}{3} \\\\\n", "P(\\mathcal{H}=\\rm{car~at~door~2}|\\mathcal{D}=\\rm{open~2}) & = & 0 \\\\\n", "P(\\mathcal{H}=\\rm{car~at~door~3}|\\mathcal{D}=\\rm{open~2}) & = & \\frac{2}{3}  \n", "\\end{eqnarray}\n", "$$\n", "\n", "So, unintuitively, you should switch your guess! Let's look at this more precisely:\n", "\n", "\n", "$$\n", "\\begin{equation}\n", "P(\\mathcal{H}=\\rm{car~at~door~1}|\\mathcal{D}=\\rm{open~2}) = \\frac{P(\\mathcal{D}=\\rm{open~2}|\\mathcal{H}=\\rm{car~at~door~1})P(\\mathcal{H}=\\rm{door~1})}{\\rm{all~combinations}}\n", "\\end{equation}\n", "$$\n", "\n", "or in other words\n", "\n", "$$\n", "\\begin{equation}\n", "P(\\mathcal{H}=\\rm{car~at~door~1}|\\mathcal{D}=\\rm{open~2}) = \\frac{P(\\mathcal{D}=\\rm{open~2}|\\mathcal{H}=\\rm{car~at~door~1})P(\\mathcal{H}=\\rm{door~1})}{P\\left(\\mathcal{D}=\\rm{car~at~door~1}\\right)+P\\left(\\mathcal{D}=\\rm{car~at~door~2}\\right)+P\\left(\\mathcal{D}=\\rm{car~at~door~3}\\right)}\n", "\\end{equation}\n", "$$\n", "\n", "More generically, we can write this as what is known as Bayes theorem\n", "\n", "$$\n", "\\begin{equation}\n", " P\\left(\\mathcal{H} | \\mathcal{D} \\right) =  \\frac{P\\left(\\mathcal{D} | \\mathcal{H} \\right)P(\\mathcal{H})}{P(\\mathcal{D})} \n", "\\end{equation}\n", "$$\n", "\n", "We can also relabel these terms as: \n", "\n", "$$\n", "\\begin{eqnarray}\n", " P\\left(\\mathcal{H} | \\mathcal{D} \\right) & = & \\rm{Posterior} \\\\\n", " P\\left(\\mathcal{D} | \\mathcal{H} \\right) & = & \\rm{Likelihood} \\\\\n", " P\\left(\\mathcal{H} \\right) & = & \\rm{Prior} \\\\\n", " P\\left(\\mathcal{D} \\right) & = & \\rm{Normalizer(all~possibilities)} \n", "\\end{eqnarray} \n", "$$\n", "\n", "The Posterior is our observed result, the Prior is our initial guess, the likelihood is what we actually observe, and finally the bottom term, the normalizer, is to ensure that our probabilities integrate to $1$ (aka, we have covered, and only covered, all possibilities).  As a general rule of thumb, the way to remember this is\n", "\n", "$$\n", "\\begin{equation}\n", " \\rm{posterior} \\propto \\rm{likelihood} \\times \\rm{prior}\n", "\\end{equation}\n", "$$"]}, {"cell_type": "markdown", "id": "c9db157d", "metadata": {"id": "c9db157d", "tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='exercises_10_3'></a>     \n", "\n", "| [Top](#section_10_0) | [Restart Section](#section_10_3) | [Next Section](#section_10_4) |\n"]}, {"cell_type": "markdown", "id": "d3887ca5", "metadata": {"id": "d3887ca5", "tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-10.3.1</span>\n", "\n", "If you roll a normal die and observe the outcome 6 three times in a row, are you surprised? What is the prior in this case? Enter the value for your prior as a fraction with precision 1e-4.\n", "\n", "Hint: You could use `stats.binom.pmf` to compute this value."]}, {"cell_type": "code", "execution_count": null, "id": "2xizPFcQZFD3", "metadata": {"id": "2xizPFcQZFD3", "tags": ["py", "draft", "learner_chopped"]}, "outputs": [], "source": ["#>>>EXERCISE: L10.3.1\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n"]}, {"cell_type": "markdown", "id": "aboU7ebvf8D_", "metadata": {"id": "aboU7ebvf8D_", "tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-10.3.2</span>\n", "\n", "Let's say your prior is a Gaussian with mean 0 and width 1, but your true, posterior distribution is a Gaussian with mean 0.5 and width 1. Generate 1000 events using this posterior distribution and find the most extreme, positive value (farthest from 0). What is the p-value of this extreme value (i.e., the probability of observing this extreme value or higher), given your prior? Enter your answer as a number with precision 1e-6.\n", "\n", "Given that you have 1000 samples from this posterior distribution, how does this probability compare with what you would expect? In other words, is it likely to have occured once in 1000 samples?"]}, {"cell_type": "code", "execution_count": null, "id": "2-Ff2OQef8D_", "metadata": {"id": "2-Ff2OQef8D_", "tags": ["draft", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>EXERCISE: L10.3.2\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "#use this random seed when calculating/reporting your answers\n", "np.random.seed(101)\n", "\n", "#1000 random samples from posterior distribution\n", "sample=#YOUR CODE HERE\n", "\n", "#most extreme value from samples (take the largest absolute value)\n", "exvalue=#YOUR CODE HERE\n", "\n", "#the probability of attaining that extreme value (or greater), given your prior\n", "pvalue=#YOUR CODE HERE\n", "\n", "#print results\n", "print(\"extreme value\",exvalue,\"pvalue\",pvalue)"]}, {"cell_type": "markdown", "id": "251835bf", "metadata": {"id": "251835bf", "tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_10_4'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L10.4 Bayesian vs. Frequentist and Likelihood</h2>  \n", "\n", "| [Top](#section_10_0) | [Previous Section](#section_10_3) | [Exercises](#exercises_10_4) | [Next Section](#section_10_5) |\n"]}, {"cell_type": "markdown", "id": "2cb4d028", "metadata": {"id": "2cb4d028", "tags": ["learner", "md"]}, "source": ["*The material in this section is discussed in the video **<a href=\"https://courses.mitxonline.mit.edu/learn/course/course-v1:MITxT+8.S50.2x+1T2025/block-v1:MITxT+8.S50.2x+1T2025+type@sequential+block@seq_LS10/block-v1:MITxT+8.S50.2x+1T2025+type@vertical+block@vert_LS10_vid4\" target=\"_blank\">HERE</a>.** You are encouraged to watch that video and use this notebook concurrently.*"]}, {"cell_type": "markdown", "id": "352dd78f", "metadata": {"id": "352dd78f", "tags": ["learner", "md"]}, "source": ["<h3>Slides</h3>\n", "\n", "Run the code below to view the slides for this section, which are discussed in the related video. You can also open the slides in a separate window <a href=\"https://mitx-8s50.github.io/slides/L10/slides_L10_04.html\" target=\"_blank\">HERE</a>."]}, {"cell_type": "code", "execution_count": null, "id": "be50f51c", "metadata": {"id": "be50f51c", "outputId": "d714b17a-e3d9-4d42-e4b2-3e23045a5ac6", "tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L10.4-slides\n", "\n", "from IPython.display import IFrame\n", "IFrame(src='https://mitx-8s50.github.io/slides/L10/slides_L10_04.html', width=970, height=550)"]}, {"cell_type": "markdown", "id": "b0f0b314", "metadata": {"id": "f1b2297c", "tags": ["learner", "md", "lect_04"]}, "source": ["<h3>Overview</h3>\n", "\n", "Now, let's start to think about how we wish to quote our results. If our priors are incorrect, and we continue to progressively take data, at some point our priors are going to be wrong. Dealing with how this is wrong depends on what sort of statistician you are.\n", "\n", "**Frequentist**: **The data guides our model.** If I see a data distribution, I can come up with a way to fit it. By getting a good fit, I can explain the next result.  We can use what we observe in the data to explain how we will observe these phenomena in the future. \n", "\n", "**Bayesian**: It is possible to create a model of everything, and within your model you can explain all random phenomena. As we take more data, we can fine tune our model to be ever more predictive. "]}, {"cell_type": "markdown", "id": "545dad41", "metadata": {"id": "34f439e6", "tags": ["learner", "md", "lect_04"]}, "source": ["<h3>Bayesian View</h3>\n", "\n", "Given Bayes theorem, let's go back to our original measurement. In that case, we had two normal distributions, a posterior about two, and a prior about zero. How do we connect these two? \n", "\n", "$$\n", "\\begin{equation}\n", " P\\left(\\mathcal{H} | \\mathcal{D} \\right) =  \\frac{P\\left(\\mathcal{D} | \\mathcal{H} \\right)P(\\mathcal{H})}{P(\\mathcal{D})} \\\\\n", "\\mathcal{N}(x,\\mu=2,1) = \\frac{P\\left(\\mathcal{D} | \\mathcal{H} \\right)}{P(\\mathcal{D})}\\mathcal{N}(x,\\mu=0,1) \\\\\n", "P\\left(\\mathcal{D} | \\mathcal{H} \\right) = \\frac{\\mathcal{N}(x,\\mu=2,1)}{\\mathcal{N}(x,\\mu=0,1)} P(\\mathcal{D}) \n", "\\end{equation}\n", "$$\n", "\n", "Since $P(\\mathcal{D})$ is just a constant to ensure that our resulting probability is normalized, we can pretty easily compute the likelihood ratio in our first example. "]}, {"cell_type": "markdown", "id": "fa2dfc98", "metadata": {"id": "34f439e6", "tags": ["learner", "md", "lect_04"]}, "source": ["**Note:** In the remainder of this section, the terms \"likelihood\" and \"likelihood ratio\" are used to refer to the ratio of the probability of the data or posterior divided by the probability of the prior, both evaluated at a specific point. This ratio gets very large at any point where data appears (or, equivalently, the probability of the posterior is not small) but the prior predicts a very low probability of that happening. So, with this definition a large *likelihood* indicates the presence of very *unlikely* data if the prior is correct. You will see graphs labeled \"Likelihood\" which shoot up to extremely large values when the posterior exceeds the prior by a large factor.\n", "\n", "Section 10.5 will discuss fitting and then we will return to the more common definition of the likelihood as the probability of an entire dataset for a given prior in Section 10.6. \n", "\n", "For now, we continue with this definition of \"Likelihood\", and begin by plotting the ratio described above using a posterior Gaussian with a mean of 2 and a prior Gaussian with a mean of 0, both with widths of 1. \n", " "]}, {"cell_type": "code", "execution_count": null, "id": "ac59db9e", "metadata": {"id": "ac59db9e", "outputId": "8a402ae5-8199-4c46-9e24-fd13152b417d", "tags": ["learner", "py", "lect_04", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L10.4-runcell01\n", "\n", "#Our measurement probability\n", "def gaus(mu=0,sigma=1): \n", "    x = np.arange(-10, 10, 0.001)\n", "    y = stats.norm.pdf(x,mu,sigma)\n", "    return x,y\n", "\n", "def plotGausSampleLike(iZ,iSample):\n", "    plt.style.use('fast')\n", "    fig, ax = plt.subplots(figsize=(9,6))\n", "    #Sample\n", "    samples = np.random.normal(iZ,1,iSample)\n", "    x,y=gaus(iZ,1)\n", "    #prior\n", "    xs,ys=gaus(0,1)\n", "    #likelihood\n", "    yratio=np.minimum(y/(ys*20.),1.)\n", "    #plot\n", "    ax.plot(xs,ys,label='prior')\n", "    ax.plot(x,y,label='posterior')\n", "    ax.plot(x,yratio,label='Likelihood/20')\n", "    count, bins, ignored = plt.hist(samples, 30, density=True)\n", "    ax.fill_between(xs,ys,0, alpha=0.1)\n", "    ax.set_xlim([-6,6])\n", "    ax.set_xlabel('x')\n", "    ax.set_ylabel('p')\n", "    ax.legend()\n", "    plt.show()\n", "\n", "plotGausSampleLike(2,1000)"]}, {"cell_type": "markdown", "id": "426f8890", "metadata": {"id": "426f8890", "tags": ["learner", "md", "lect_04"]}, "source": ["As you can see, the likelihood shoots up, and in fact shoots up above 1 (note that what is plotted in likelihood/20). This means that our likelihood is unphysical, and we are not capturing our physics, or more importantly, this means that **our prior is wrong**. What's the right prior in this scenario? \n", "\n", "Let's tweak our prior to have a large sigma $\\mathcal{N}(\\mu=0,\\sigma=2)$."]}, {"cell_type": "code", "execution_count": null, "id": "7379f233", "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 689}, "id": "7379f233", "outputId": "e6bb4320-ada7-4f67-b2a0-e8c60665e169", "tags": ["learner", "py", "lect_04", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L10.4-runcell02\n", "\n", "def plotGausSampleLikeNew(iZ,iSample,iSigMax):\n", "    plt.style.use('fast')\n", "    fig, ax = plt.subplots(figsize=(9,6))\n", "    #ample our posterior\n", "    samples = np.random.normal(iZ,1,iSample)\n", "    x,y=gaus(iZ,1)\n", "    #Sample our prior\n", "    xs,ys=gaus(0,iSigMax) #######<<<<< This is our tweak\n", "    #now compute the likelihood\n", "    yratio=np.minimum(0.05*y/ys,20.)\n", "    #plot this stuff\n", "    ax.plot(xs,ys,label='prior')\n", "    ax.plot(x,y,label='posterior')\n", "    ax.plot(x,yratio,label='Likelihood/20')\n", "    count, bins, ignored = plt.hist(samples, 30, density=True)\n", "    ax.fill_between(xs,ys,0, alpha=0.1)\n", "    ax.set_xlim([-6,6])\n", "    ax.set_xlabel('x')\n", "    ax.set_ylabel('p')\n", "    ax.legend()\n", "    plt.show()\n", "    #now return our sampled normal distribution\n", "    return samples\n", "    \n", "samples=plotGausSampleLikeNew(2,1000,2)"]}, {"cell_type": "markdown", "id": "b033ec4f", "metadata": {"id": "b033ec4f", "tags": ["learner", "md", "lect_04"]}, "source": ["What have we done?  Essentially, we had previously claimed that our measurement was at zero with uncertainty (expected $\\sigma$=1). Now, what we have done is made the claim that our uncertainty (the width of our prior) is much larger. This new, larger uncertainty lowered our likelihood to some number that is now not insanely large. "]}, {"cell_type": "markdown", "id": "d04556aa", "metadata": {"id": "d04556aa", "tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='exercises_10_4'></a>     \n", "\n", "| [Top](#section_10_0) | [Restart Section](#section_10_4) | [Next Section](#section_10_5) |\n"]}, {"cell_type": "markdown", "id": "281f9e8e", "metadata": {"id": "729ba31a", "tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-10.4.1</span>\n", "\n", "The choice of prior can really change your outlook on fittings. Here you will compare the likelihood using two different priors.\n", "\n", "First, plot the maximum likelihood from -5 to 5, using a posterior Gaussian distribution centered at 0.5 and a prior Gaussian distribution centered at 0. Next, use a prior that is a flat distribution, and compare the new likelihood ratio to your previous result. Which prior would be better to use, and why? Choose the best answer from the following:\n", "\n", "A) Using a Gaussian prior yields a likelihood ratio that is also Gaussian.\\\n", "B) Using a Gaussian prior yields a likelihood ratio that is easier to interpret.\\\n", "C) Using a flat prior ensures that the likelihood ratio is a contant value.\\\n", "D) Using a flat prior ensures that the likelihood ratio does not skyrocket."]}, {"cell_type": "code", "execution_count": null, "id": "e36a1e8d", "metadata": {"id": "e36a1e8d", "outputId": "e7aebda7-27c0-4478-95f7-e466a8802059", "tags": ["draft", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>EXERCISE: L10.4.1\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "def flat(iX):\n", "    return np.where(iX < 5, np.where(iX < -5, 0, 0.1 ), 0)\n", "\n", "def plotGausSampleLikeNew2(iPost,iPrior):\n", "    plt.style.use('fast')\n", "    fig, ax = plt.subplots(figsize=(9,6))\n", "    \n", "    #Base Posterior a function\n", "    x,y=gaus(iPost,1)\n", "        \n", "    #Gaussian prior\n", "    xs,ys = ### Add code here    \n", "\n", "    #Flat Prior\n", "    yflat = ### Add code here    \n", "    \n", "    #now compute the likelihood ratio\n", "    #use 20. as a maximum value\n", "    ygaussratio = ### add code here\n", "    yflatratio = ### add code here\n", "    \n", "    #plot this stuff\n", "    ax.plot(x,y,label='posterior')\n", "    \n", "    #plot Gaussian prior and likelihood\n", "    ax.plot(xs,ys,label='Gaussian prior')\n", "    ax.fill_between(xs,ys,0, alpha=0.1)\n", "    ax.plot(x,yratio/20.,label='Gaussian Likelihood/20')\n", "    \n", "    #plot flat prior and likelihood\n", "    ax.plot(x,yflat,label='Flat prior')\n", "    ax.plot(x,yflatratio/20.,label='Flat Likelihood/20')\n", "    \n", "    ax.set_xlim([-5,5])\n", "    ax.set_xlabel('x')\n", "    ax.set_ylabel('p')\n", "    ax.legend()\n", "    plt.show()\n", "    #now return our sampled normal distribution\n", "    return samples\n", "    \n", "samples=plotGausSampleLikeNew2(2,0)\n"]}, {"cell_type": "markdown", "id": "44603f5d", "metadata": {"id": "729ba31a", "tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-10.4.2</span>\n", "\n", "Now let's calculate the likelihood ratio for the maximum value obtained from simulated data, where the data are randomly drawn from a Gaussian distribution with mean=0.25 and sigma=1. We will use a Gaussian prior with mean=0 and sigma=1. \n", "\n", "First, complete the code, which should do the following: the `maxlikelihood` function should calculate the likelihood ratio between two Gaussian distributions (posterior/prior), evaluated at the value `val`, where `val` is the maximum value obtained from the randomly sampled array`isamples`. In the function `maxlike`, the array `samples` is drawn from a Gaussian distribution with mean=0.25 and sigma=1, and `plotGausSampleLikeNew` is used to obtain the data and make the plots.\n", "\n", "After completing the code, compare the output of `maxlike` using 1e2 samples and 1e6 samples. What is the maximum likelihood value in each case? Would you expect it to increase or decrease with more samples? What does this mean about the distributions (do we have the right prior)?\n", "\n", "Report your answer as a list of two numbers with precition 1e-2: `[max likelihood 1e2 samples, max likelihood 1e6 samples]`\n", "\n"]}, {"cell_type": "code", "execution_count": null, "id": "c5a46e15", "metadata": {"id": "3784c760", "outputId": "da0a7e7b-7fe5-452b-a85b-803c51eda779", "tags": ["learner", "learner_chopped", "draft"]}, "outputs": [], "source": ["#>>>EXERCISE: L10.4.2\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "np.random.seed(3)\n", "\n", "def maxlikelihood(isamples,mu1=0.25,sig1=1,mu2=0,sig2=1):\n", "    val=np.max(isamples)#compute the highest sampled gaussian\n", "    #now compute the likelihood of these two\n", "    like=#YOUR CODE HERE\n", "    return like\n", "\n", "def maxlike(iN):\n", "    samples=plotGausSampleLikeNew(0.25,iN,1)\n", "    like=maxlikelihood(samples)\n", "    print(\"Max likelihood:\",iN,\" is \",like)\n", "\n", "maxlike(100)\n", "maxlike(1000000)"]}, {"cell_type": "markdown", "id": "3a14e017", "metadata": {"id": "3a14e017", "tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_10_5'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L10.5 Bayesian vs. Frequentist Fitting Example</h2>  \n", "\n", "| [Top](#section_10_0) | [Previous Section](#section_10_4) | [Exercises](#exercises_10_5) | [Next Section](#section_10_6) |\n"]}, {"cell_type": "markdown", "id": "83204fdc", "metadata": {"id": "b033ec4f", "tags": ["learner", "md", "lect_05"]}, "source": ["<h3>Overview</h3>\n", "\n", "Note: There is no video associated with this content.\n", "\n", "To try to capture these ideas, let's try fitting a Gaussian with two separate approaches: Bayesian vs. Frequentist. To get the data, we will randomly sample a Gaussian distribution with mean 0 and standard deviation 1.\n", "\n", "**Note: You may refer to the first module of this course series, 8.S50.1x, for more details on fitting. In particular, we are using lmfit, which is introduced in Lesson 4 of the course.**\n"]}, {"cell_type": "markdown", "id": "d2c590b7", "metadata": {"id": "b033ec4f", "tags": ["learner", "md", "lect_05"]}, "source": ["<h3>Frequentist Approach</h3>\n", "\n", "First, we use the Frequentist approach, in which we fit the data with a Gaussian with all of its parameters allowed to vary. Let's suggest to the fit model that the center should be at 2, but all it to freely fit this parameter."]}, {"cell_type": "code", "execution_count": null, "id": "2a605327", "metadata": {"id": "2a605327", "outputId": "033b4b9b-0a28-4157-c11c-7d8860700785", "scrolled": false, "tags": ["learner", "py", "lect_05", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L10.5-runcell01\n", "\n", "#with Bayesian, we hypothesize a Gaussian and fit it\n", "from lmfit.models import GaussianModel\n", "\n", "np.random.seed(42)\n", "\n", "#randomly sample 100 events from a Gaussian\n", "lN=100\n", "samples = np.random.normal(0,1,lN)\n", "#make a histogram\n", "count, bins, ignored = plt.hist(samples,30)\n", "binscenters = np.array([0.5 * (bins[i] + bins[i+1]) for i in range(len(bins)-1)])\n", "#poisson unc.\n", "weight=1./np.sqrt(count)\n", "weight[weight==float('+inf')] = 0\n", "plt.show()\n", "\n", "#Now we can consider two ways to interpret the data\n", "def frequentist(iBins,iCount,weight): #fit a Gaussian floating all parameters\n", "    model = GaussianModel()\n", "    params = model.make_params(center=2, amplitude=1, sigma=1) \n", "    result = model.fit(iCount, params, x=iBins,weights=weight)\n", "    result.plot()\n", "    print(result.fit_report())\n", "    \n", "\n", "frequentist(binscenters,count,weight)\n"]}, {"cell_type": "markdown", "id": "0a1a830d", "metadata": {"id": "b033ec4f", "tags": ["learner", "md", "lect_05"]}, "source": ["<h3>Bayesian Approach</h3>\n", "\n", "Next, we use the Bayesian approach with a prior that is a Gaussian centered at 2 (note this is the incorrect prior, which we are using on purpose). We will fix this value and try to force the fit."]}, {"cell_type": "code", "execution_count": null, "id": "df976a40", "metadata": {"id": "2a605327", "outputId": "033b4b9b-0a28-4157-c11c-7d8860700785", "scrolled": false, "tags": ["learner", "py", "lect_05", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L10.5-runcell02\n", "\n", "np.random.seed(42)\n", "    \n", "def bayesianBad(iBins,iCount,weight):#fit a gaussian fix the mean and sgima\n", "    model = GaussianModel()\n", "    params = model.make_params(center=2, amplitude=1, sigma=1) \n", "    params['center'].vary=False\n", "    params['sigma'].vary=False\n", "    result = model.fit(iCount, params, x=iBins,weights=weight)\n", "    result.plot()\n", "    print(result.fit_report())\n", "\n", "bayesianBad(binscenters,count,weight)"]}, {"cell_type": "markdown", "id": "66305cef", "metadata": {"id": "66305cef", "tags": ["learner", "md", "lect_05"]}, "source": ["**Frequentist:** From the example shown above, what you see is that in the frequentist scenario, we just fit the data distribution and extract the parameters. In the frequentist approach, the data is key, and so if our $\\chi^{2}$ is good for our fitted model, we can declare success. \n", "\n", "**Bayesian:** In the Bayesian approach, we need to reconcile our prior with our observed data. If our prior is that our data should behave as a Gaussian with a mean of 2, and we try to fit it to the data, you see there is not a very good $\\chi^{2}$ value, and the fit is clearly way off. "]}, {"cell_type": "markdown", "id": "43a6264b", "metadata": {"id": "66305cef", "tags": ["learner", "md", "lect_05"]}, "source": ["<h3>Modifying the Model</h3>\n", "\n", "To reconcile our poor quality Bayesian fit, what we need to do is modify our model so that we can actually go from our prior to our fitted function. To do this, we need to insert a new prior. In this case, our prior will still be a Gaussian but with an unknown mean, so that we allow the mean to vary in the fit. Let's write this down.  \n", "\n", "$$\n", "\\begin{eqnarray}\n", " P\\left(\\mathcal{H}=x\\right|\\mu,\\sigma) & = & \\mathcal{N}(x,\\mu=2,\\sigma=1) \\\\\n", " P\\left(\\mathcal{H}=\\mu\\right|\\sigma) & = & \\frac{1}{b-a}~\\forall~\\mu~\\in~[a,b] \\\\ \n", "                               & = & 0~~~~~~~~~\\forall~\\mu~\\notin~[a,b]\n", "\\end{eqnarray}\n", "$$\n", "\n", "All we are saying here is that $\\mu$ can now vary between a and b. Let's now fit the data with this new approach. To do this, we are going to use a new feature in `lmfit`. We will use the function `lmfit.minimize` and feed it a modified loss function, which we define as `resid` (aka the residual function). We will use the likelihood as our loss function.\n", "\n", "Additionally, we will add another constraint into the fit. In particular, we will add a parameter $\\Delta_{\\mu}$, which we call the bias, such that the loss and $\\mu$ will be written as: \n", "\n", "$$\n", "\\begin{eqnarray}\n", "\\mu_{\\rm new} & = & \\mu -\\Delta_{\\mu} \\\\\n", "\\mathcal{L}_{\\rm new} & = & \\mathcal{L} + 0~\\forall~\\mu~\\in~[a,b] \\\\ \n", "                      & = & \\mathcal{L} + \\inf~\\forall~\\mu~\\notin~[a,b] \n", "\\end{eqnarray}\n", "$$\n", "\n", "In this case, we will just use a really large number to approximate infinity, and allow the mean to vary by $\\pm3$. "]}, {"cell_type": "code", "execution_count": null, "id": "7c658423", "metadata": {"id": "7c658423", "outputId": "cf2055f2-4940-41c6-b083-e6b1908c3958", "tags": ["learner", "py", "lect_05", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L10.5-runcell03\n", "\n", "np.random.seed(42)\n", "\n", "import lmfit\n", "#here is our modified function\n", "def gauss(x, amp, mu, sigma,dmu):\n", "    return amp * np.exp(-(x-mu+dmu)**2 / (2.*sigma**2))\n", "\n", "#now we define our loss we want to minimize\n", "def resid(params, x, ydata,weights):\n", "    mu    = params['center'].value\n", "    sigma = params['sigma'].value\n", "    amp   = params['amplitude'].value\n", "    dmu   = params['deltamu'].value\n", "    lossshift=0\n", "    if abs(dmu) > 3:\n", "        lossshift=1e32\n", "    y_model= gauss(x,amp,mu,sigma,dmu)\n", "    residarr = (y_model - ydata)*weights\n", "    #now append our constraint to the loss\n", "    residarr = np.append(residarr,lossshift)\n", "    return residarr\n", "    \n", "def bayesianGood(iBins,iCount,weights,initial=2):\n", "    model = GaussianModel()\n", "    params = model.make_params(center=initial, amplitude=1, sigma=1) \n", "    params['center'].vary=False\n", "    params['sigma'].vary=False\n", "    params.add(\"deltamu\", value=0.0, min=-10, max=10) #Our new line of code\n", "    result = lmfit.minimize(resid, params, args=(iBins, iCount,weights))\n", "    lmfit.report_fit(result)\n", "    #Now we plot it. \n", "    plt.errorbar(iBins, iCount,np.sqrt(iCount), lw=2,fmt=\".k\", capsize=0)\n", "    plt.plot(binscenters,gauss(binscenters,result.params['amplitude'].value,result.params['center'].value,result.params['sigma'].value,result.params['deltamu'].value))\n", "    plt.xlabel(\"x\")\n", "    plt.ylabel(\"p\")\n", "    plt.show()\n", "\n", "bayesianGood(binscenters,count,weight)\n", "\n"]}, {"cell_type": "markdown", "id": "92ce12f3", "metadata": {"id": "92ce12f3", "tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='exercises_10_5'></a>     \n", "\n", "| [Top](#section_10_0) | [Restart Section](#section_10_5) | [Next Section](#section_10_6) |\n"]}, {"cell_type": "markdown", "id": "77b83f34", "metadata": {"id": "77b83f34", "tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-10.5.1</span>\n", "\n", "Run the above Bayesian good fit, centering the mean at an incorrect value of 1 and then the correct value 0 (this can be set using the `initial` parameter in the function).\n", "\n", "What is the bias on mu (i.e., `deltamu`) in each case? Is it significant? Report your answer as a list of two numbers with precision 1e-2: `[deltamu1, deltamu0]`.\n"]}, {"cell_type": "code", "execution_count": null, "id": "26e25076", "metadata": {"id": "26e25076", "outputId": "001a283e-4e4a-4706-970b-e270cdce3749", "tags": ["draft", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>EXERCISE: L10.5.1\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "np.random.seed(42)\n", "#YOUR CODE HERE"]}, {"cell_type": "markdown", "id": "88302269", "metadata": {"id": "88302269", "tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_10_6'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L10.6 Maximum Likelihood</h2>     \n", "\n", "| [Top](#section_10_0) | [Previous Section](#section_10_5) | [Exercises](#exercises_10_6) |\n"]}, {"cell_type": "markdown", "id": "00f28f42", "metadata": {"id": "00f28f42", "tags": ["learner", "md"]}, "source": ["*The material in this section is discussed in the video **<a href=\"https://courses.mitxonline.mit.edu/learn/course/course-v1:MITxT+8.S50.2x+1T2025/block-v1:MITxT+8.S50.2x+1T2025+type@sequential+block@seq_LS10/block-v1:MITxT+8.S50.2x+1T2025+type@vertical+block@vert_LS10_vid6\" target=\"_blank\">HERE</a>.** You are encouraged to watch that video and use this notebook concurrently.*"]}, {"cell_type": "markdown", "id": "726afeb3", "metadata": {"id": "3b2fc1fc", "tags": ["learner", "md", "lect_06"]}, "source": ["<h3>Maximum Likelihood</h3>\n", "\n", "Let's say we have a sample that is Gaussian distributed, and we want to find the maximum likelihood value of the mean and the resolution of this sample. \n", "\n", "**Note:** Here, we return to the more common definition of the likelihood as the probability of an entire dataset for a given prior. This can be found by multiplying the probabilities of each individual data point. For a prior probability that is a Gaussian, this product of probabilities can be written as a single exponential of the sum of exponents for each datapoint, as shown below.\n", "\n", "For this sample, we can write the likelihood and log likelihood as:  \n", "\n", "$$\n", "\\begin{equation}\n", "\\mathcal{L}(x|\\mu_{i},\\sigma_{i}) = \\left(\\frac{1}{\\sqrt{2\\pi\\sigma^{2}}}\\right)^{N} \\exp\\left(-\\sum_{i=0}^{N} \\frac{(x_{i}-\\mu)^2}{2\\sigma^{2}}\\right)\\\\\n", "\\log\\left(\\mathcal{L}(x|\\mu_{i},\\sigma_{i}) \\right) =  -\\frac{N}{2}\\log\\left(2\\pi\\sigma^{2}\\right) - \\sum_{i=0}^{N} \\frac{(x_{i}-\\mu)^2}{2\\sigma^{2}}\n", "\\end{equation}\n", "$$\n", "\n", "To get the value of $\\mu$ that gives the maximum likelihood, we can write: \n", "\n", "$$\n", "\\begin{eqnarray}\n", "\\frac{\\partial}{\\partial \\mu}\\log\\left(\\mathcal{L}(x|\\mu_{i},\\sigma_{i}) \\right) & = & \\frac{\\partial}{\\partial \\mu} \\left( -\\frac{N}{2} \\log\\left(2\\pi\\sigma^{2}\\right) - \\sum_{i=0}^{N} \\frac{(x_{i}-\\mu)^2}{2\\sigma^{2}} \\right)  = 0\\\\\n", "& = &  \\left( \\sum_{i=0}^{N} \\frac{(x_{i}-\\mu)}{\\sigma^{2}} \\right)  = 0\\\\\n", "\\sum_{i=0}^{N} (x_{i}-\\hat{\\mu}) & = & 0 \\\\\n", "\\sum_{i=0}^{N} x_{i} - N \\hat{\\mu} & = & 0 \\\\\n", "\\end{eqnarray}\n", "$$\n", "\n", "This gives us a best fit value for $\\mu$, indicated as $\\hat{\\mu}$ of\n", "\n", "$$\n", "\\begin{eqnarray}\n", "\\hat{\\mu} & = & \\frac{1}{N} \\sum_{i=0}^{N} x_{i}\n", "\\end{eqnarray}\n", "$$\n", "\n", "Now for $\\sigma^{2}$, we can do the same thing, and we get (for a best fit $\\hat{\\sigma}^{2}$):  \n", "\n", "$$\n", "\\begin{eqnarray}\n", "\\frac{\\partial}{\\partial \\sigma^{2}}\\log\\left(\\mathcal{L}(x|\\mu_{i},\\sigma_{i}) \\right) & = & \\frac{\\partial}{\\partial \\sigma^{2}} \\left( -\\frac{N}{2} \\log\\left(2\\pi\\sigma^{2}\\right) - \\sum_{i=0}^{N} \\frac{(x_{i}-\\mu)^2}{2\\sigma^{2}} \\right)  = 0 \\\\\n", "& = & \\left( -\\frac{N}{2\\sigma^{2}} +  \\sum_{i=0}^{N} \\frac{(x_{i}-\\mu)^2}{2\\sigma^{4}} \\right) \\\\\n", "0 & = & \\left( -N +  \\sum_{i=0}^{N} \\frac{(x_{i}-\\mu)^2}{\\hat{\\sigma}^{2}}\\right) \\\\\n", "\\hat{\\sigma}^{2} & = & \\frac{1}{N} \\sum_{i=0}^{N}(x_{i}-\\mu)^2  \n", "\\end{eqnarray}\n", "$$\n", "\n", "Now, none of this is a big surprise. The best fit for the mean and variance of a fitted Gaussian distribution is the mean and variance of the data. However, this derivation shows you the thought process that is undergone when trying to construct a maximum likelihood estimator of a sample. \n", "\n"]}, {"cell_type": "markdown", "id": "567a1ffa", "metadata": {"id": "567a1ffa", "tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='exercises_10_6'></a>   \n", "\n", "| [Top](#section_10_0) | [Restart Section](#section_10_6) |\n"]}, {"cell_type": "markdown", "id": "db83a929", "metadata": {"id": "db83a929", "tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-10.6.1</span>\n", "\n", "Which of the following statements best explains why we maximize the likelihood function in statistical inference?\n", "\n", "A) To obtain estimates of unknown parameters based on observed data\\\n", "B) To minimize the sum of squared errors between the predicted and actual values\\\n", "C) To test the hypothesis that the data was generated by a particular model\\\n", "D) To calculate the mean and variance of the data\n"]}], "metadata": {"celltoolbar": "Tags", "kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}