{"cells": [{"cell_type": "markdown", "id": "eca9cd34", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<hr style=\"height: 1px;\">\n", "<i>This notebook was authored by the 8.S50x Course Team, Copyright 2022 MIT All Rights Reserved.</i>\n", "<hr style=\"height: 1px;\">\n", "<br>\n", "\n", "<h1>Lesson 11: Hypothesis Testing Part 1</h1>\n"]}, {"cell_type": "markdown", "id": "62e126bf", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_11_0'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L11.0 Overview</h2>\n"]}, {"cell_type": "markdown", "id": "044d26a9", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<h3>Navigation</h3>\n", "\n", "\n", "<table style=\"width:100%\">\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_11_1\">L11.1 Likelihood Ratio and Hypothesis Testing</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_11_1\">L11.1 Exercises</a></td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_11_2\">L11.2 Hypothesis Test Example</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_11_2\">L11.2 Exercises</a></td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_11_3\">L11.3 A More Realistic Example</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_11_3\">L11.3 Exercises</a></td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_11_4\">L11.4 Probability of Fit</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_11_4\">L11.4 Exercises</a></td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_11_5\">L11.5 The t-test Statistic</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_11_5\">L11.5 Exercises</a></td>\n", "    </tr>\n", "</table>\n", "\n"]}, {"cell_type": "code", "execution_count": null, "id": "ee2b6ce0", "metadata": {"tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L11.0-runcell00\n", "\n", "!pip install lmfit"]}, {"cell_type": "code", "execution_count": null, "id": "bea8c546", "metadata": {"tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L11.0-runcell01\n", "\n", "import numpy as np                #https://numpy.org/doc/stable/ \n", "import lmfit                      #https://lmfit.github.io/lmfit-py/ \n", "import matplotlib.pyplot as plt   #https://matplotlib.org/3.5.3/api/_as_gen/matplotlib.pyplot.html\n", "from scipy import stats           #https://docs.scipy.org/doc/scipy/reference/stats.html"]}, {"cell_type": "code", "execution_count": null, "id": "025d78e8", "metadata": {"tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L11.0-runcell02\n", "\n", "#set plot resolution\n", "%config InlineBackend.figure_format = 'retina'\n", "\n", "#set default figure parameters\n", "plt.rcParams['figure.figsize'] = (9,6)\n", "\n", "medium_size = 12\n", "large_size = 15\n", "\n", "plt.rc('font', size=medium_size)          # default text sizes\n", "plt.rc('xtick', labelsize=medium_size)    # xtick labels\n", "plt.rc('ytick', labelsize=medium_size)    # ytick labels\n", "plt.rc('legend', fontsize=medium_size)    # legend\n", "plt.rc('axes', titlesize=large_size)      # axes title\n", "plt.rc('axes', labelsize=large_size)      # x and y labels\n", "plt.rc('figure', titlesize=large_size)    # figure title\n", "\n"]}, {"cell_type": "markdown", "id": "1db6545d", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_11_1'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L11.1 Likelihood Ratio and Hypothesis Testing</h2>  \n", "\n", "| [Top](#section_11_0) | [Previous Section](#section_11_0) | [Exercises](#exercises_11_1) | [Next Section](#section_11_2) |\n"]}, {"cell_type": "code", "execution_count": null, "id": "c187e12d", "metadata": {"tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L11.1-slides\n", "\n", "from IPython.display import IFrame\n", "IFrame(src='https://mitx-8s50.github.io/slides/L10/slides_L10_06.html', width=970, height=550)"]}, {"cell_type": "code", "execution_count": null, "id": "d7c0ae36", "metadata": {"tags": ["learner", "py", "lect_01", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L11.1-runcell01\n", "\n", "np.random.seed(5)\n", "\n", "testSamplesA = np.random.normal(0  ,1, 50)\n", "testSamplesB = np.random.normal(0.2,1, 50)\n", "\n", "def plotHist(iSamples,iLabel,bin_edges=None):\n", "    if bin_edges is None:\n", "        y, bin_edges = np.histogram(iSamples, bins=5)\n", "    else:\n", "        y, bin_edges = np.histogram(iSamples, bins=bin_edges)\n", "    bin_centers = 0.5*(bin_edges[1:] + bin_edges[:-1])\n", "    norm=len(iSamples)\n", "    plt.errorbar(bin_centers,y/norm,yerr=y**0.5/norm,fmt=\".\",label=iLabel)\n", "    return bin_edges\n", "\n", "bin_edges=plotHist(testSamplesA,\"p$_{a}$(x)\")\n", "bin_edges=plotHist(testSamplesB,\"p$_{b}$(x)\",bin_edges)\n", "plt.xlabel(\"x\")\n", "plt.ylabel(\"p\")\n", "plt.legend()\n", "plt.show()"]}, {"cell_type": "code", "execution_count": null, "id": "10cf081c", "metadata": {"tags": ["learner", "py", "lect_01", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L11.1-runcell02\n", "\n", "#Now let's compute the log-likelihood values of our samples\n", "#ulimately, we will use this function to see if Wilks' theorem is true (in the next section)\n", "\n", "#Let's define the log-likelihood in the simplest way possible (assume we don't know math)\n", "#note we will use -1 to compute these on the fly\n", "def loglikelihoodGaus(isample,mu0=-1,sigma0=-1):\n", "    n=len(isample)\n", "    if mu0 == -1:\n", "        mu0=isample.mean()\n", "    if sigma0 == -1:\n", "        sigmahat2=(isample-mu0)**2/len(isample)\n", "        sigmahat2=sum(sigmahat2)\n", "    val=-n/2*(np.log(2*np.pi*sigmahat2)+1)\n", "    return val\n", "\n", "hypoth=loglikelihoodGaus(testSamplesA,0) #compute with a fixed mean of 0\n", "null  =loglikelihoodGaus(testSamplesB) #compute with mean floating\n", "print(hypoth-null)"]}, {"cell_type": "markdown", "id": "154b4b30", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='exercises_11_1'></a>     \n", "\n", "| [Top](#section_11_0) | [Restart Section](#section_11_1) | [Next Section](#section_11_2) |\n"]}, {"cell_type": "markdown", "id": "db83a929", "metadata": {"id": "db83a929", "tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-11.1.1</span>\n", "\n", "Which of the following best describes the use of the likelihood ratio in statistical inference?\n", "\n", "A) To compare the fit of two models to a given set of data\\\n", "B) To calculate the probability of observing a particular value of a parameter\\\n", "C) To determine the significance of a test statistic\\\n", "D) To estimate the distribution of a random variable"]}, {"cell_type": "markdown", "id": "f5cfbc81", "metadata": {"id": "db83a929", "tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-11.1.2</span>\n", "\n", "The first hypothesis test was developed by a statistician (Fisher) when he was serving tea to Muriel Bristol and her fiancee William Roach. Muriel claimed that she could tell whether milk was poured into a tea cup before the tea was poured in or after it was poured in.\n", "\n", "To test this, the statistician made 8 cups of tea, with 4 of those cups having milk before the tea was poured, and 4 of those cups having milk after the tea was poured. \n", "\n", "What is the null Hypothesis in this test? Choose the best answer from the following:\n", "\n", "A) Muriel will select all cups correctly.\\\n", "B) Muriel will not have any correct selections.\\\n", "C) Muriel will just guess randomly, and so will have some correct and some incorrect selections.\\\n", "D) Muriel will have exactly 1/2 correct selections and 1/2 incorrect selections."]}, {"cell_type": "markdown", "id": "Nir0gkF07Up4", "metadata": {"id": "Nir0gkF07Up4", "tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-11.1.3</span>\n", "\n", "Muriel apparently guessed all 4 cups correctly that had milk poured in before (and likewise the other 4 that had it poured after). What is the probability that she guessed randomly and just got lucky? Enter your answer as a number with precision 1e-3."]}, {"cell_type": "code", "execution_count": null, "id": "2bce8b57", "metadata": {"id": "2bce8b57", "outputId": "959236dc-a9e4-4b40-e3e4-86edba76d426", "tags": ["draft", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>EXERCISE: L11.1.3\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "from scipy.special import comb\n", "\n", "#Hint: use comb(n,k), where\n", "#comb(n,k) is the number of combinations of n with k chosen\n"]}, {"cell_type": "markdown", "id": "9e6eb358", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_11_2'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L11.2 Hypothesis Test Example</h2>  \n", "\n", "| [Top](#section_11_0) | [Previous Section](#section_11_1) | [Exercises](#exercises_11_2) | [Next Section](#section_11_3) |\n"]}, {"cell_type": "code", "execution_count": null, "id": "be557d26", "metadata": {"tags": ["learner", "py", "learner_chopped", "lect_02"]}, "outputs": [], "source": ["#>>>RUN: L11.2-runcell01\n", "\n", "np.random.seed(41)\n", "\n", "def sampleGaus(iMean,iNToy=10000):\n", "    ntoys=iNToy\n", "    deltaLL=[]\n", "    for i0 in range(ntoys):\n", "        testsamples1 = np.random.normal(iMean,1, 50)\n", "        hypoth=loglikelihoodGaus(testsamples1,0) #compute with a fixed mean of 0\n", "        null  =loglikelihoodGaus(testsamples1) #compute with mean floating\n", "        val=hypoth-null\n", "        deltaLL.append(-2*val)\n", "    return deltaLL\n", "\n", "deltaLL1 = sampleGaus(0)\n", "deltaLL2 = sampleGaus(0.2)\n", "    \n", "y1, bin_edges = np.histogram(deltaLL1, bins=20)\n", "y2, bin_edges = np.histogram(deltaLL2, bins=bin_edges)\n", "\n", "#Now plot a chi2 with 1 dof\n", "bin_centers = 0.5*(bin_edges[1:] + bin_edges[:-1])\n", "\n", "#And Draw\n", "fig, ax = plt.subplots(figsize=(9,6))\n", "plt.style.use('fast')\n", "plt.errorbar(bin_centers,y1/len(deltaLL1),yerr=y1**0.5/len(deltaLL1),marker='.',drawstyle = 'steps-mid',label='$\\mu=0$')\n", "plt.errorbar(bin_centers,y2/len(deltaLL2),yerr=y2**0.5/len(deltaLL2),marker='.',drawstyle = 'steps-mid',label='$\\mu=0.2$')\n", "plt.xlabel(\"$\\Delta$ log-likelihood\")\n", "plt.ylabel(\"Toys\")\n", "ax.set_yscale('log')\n", "plt.legend()\n", "plt.show()"]}, {"cell_type": "code", "execution_count": null, "id": "c9b9174a", "metadata": {"tags": ["learner", "py", "learner_chopped", "lect_02"]}, "outputs": [], "source": ["#>>>RUN: L11.2-runcell02\n", "\n", "np.random.seed(41)\n", "\n", "#Now plot a chi2 with 1 dof\n", "chi2 = []\n", "for x in bin_centers:#range(len(bin_centers)):\n", "    chi2val=stats.chi2.pdf(x, 1)\n", "    chi2.append(chi2val)\n", "\n", "#And Draw\n", "fig, ax = plt.subplots(figsize=(9,6))\n", "plt.style.use('fast')\n", "plt.errorbar(bin_centers,y1/len(deltaLL1),yerr=y1**0.5/len(deltaLL1),marker='.',drawstyle = 'steps-mid',label='$\\mu=0$')\n", "plt.errorbar(bin_centers,y2/len(deltaLL2),yerr=y2**0.5/len(deltaLL2),marker='.',drawstyle = 'steps-mid',label='$\\mu=0.2$')\n", "plt.plot(bin_centers,chi2,label='$\\chi^{2}_{1}(x)$')\n", "plt.xlabel(\"$\\Delta$ log-likelihood\")\n", "plt.ylabel(\"Toys\")\n", "ax.set_yscale('log')\n", "plt.legend()\n", "plt.show()"]}, {"cell_type": "code", "execution_count": null, "id": "0dbe5080", "metadata": {"tags": ["learner", "py", "learner_chopped", "lect_02"]}, "outputs": [], "source": ["#>>>RUN: L11.2-runcell03\n", "\n", "#let's try to understand the deltaLL values \n", "#of the distributions we defined earlier\n", "\n", "np.random.seed(5)\n", "\n", "testSamplesA = np.random.normal(0  ,1, 50)\n", "testSamplesB = np.random.normal(0.2,1, 50)\n", "\n", "hypoth=loglikelihoodGaus(testSamplesA,0) #compute with a fixed mean of 0\n", "null  =loglikelihoodGaus(testSamplesA) #compute with mean floating\n", "deltaLL = -2.*(hypoth-null)\n", "print('deltaLL testSamplesA:', deltaLL)\n", "\n", "hypoth=loglikelihoodGaus(testSamplesB,0) #compute with a fixed mean of 0\n", "null  =loglikelihoodGaus(testSamplesB) #compute with mean floating\n", "deltaLL = -2.*(hypoth-null)\n", "print('deltaLL testSamplesB:', deltaLL)"]}, {"cell_type": "markdown", "id": "062a6d7a", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='exercises_11_2'></a>     \n", "\n", "| [Top](#section_11_0) | [Restart Section](#section_11_2) | [Next Section](#section_11_3) |\n"]}, {"cell_type": "markdown", "id": "dbdee5c5", "metadata": {"id": "db83a929", "tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-11.2.1</span>\n", "\n", "In the first plot in this section, why is the orange histogram (calculated from a distribution with `mu=0`) above the blue histogram (calculated from a distribution with `mu=0.2`)?\n", "\n", "A) The orange data is well described by the model with `mu=0.2`\\\n", "B) There are systematic errors or biases in the orange data\\\n", "C) The model being tested is overfitting the orange data\\\n", "D) The orange data is not well described by the model with `mu=0`"]}, {"cell_type": "markdown", "id": "f702bb58", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_11_3'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L11.3 A More Realistic Example</h2>  \n", "\n", "| [Top](#section_11_0) | [Previous Section](#section_11_2) | [Exercises](#exercises_11_3) | [Next Section](#section_11_4) |\n"]}, {"cell_type": "code", "execution_count": null, "id": "d8ddd461", "metadata": {"tags": ["learner", "py", "learner_chopped", "lect_03"]}, "outputs": [], "source": ["#>>>RUN: L11.3-runcell01\n", "\n", "np.random.seed(42)\n", "\n", "#First let's make a data sample and fit it\n", "bkg = np.random.uniform(0,200, 300)\n", "sig = np.random.normal (100,15,200) #choose different options for the number of signal events\n", "#sig = np.random.normal (100,15,80)\n", "data = np.append(sig,bkg)\n", "y, bin_edges = np.histogram(data, bins=20)\n", "ys, bin_edges = np.histogram(sig, bins=bin_edges)\n", "yb, bin_edges = np.histogram(bkg, bins=bin_edges)\n", "\n", "bin_centers = 0.5*(bin_edges[1:] + bin_edges[:-1])\n", "plt.errorbar(bin_centers, y,yerr= y**0.5,marker='.',linestyle = 'None', color = 'black')\n", "#plt.errorbar(bin_centers,yb,yerr=ys**0.5,marker='.',linestyle = '-', color = 'red',label='bkg')\n", "#plt.errorbar(bin_centers,ys,yerr=yb**0.5,marker='.',linestyle = '-', color = 'blue',label='signal')\n", "#plt.legend()\n", "plt.xlabel(\"x\")\n", "plt.ylabel(\"N\")\n", "plt.show()\n"]}, {"cell_type": "code", "execution_count": null, "id": "d73c3d43", "metadata": {"tags": ["learner", "py", "learner_chopped", "lect_03"]}, "outputs": [], "source": ["#>>>RUN: L11.3-runcell02\n", "\n", "from scipy.stats import norm\n", "import lmfit\n", "\n", "def fNull(params,x):\n", "    val = norm.pdf(x,params[\"mu\"],params[\"sigma\"])\n", "    return params[\"amp\"]*val + params[\"c\"]\n", "\n", "def fAlt(params,x):\n", "    return params[\"c\"]*(x+1)/(x+1) #hack to output array\n", "\n", "def binnedLikelihood(params, x, ydata, weights, func):\n", "    y_model= func(params,x)\n", "    residarr = np.sqrt(((y_model - ydata)/(2*weights))**2 + np.log(2*np.pi*weights**2))\n", "    return residarr\n", "\n", "def fitData(iX,iY,iFunc,iPlot=False):\n", "    params = lmfit.Parameters()\n", "    params.add('c',    value=1,min=0,max=np.inf)\n", "    params.add('mu',   value=100,min=0,max=200)\n", "    params.add('sigma',value=10,min=0,max=200)\n", "    params.add('amp',  value=20,min=0,max=np.inf)\n", "    result = lmfit.minimize(binnedLikelihood, params, args=(iX,iY,(iY**0.5),iFunc))\n", "    #lmfit.report_fit(result)\n", "    if iPlot:\n", "        #Now we plot it. \n", "        plt.errorbar(iX, iY,np.sqrt(iY), lw=2,fmt=\".k\", capsize=0)\n", "        plt.plot(iX,iFunc(result.params,iX))\n", "        plt.xlabel(\"x\")\n", "        plt.ylabel(\"N\")\n", "        plt.show()\n", "    return result.residual\n", "\n", "def deltaNLL(x,y,iPlot=False):\n", "    LLNull=fitData(x,y,fNull,iPlot)\n", "    LLAlt=fitData(x,y,fAlt,iPlot)\n", "    return 2.*(np.sum(LLAlt*LLAlt)-np.sum(LLNull*LLNull))\n", "\n", "x=bin_centers\n", "NLL = deltaNLL(x,y,True)\n", "print(\"Delta negative log-likelihood:\",NLL)"]}, {"cell_type": "code", "execution_count": null, "id": "ab0af604", "metadata": {"tags": ["learner", "py", "learner_chopped", "lect_03"]}, "outputs": [], "source": ["#>>>RUN: L11.3-runcell03\n", "\n", "def toyNLL(iNEvents,iNToys,iBin_Edges):\n", "    deltaNLLArr=np.array([])\n", "    for i0 in range(iNToys):\n", "        if i0 % 50 == 0: #change to output message more frequently, e.g., i0 % 10 == 0\n", "            print(\"Toy:\",i0,\" of \",iNToys)\n", "        bkg = np.random.uniform(0,200, iNEvents)\n", "        y, bin_edges = np.histogram(bkg, bins=iBin_Edges)\n", "        x = 0.5*(bin_edges[1:] + bin_edges[:-1])\n", "        nll = deltaNLL(x,y)\n", "        deltaNLLArr=np.append(deltaNLLArr,nll)\n", "    return deltaNLLArr\n", "\n", "NLL1 = toyNLL(300,100,bin_edges) #adjust the number of runs as needed\n", "ll1, ll_bin_edges = np.histogram(NLL1, bins=20,density=True)\n", "ll_bin_centers = 0.5*(ll_bin_edges[1:] + ll_bin_edges[:-1])\n", "chi21 = np.array([])\n", "for x in ll_bin_centers:#range(len(bin_centers)):\n", "    chi2val=stats.chi2.pdf(x, 3) #len is to normalize\n", "    chi21 = np.append(chi21,chi2val)\n", "\n", "fig, ax = plt.subplots(figsize=(9,6))\n", "plt.plot(ll_bin_centers,ll1,drawstyle = 'steps-mid',label='100 signal toys')\n", "plt.plot(ll_bin_centers,chi21,label='$\\chi^{2}_{3}$')\n", "ax.axvline(x=NLL,linewidth=3,c='b',label=\"observed $\\Delta$ LL\")\n", "print(\"Significance with 3 floating\",stats.chi2.cdf(NLL,3))\n", "plt.legend()\n", "plt.yscale('log')\n", "plt.show()"]}, {"cell_type": "code", "execution_count": null, "id": "229f4582", "metadata": {"tags": ["learner", "py", "learner_chopped", "lect_03"]}, "outputs": [], "source": ["#>>>RUN: L11.3-runcell04\n", "\n", "#NOW WE FIX MU IN OUR FITTING PROCEDURE: params[\"mu\"].vary = False\n", "\n", "def fitData(iX,iY,iFunc,iPlot=False):\n", "    params = lmfit.Parameters()\n", "    params.add('c',    value=1,min=0,max=np.inf)\n", "    params.add('mu',   value=100,min=0,max=200) #try changing the value of mu, when we are leaving it fixed\n", "    params.add('sigma',value=10,min=0,max=200)\n", "    params.add('amp',  value=20,min=0,max=np.inf)\n", "    params[\"mu\"].vary = False\n", "    result = lmfit.minimize(binnedLikelihood, params, args=(iX,iY,(iY**0.5),iFunc))\n", "    #lmfit.report_fit(result)\n", "    if iPlot:\n", "        #Now we plot it. \n", "        plt.errorbar(iX, iY,np.sqrt(iY), lw=2,fmt=\".k\", capsize=0)\n", "        plt.plot(iX,iFunc(result.params,iX))\n", "        plt.xlabel(\"x\")\n", "        plt.ylabel(\"N\")\n", "        plt.show()\n", "    return result.residual\n", "\n", "def deltaNLL(x,y,iPlot=False):\n", "    LLNull=fitData(x,y,fNull,iPlot)\n", "    LLAlt=fitData(x,y,fAlt,iPlot)\n", "    return 2.*(np.sum(LLAlt*LLAlt)-np.sum(LLNull*LLNull))\n", "\n", "x=bin_centers\n", "NLL = deltaNLL(x,y,True)\n", "print(\"Delta negative log-likelihood:\",NLL)"]}, {"cell_type": "markdown", "id": "02a1dd4e", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='exercises_11_3'></a>     \n", "\n", "| [Top](#section_11_0) | [Restart Section](#section_11_3) | [Next Section](#section_11_4) |\n"]}, {"cell_type": "markdown", "id": "536df40d", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-11.3.1</span>\n", "\n", "How many degrees of freedom should we use when calculating the chi-square value associated with the delta log-likelihood, for the model where `mu` is a fixed parameter? Enter your answer as an integer."]}, {"cell_type": "markdown", "id": "e127224e", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-11.3.2</span>\n", "\n", "Scan the mean of the bump, and make a plot of delta log-likelihood vs mean. What are the features of this plot?\n", "\n", "Specifically, complete the following code, which varies the parameter $\\mu$ over the range `[60-140]` and generates a plot of `NLL` vs. $\\mu$. Enter the value of $\\mu$ that maximizes `NLL` as a number with precision 1e-1."]}, {"cell_type": "code", "execution_count": null, "id": "39156e74", "metadata": {"tags": ["draft", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>EXERCISE: L11.3.2\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "def fitData_vary_mu(iX,iY,iFunc,imu,iPlot=False):\n", "    params = lmfit.Parameters()\n", "    params.add('c',    value=1,min=0,max=np.inf)\n", "    params.add('mu',   value=imu,min=0,max=200) #try changing the value of mu, when we are leaving it fixed\n", "    params.add('sigma',value=10,min=0,max=200)\n", "    params.add('amp',  value=20,min=0,max=np.inf)\n", "    params[\"mu\"].vary = False\n", "    result = lmfit.minimize(binnedLikelihood, params, args=(iX,iY,(iY**0.5),iFunc))\n", "    #lmfit.report_fit(result)\n", "    if iPlot:\n", "        #Now we plot it. \n", "        plt.errorbar(iX, iY,np.sqrt(iY), lw=2,fmt=\".k\", capsize=0)\n", "        plt.plot(iX,iFunc(result.params,iX))\n", "        plt.xlabel(\"x\")\n", "        plt.ylabel(\"N\")\n", "        plt.show()\n", "    return result.residual\n", "\n", "def deltaNLL_vary_mu(x,y,imu,iPlot=False):\n", "    LLNull=fitData_vary_mu(x,y,fNull,imu,iPlot)\n", "    LLAlt=fitData_vary_mu(x,y,fAlt,imu,iPlot)\n", "    return 2.*(np.sum(LLAlt*LLAlt)-np.sum(LLNull*LLNull))\n", "\n", "\n", "n_points = 100  # number of points to sample between 60 and 140\n", "imu_values = pass #YOUR CODE HERE (sample n_points between 60 and 140)\n", "\n", "# array to store NLL values\n", "nll_values = pass #YOUR CODE HERE\n", "\n", "# loop over imu values and calculate deltaNLL for each value\n", "# store in nll_values\n", "# e.g., for i, imu in ...\n", "    \n", "\n", "# find the value of imu that minimizes NLL (i.e., maximizes deltaNLL)\n", "imu_max = pass #YOUR CODE HERE\n", "print(\"mu value that maximizes deltaNLL:\", imu_max)\n", "    \n", "# plot NLL vs imu\n", "plt.plot(imu_values, nll_values)\n", "plt.axvline(imu_max, color='r', linestyle='--')\n", "plt.xlabel(\"mu\")\n", "plt.ylabel(\"NLL\")\n", "plt.show()\n"]}, {"cell_type": "markdown", "id": "49620bb6", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_11_4'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L11.4 Probability of Fit</h2>     \n", "\n", "| [Top](#section_11_0) | [Previous Section](#section_11_3) | [Exercises](#exercises_11_4) | [Next Section](#section_11_5) |\n"]}, {"cell_type": "code", "execution_count": null, "id": "1411e708", "metadata": {"tags": ["learner", "py", "learner_chopped", "lect_04"]}, "outputs": [], "source": ["#>>>RUN: L11.4-runcell01\n", "\n", "#HERE WE AGAIN DEFINE THE FUNCTIONS\n", "\n", "def fitData(iX,iY,iFunc,iPlot=False):\n", "    params = lmfit.Parameters()\n", "    params.add('c',    value=1,min=0,max=np.inf)\n", "    params.add('mu',   value=100,min=0,max=200) #try changing the value of mu, when we are leaving it fixed\n", "    params.add('sigma',value=10,min=0,max=200)\n", "    params.add('amp',  value=20,min=0,max=np.inf)\n", "    params[\"mu\"].vary = False\n", "    result = lmfit.minimize(binnedLikelihood, params, args=(iX,iY,(iY**0.5),iFunc))\n", "    #lmfit.report_fit(result)\n", "    if iPlot:\n", "        #Now we plot it. \n", "        plt.errorbar(iX, iY,np.sqrt(iY), lw=2,fmt=\".k\", capsize=0)\n", "        plt.plot(iX,iFunc(result.params,iX))\n", "        plt.xlabel(\"x\")\n", "        plt.ylabel(\"N\")\n", "        plt.show()\n", "    return result.residual\n", "\n", "def deltaNLL(x,y,iPlot=False):\n", "    LLNull=fitData(x,y,fNull,iPlot)\n", "    LLAlt=fitData(x,y,fAlt,iPlot)\n", "    return 2.*(np.sum(LLAlt*LLAlt)-np.sum(LLNull*LLNull))\n", "\n", "\n", "x=bin_centers\n", "NLL = deltaNLL(x,y,True)\n", "print(\"Delta negative Log Likelihood:\",NLL)\n", "\n", "probability = 1-stats.chi2.cdf(NLL, 2)\n", "print(\"Our Probability of this happening\",probability)"]}, {"cell_type": "code", "execution_count": null, "id": "dfe8fddc", "metadata": {"tags": ["learner", "py", "learner_chopped", "lect_04"]}, "outputs": [], "source": ["#>>>RUN: L11.4-runcell02\n", "\n", "np.random.seed(42)\n", "\n", "#First let's make a data sample and fit it\n", "bkg = np.random.uniform(0,200, 300)\n", "sig = np.random.normal (100,15,10) #choose different options for the number of signal events\n", "data = np.append(sig,bkg)\n", "y, bin_edges = np.histogram(data, bins=20)\n", "ys, bin_edges = np.histogram(sig, bins=bin_edges)\n", "yb, bin_edges = np.histogram(bkg, bins=bin_edges)\n", "\n", "bin_centers = 0.5*(bin_edges[1:] + bin_edges[:-1])\n", "plt.errorbar(bin_centers, y,yerr= y**0.5,marker='.',linestyle = 'None', color = 'black')\n", "#plt.errorbar(bin_centers,yb,yerr=ys**0.5,marker='.',linestyle = '-', color = 'red',label='bkg')\n", "#plt.errorbar(bin_centers,ys,yerr=yb**0.5,marker='.',linestyle = '-', color = 'blue',label='signal')\n", "#plt.legend()\n", "plt.xlabel(\"x\")\n", "plt.ylabel(\"N\")\n", "plt.show()\n"]}, {"cell_type": "code", "execution_count": null, "id": "f762894e", "metadata": {"tags": ["learner", "py", "learner_chopped", "lect_04"]}, "outputs": [], "source": ["#>>>RUN: L11.4-runcell03\n", "\n", "x=bin_centers\n", "NLL = deltaNLL(x,y,True)\n", "print(\"Delta negative log-likelihood:\",NLL)\n", "\n", "\n", "probability = 1-stats.chi2.cdf(NLL, 2)\n", "print(\"Our Probability of this happening by chance\",probability)"]}, {"cell_type": "code", "execution_count": null, "id": "8b821363", "metadata": {"tags": ["learner", "py", "learner_chopped", "lect_04"]}, "outputs": [], "source": ["#>>>RUN: L11.4-runcell04\n", "\n", "np.random.seed(42)\n", "\n", "#First let's make a data sample and fit it\n", "bkg = np.random.uniform(0,200, 300)\n", "sig = np.random.normal (100,15,30) #choose different options for the number of signal events\n", "data = np.append(sig,bkg)\n", "y, bin_edges = np.histogram(data, bins=20)\n", "ys, bin_edges = np.histogram(sig, bins=bin_edges)\n", "yb, bin_edges = np.histogram(bkg, bins=bin_edges)\n", "\n", "bin_centers = 0.5*(bin_edges[1:] + bin_edges[:-1])\n", "plt.errorbar(bin_centers, y,yerr= y**0.5,marker='.',linestyle = 'None', color = 'black')\n", "#plt.errorbar(bin_centers,yb,yerr=ys**0.5,marker='.',linestyle = '-', color = 'red',label='bkg')\n", "#plt.errorbar(bin_centers,ys,yerr=yb**0.5,marker='.',linestyle = '-', color = 'blue',label='signal')\n", "#plt.legend()\n", "plt.xlabel(\"x\")\n", "plt.ylabel(\"N\")\n", "plt.show()\n"]}, {"cell_type": "code", "execution_count": null, "id": "15a6fcd1", "metadata": {"tags": ["learner", "py", "learner_chopped", "lect_04"]}, "outputs": [], "source": ["#>>>RUN: L11.4-runcell05\n", "\n", "x=bin_centers\n", "NLL = deltaNLL(x,y,True)\n", "print(\"Delta negative log-likelihood:\",NLL)\n", "\n", "probability = 1-stats.chi2.cdf(NLL, 2)\n", "print(\"Our Probability of this happening by chance\",probability)"]}, {"cell_type": "markdown", "id": "a4d4aa33", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-11.4.1</span>\n", "\n", "Consider your plot from Ex-11.3.2, where you scanned over a range of values for the mean of the bump and calculated the delta log-likelihood.\n", "\n", "Now, define a function to change the y-axis to a pvalue. Complete the following code to define the function `get_prob_NLL(NLL)`, then run the cell to plot the probability as a function of $\\mu$. Note, you will need to have run your solution for Ex-11.3.2 to make this plot."]}, {"cell_type": "code", "execution_count": null, "id": "1cc7c446", "metadata": {"tags": ["draft", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>EXERCISE: L11.4.1\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "\n", "def get_prob_NLL(NLL):\n", "    return #YOUR CODE HERE\n", "\n", "\n", "#process the NLL array to reassign any outliers\n", "def reassgin_outliers(arr):\n", "    for i in range(1, len(arr)-1):\n", "        if abs(arr[i] - arr[i-1]) / arr[i-1] > 0.5:\n", "            arr[i] = (arr[i-1]+arr[i+1])/2.\n", "    return arr\n", "\n", "\n", "nll_values_processed = reassgin_outliers(nll_values)\n", "\n", "# plot prob vs imu\n", "plt.plot(imu_values, get_prob_NLL(nll_values_processed))\n", "plt.axvline(imu_max, color='r', linestyle='--')\n", "plt.xlabel(\"mu\")\n", "plt.ylabel(\"Probability\")\n", "plt.show()\n"]}, {"cell_type": "markdown", "id": "23374f0d", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_11_5'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L11.5 The t-test Statistic</h2>     \n", "\n", "| [Top](#section_11_0) | [Previous Section](#section_11_4) | [Exercises](#exercises_11_5) |\n"]}, {"cell_type": "code", "execution_count": null, "id": "7713bd0e", "metadata": {"tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L11.5-slides\n", "\n", "from IPython.display import IFrame\n", "IFrame(src='https://mitx-8s50.github.io/slides/L12/slides_L12_01.html', width=970, height=550)"]}, {"cell_type": "code", "execution_count": null, "id": "7e648bb4", "metadata": {"tags": ["learner", "py", "lect_05", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L11.5-runcell01\n", "\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "import scipy.stats as stats\n", "\n", "#now let's test if our observation is consistent with a given value of mu\n", "def ttest(isamples,iMu=0):\n", "    t = (isamples.mean()-iMu)/(isamples.std()/np.sqrt(len(isamples)))\n", "    p = 1 - stats.t.cdf(t,df=len(isamples)-1)\n", "    return p\n", "\n", "xvals=[]\n", "pvalues1=[]\n", "pvalues2=[]\n", "pvalues3=[]\n", "for i0 in range(50):\n", "    testsamples1 = np.random.normal(0.2,1, i0)\n", "    pvalue = ttest(testsamples1)\n", "    pvalues1.append(pvalue)\n", "    testsamples2 = np.random.normal(1,1, i0)\n", "    pvalue = ttest(testsamples2)\n", "    pvalues2.append(pvalue)\n", "    testsamples3 = np.random.normal(2,1, i0)\n", "    pvalue = ttest(testsamples3)\n", "    pvalues3.append(pvalue)\n", "    xvals.append(i0)\n", "\n", "fig, ax = plt.subplots(figsize=(9,6))\n", "plt.style.use('fast')\n", "ax.plot(xvals,pvalues1,label=\"$\\mu=0.2$\")\n", "ax.plot(xvals,pvalues2,label=\"$\\mu=1.0$\")\n", "ax.plot(xvals,pvalues3,label=\"$\\mu=2.0$\")\n", "ax.set_xlabel('N')\n", "ax.set_ylabel('p-value')\n", "ax.set_yscale('log')\n", "plt.legend()\n", "plt.show()"]}, {"cell_type": "markdown", "id": "0e6cfce9", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-11.5.1</span>\n", "\n", "Complete the code below, to compare the t-test vs. $\\chi^{2}$ statistic for measuring the similarity of two data sets. These functions should return the p-values for the two tests. Which test is more sensitive?"]}, {"cell_type": "code", "execution_count": null, "id": "7f176b93", "metadata": {"tags": ["draft", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>EXERCISE: L11.5.1\n", "\n", "np.random.seed(42)\n", "\n", "testSamplesA = np.random.normal(0  ,1, 50)\n", "testSamplesB = np.random.normal(0.2,1, 50)\n", "\n", "def chi2test(iSampleB,iSampleA,nbins=5):\n", "    countA, binsA, ignored = plt.hist(iSampleA, nbins,      density=True ,alpha=0.5,label='a')\n", "    countB, binsB, ignored = plt.hist(iSampleB, bins=binsA, density=True,alpha=0.5,label='b')\n", "    chi2=0\n", "    ndof=0\n", "    for i0 in range(len(binsA)-1):\n", "        if countB[i0] > 0 and countA[i0] > 0:\n", "            unc2 = countA[i0]/len(iSampleA) +  countB[i0]/len(iSampleB)\n", "            chi2+=(countB[i0]-countA[i0])**2/unc2\n", "            ndof=ndof+1\n", "    #now we rely on the chi2 distribution to get the probability\n", "    p= #YOUR CODE HERE\n", "    return p\n", "\n", "def ttest_comp(iSamplesA,iSamplesB):\n", "    s2 = (len(iSamplesA)-1)*iSamplesA.std()**2 + (len(iSamplesB)-1)*iSamplesB.std()**2\n", "    s2 = s2/(len(iSamplesA)+ len(iSamplesB) - 2)\n", "    s  = np.sqrt(s2)*(1/len(iSamplesA) + 1/len(iSamplesB))\n", "    t  = ### YOUR CODE HERE\n", "    p  = stats.t.cdf(t,df=len(iSamplesA)+len(iSamplesB)-2)\n", "    return p\n", "\n", "print(\"Chi2\",chi2test(testSamplesA,testSamplesB))\n", "print(\"t-test\",ttest_comp(testSamplesA,testSamplesB))"]}], "metadata": {"celltoolbar": "Tags", "kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}