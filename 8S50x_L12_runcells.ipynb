{"cells": [{"cell_type": "markdown", "id": "701b2911", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<hr style=\"height: 1px;\">\n", "<i>This notebook was authored by the 8.S50x Course Team, Copyright 2022 MIT All Rights Reserved.</i>\n", "<hr style=\"height: 1px;\">\n", "<br>\n", "\n", "<h1>Lesson 12: Hypothesis Testing Part 2</h1>\n"]}, {"cell_type": "markdown", "id": "6fdaa155", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_12_0'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L12.0 Overview</h2>\n"]}, {"cell_type": "markdown", "id": "15c89328", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<h3>Navigation</h3>\n", "\n", "<table style=\"width:100%\">\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_12_1\">L12.1 Looking at Higgs Data</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_12_1\">L12.1 Exercises</a></td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_12_2\">L12.2 Fitting the Higgs Data and Introducing the f-test</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_12_2\">L12.2 Exercises</a></td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_12_3\">L12.3 Computation Using the f-statistic</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_12_3\">L12.3 Exercises</a></td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_12_4\">L12.4 Fitting the Higgs Signal Part 1</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_12_4\">L12.4 Exercises</a></td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_12_5\">L12.5 Fitting the Higgs Signal Part 2</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_12_5\">L12.5 Exercises</a></td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_12_6\">L12.6 Fitting Using Higher Order Polynomials</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_12_6\">L12.6 Exercises</a></td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_12_7\">L12.7 Building Interpolated Distributions</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_12_7\">L12.7 Exercises</a></td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_12_8\">L12.8 An Example Fitting Z Boson Data</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_12_8\">L12.8 Exercises</a></td>\n", "    </tr>\n", "</table>\n"]}, {"cell_type": "markdown", "id": "182f0557", "metadata": {"tags": ["learner", "catsoop_00", "md", "learner_chopped"]}, "source": ["<h3>Data</h3>\n", "\n", ">description: Higgs to di-photon<br>\n", ">source: https://zenodo.org/record/8035284 <br>\n", ">attribution: Harris, Philip based on Staufen, Christian (CMS Collaboration), DOI:10.5281/zenodo.8035284  "]}, {"cell_type": "code", "execution_count": null, "id": "58c4d27c", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L12.0-runcell00\n", "\n", "#If you are in a Google Colab environment, run this cell to import the data for this notebook.\n", "#Otherwise, if you have downloaded the course repository, you do not have to run this cell.\n", "\n", "!git init\n", "!git remote add -f origin https://github.com/mitx-8s50/nb_LEARNER/\n", "!git config core.sparseCheckout true\n", "!echo 'data/L12' >> .git/info/sparse-checkout\n", "!git pull origin main"]}, {"cell_type": "code", "execution_count": null, "id": "f9cc7fd9", "metadata": {"tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L12.0-runcell01\n", "\n", "!pip install george #or use #conda install -c conda-forge george\n", "!pip install lmfit\n"]}, {"cell_type": "code", "execution_count": null, "id": "fcf928c6", "metadata": {"tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L12.0-runcell02\n", "\n", "import numpy as np                #https://numpy.org/doc/stable/ \n", "import lmfit                      #https://lmfit.github.io/lmfit-py/ \n", "import matplotlib.pyplot as plt   #https://matplotlib.org/3.5.3/api/_as_gen/matplotlib.pyplot.html\n", "from scipy import stats           #https://docs.scipy.org/doc/scipy/reference/stats.html\n", "from scipy import interpolate     #https://docs.scipy.org/doc/scipy/reference/interpolate.html\n", "import george                     #https://george.readthedocs.io/en/latest/\n", "from george import kernels        #https://george.readthedocs.io/en/latest/user/kernels"]}, {"cell_type": "code", "execution_count": null, "id": "934b8a32", "metadata": {"tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L12.0-runcell03\n", "\n", "#set plot resolution\n", "%config InlineBackend.figure_format = 'retina'\n", "\n", "#set default figure parameters\n", "plt.rcParams['figure.figsize'] = (9,6)\n", "\n", "medium_size = 12\n", "large_size = 15\n", "\n", "plt.rc('font', size=medium_size)          # default text sizes\n", "plt.rc('xtick', labelsize=medium_size)    # xtick labels\n", "plt.rc('ytick', labelsize=medium_size)    # ytick labels\n", "plt.rc('legend', fontsize=medium_size)    # legend\n", "plt.rc('axes', titlesize=large_size)      # axes title\n", "plt.rc('axes', labelsize=large_size)      # x and y labels\n", "plt.rc('figure', titlesize=large_size)    # figure title\n"]}, {"cell_type": "markdown", "id": "17598e94", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_12_1'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L12.1 Looking at Higgs Data</h2>  \n", "\n", "| [Top](#section_12_0) | [Previous Section](#section_12_0) | [Exercises](#exercises_12_1) | [Next Section](#section_12_2) |"]}, {"cell_type": "code", "execution_count": null, "id": "cd6274bd", "metadata": {"tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L12.1-slides\n", "\n", "from IPython.display import IFrame\n", "IFrame(src='https://mitx-8s50.github.io/slides/L12/slides_L12_02.html', width=970, height=550)"]}, {"cell_type": "code", "execution_count": null, "id": "fa6956c1", "metadata": {"tags": ["learner", "py", "learner_chopped", "lect_01"]}, "outputs": [], "source": ["#>>>RUN: L12.1-runcell01\n", "\n", "import numpy as np\n", "import csv\n", "import matplotlib.pyplot as plt\n", "\n", "#Let's fit a bunch of polynomials with lmfit\n", "x = []\n", "y = []\n", "y_err = []\n", "\n", "#change the filename here\n", "\n", "#2012 data\n", "#filename = 'out.txt'\n", "#filename = 'out2.txt'\n", "#filename = 'out3.txt'\n", "#filename = 'out4.txt'\n", "#filename = 'out5.txt'\n", "\n", "#2011 data\n", "filename = 'out_2011.txt' #TO MATCH THE VIDEO, USE THIS DATA SET\n", "#filename = 'out2_2011.txt'\n", "#filename = 'out3_2011.txt'\n", "#filename = 'out4_2011.txt'\n", "#filename = 'out5_2011.txt'\n", "\n", "\n", "label='data/L12/' + filename\n", "with open(label,'r') as csvfile:\n", "    plots = csv.reader(csvfile, delimiter=' ')\n", "    for row in plots:\n", "        if float(row[1]) > 150 or float(row[1]) < 110:\n", "            continue\n", "        x.append(float(row[1]))\n", "        y.append(float(row[2]))\n", "        #add poisson uncertainties                                                                                                 \n", "        y_err.append(np.sqrt(float(row[2])))\n", "\n", "weights = np.linspace(0.,len(y),num=len(y))\n", "for i0 in range(len(y)):\n", "    weights[i0] = float(1./y_err[i0])\n", "\n", "#Now we plot it. \n", "plt.title(filename)\n", "plt.errorbar(x,y,y_err, lw=2,fmt=\".k\", capsize=0)\n", "plt.xlabel(\"$m_{\\gamma\\gamma}$\")\n", "plt.ylabel(\"$N_{events}$\")\n", "plt.show()"]}, {"cell_type": "markdown", "id": "6e368ce5", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='exercises_12_1'></a>     \n", "\n", "| [Top](#section_12_0) | [Restart Section](#section_12_1) | [Next Section](#section_12_2) |"]}, {"cell_type": "markdown", "id": "fef9d02d", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-12.1.1</span>\n", "\n", "In the code cell above, we plotted 2011 data from the highest purity category. Now, try plotting data from the other categories. Consider the following files, which represent the different categories:\n", "\n", "<pre>\n", "#2012 data\n", "#filename = 'out.txt'\n", "#filename = 'out2.txt'\n", "#filename = 'out3.txt'\n", "#filename = 'out4.txt'\n", "#filename = 'out5.txt'\n", "\n", "#2011 data\n", "filename = 'out_2011.txt' #THIS IS SHOWN IN THE VIDEO\n", "#filename = 'out2_2011.txt'\n", "#filename = 'out3_2011.txt'\n", "#filename = 'out4_2011.txt'\n", "#filename = 'out5_2011.txt'\n", "</pre>\n", "\n", "In general, which of the following statements give accurate characterizations of the data? Select all that apply. Note that the uncertainties in the data are shown by the vertical error bars.\n", "\n", "A) The data from higher number categories has larger uncertainties compared to lower categories.\\\n", "B) The 2011 data has larger uncertainties compared to the 2012 data.\\\n", "C) The data from higher number categories are flatter compared to lower categories.\\\n", "D) There appear to be other features (some resembling bumps) beyond the Higgs mass ($m_{\\gamma\\gamma}\\approx$125) in several of the datasets.\\\n", "E) The data all follow a perfectly smooth falling trend with no suggestions of bumps or other features.\n", "\n", "Hint: You can plot one file at a time by changing which `filename` line is uncommented in the code cell above, OR you can write a function to load all of the files to view them at all at once. You may find the command `plot(axs[0,0],\"data/L12/out.txt\")` useful.\n"]}, {"cell_type": "markdown", "id": "d45ac93b", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_12_2'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L12.2 Fitting Higgs Data and Introducing the f-test</h2>  \n", "\n", "\n", "| [Top](#section_12_0) | [Previous Section](#section_12_1) | [Exercises](#exercises_12_2) | [Next Section](#section_12_3) |"]}, {"cell_type": "code", "execution_count": null, "id": "7054ffa8", "metadata": {"tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L12.2-slides\n", "\n", "from IPython.display import IFrame\n", "IFrame(src='https://mitx-8s50.github.io/slides/L12/slides_L12_03.html', width=970, height=550)"]}, {"cell_type": "code", "execution_count": null, "id": "866f32c8", "metadata": {"tags": ["learner", "py", "learner_chopped", "lect_02"]}, "outputs": [], "source": ["#>>>RUN: L12.2-runcell01\n", "\n", "import lmfit \n", "\n", "#TO MATCH THE VIDEO, USE DATA 'out_2011.txt' \n", "#IF NEEDED, RERUN L12.2-runcell01 WITH THIS FILE\n", "#filename = 'out_2011.txt' \n", "\n", "def pol0(x,p0):\n", "    pols=[p0]\n", "    y = np.polyval(pols,x)\n", "    return y\n", "\n", "def pol1(x,p0,p1):\n", "    pols=[p0,p1]\n", "    y = np.polyval(pols,x)\n", "    return y\n", "\n", "def pol2(x, p0, p1,p2):\n", "    pols=[p0,p1,p2]\n", "    y = np.polyval(pols,x)\n", "    return y\n", "\n", "def pol3(x, p0, p1,p2,p3):\n", "    pols=[p0,p1,p2,p3]\n", "    y = np.polyval(pols,x)\n", "    return y\n", "\n", "def pol4(x, p0, p1,p2,p3,p4):\n", "    pols=[p0,p1,p2,p3,p4]\n", "    y = np.polyval(pols,x)\n", "    return y\n", "\n", "def pol5(x, p0, p1,p2,p3,p4,p5):\n", "    pols=[p0,p1,p2,p3,p4,p5]\n", "    y = np.polyval(pols,x)\n", "    return y\n", "\n", "def fitModel(iX,iY,iWeights,iFunc):\n", "    model  = lmfit.Model(iFunc)\n", "    p = model.make_params(p0=0,p1=0,p2=0,p3=0,p4=0,p5=0)\n", "    result = model.fit(data=iY,params=p,x=iX,weights=iWeights)\n", "    #result = lmfit.minimize(binnedLikelihood, params, args=(iX,iY,(iY**0.5),iFunc))\n", "    output = model.eval(params=result.params,x=iX)\n", "    return output\n", "\n", "result0 = fitModel(x,y,weights,pol0)\n", "result1 = fitModel(x,y,weights,pol1)\n", "result2 = fitModel(x,y,weights,pol2)\n", "result3 = fitModel(x,y,weights,pol3)\n", "result4 = fitModel(x,y,weights,pol4)\n", "result5 = fitModel(x,y,weights,pol5)\n", "\n", "plt.errorbar(x,y,y_err, lw=2,fmt=\".k\", capsize=0,label=\"data\")\n", "plt.plot(x,result0,label=\"pol0\")\n", "plt.plot(x,result1,label=\"pol1\")\n", "plt.plot(x,result2,label=\"pol2\")\n", "plt.plot(x,result3,label=\"pol3\")\n", "plt.plot(x,result4,label=\"pol4\")\n", "plt.plot(x,result5,label=\"pol5\")\n", "plt.xlabel(\"$m_{\\gamma\\gamma}$\")\n", "plt.ylabel(\"$N_{events}$\")\n", "plt.legend()\n", "plt.show()\n", "\n", "#res0.plot()\n", "#result1.plot()\n", "#result2.plot()\n", "#result3.plot()\n", "#result4.plot()\n", "#result5.plot()"]}, {"cell_type": "code", "execution_count": null, "id": "9610bf9b", "metadata": {"tags": ["learner", "py", "learner_chopped", "lect_02"]}, "outputs": [], "source": ["#>>>RUN: L12.2-runcell02\n", "\n", "plt.errorbar(x,y,y_err, lw=2,fmt=\".k\", capsize=0,label=\"data\")\n", "plt.plot(x,result5,label=\"pol5\")\n", "plt.xlabel(\"$m_{\\gamma\\gamma}$\")\n", "plt.ylabel(\"$N_{events}$\")\n", "plt.legend()\n", "plt.show()"]}, {"cell_type": "code", "execution_count": null, "id": "7e513c9f", "metadata": {"tags": ["learner", "py", "learner_chopped", "lect_02"]}, "outputs": [], "source": ["#>>>RUN: L12.2-runcell03\n", "\n", "def residual(iY,iFunc,iYErr):\n", "    resid = (iY-iFunc)/iYErr\n", "    tmp_vals, tmp_bin_edges = np.histogram(resid, bins=10,range=[-7,7])\n", "    tmp_bin_centers = 0.5*(tmp_bin_edges[1:] + tmp_bin_edges[:-1])\n", "    print(\"Mean:\",resid.mean(),\"\\tSTD:\",resid.std())\n", "    return tmp_bin_centers,tmp_vals\n", "\n", "delta_p0,delta_y0 = residual(y,result0,y_err)\n", "delta_p1,delta_y1 = residual(y,result1,y_err)\n", "delta_p5,delta_y5 = residual(y,result5,y_err)\n", "plt.errorbar(delta_p0,delta_y0,yerr=delta_y0**0.5,label=\"pol0\",marker='.',drawstyle = 'steps-mid')\n", "plt.errorbar(delta_p1,delta_y1,yerr=delta_y1**0.5,label=\"pol1\",marker='.',drawstyle = 'steps-mid')\n", "plt.errorbar(delta_p5,delta_y5,yerr=delta_y5**0.5,label=\"pol5\",marker='.',drawstyle = 'steps-mid')\n", "plt.legend()\n", "plt.show()"]}, {"cell_type": "markdown", "id": "38855a67", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-12.2.1</span>\n", "\n", "The f-function can be calculated in python using `stats.f.pdf(x,d1,d2)`, where `d1` and `d2` denote the degrees of freedom for the numerator (`dfn` in the code) and denominator (`dfd`), respectively. Run the code below to plot the f-distribution and vary the degrees of freedom in order to answer the question below. Note that \"positively skewed\" means a distribution that has a long tail extending out to larger values.\n", "\n", "Which of the following statements accurately describes the characteristics of the f-distribution as the degrees of freedom are varied? Select all that apply:\n", "\n", "A) The f-distribution is always symmetric, regardless of the degrees of freedom.\\\n", "B) The f-distribution is positively skewed when the degrees of freedom are small.\\\n", "C) The shape of the f-distribution becomes more symmetric when both the numerator and denominator degrees of freedom increase.\\\n", "D) The f-distribution becomes more concentrated around 0 as the degrees of freedom increase.\\\n", "E) The shape of the f-distribution is not affected by the degrees of freedom.\n", "\n"]}, {"cell_type": "code", "execution_count": null, "id": "a88dba52", "metadata": {"tags": ["py", "draft", "learner_chopped"]}, "outputs": [], "source": ["#>>>EXERCISE: L12.2.1\n", "\n", "#change these values to plot different degrees of freedom\n", "#degrees of freedom in numerator/denominator\n", "dfn, dfd = 5, 20  \n", "\n", "fig, ax = plt.subplots(figsize=(8, 4))\n", "\n", "x_vals = np.linspace(0, 4, 500)\n", "y_vals = stats.f.pdf(x_vals, dfn, dfd)\n", "\n", "ax.plot(x_vals, y_vals, 'r-', lw=2, alpha=0.6, label='f pdf')\n", "\n", "#plot area corresponding to p-value < 0.05\n", "plt.fill_between(x_vals, 0, y_vals, where=(x_vals >= stats.f.ppf(0.95, dfn, dfd)), \n", "                 color='red', alpha=0.2, label = '0.05 > p' )\n", "\n", "plt.xlim(0,4)\n", "plt.ylim(0,)\n", "\n", "plt.legend()\n", "plt.title('f-distribution (dfn=' + str(dfn) + ', dfd=' + str(dfd) + ')')\n", "plt.xlabel('f value')\n", "plt.ylabel('Probability density')\n", "plt.show()"]}, {"cell_type": "markdown", "id": "cafa1def", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-12.2.2</span>\n", "\n", "Let's think more about the degrees of freedom, while considering the case where we are comparing the fits of two different functions to a set of data. For instance, consider 100 data points that are fit by a zeroth-order polynomial (with one degree of freedom), compared to a first-order polynomial (with two degrees of freedom). What are the corresponding degrees of freedom in the numerator, `dfn`, and denominator, `dfd`, of the f-test? Enter your answer as a list of two numbers: `[dfn, dfd]`"]}, {"cell_type": "markdown", "id": "c6ceb172", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-12.2.3</span>\n", "\n", "Now we will use the f-test to compare two polynomial fits to a set of data (as we will do in the next section). Run the code in your notebook (not shown here), which does the following:\n", "\n", "The code first generates random data, `x_e` and `y_e`, with a slight positive correlation of 0.5. Then it defines a function `fitModel_e` for fitting a polynomial to the data using `lmfit`. It computes the sum of squared residuals using `residual2_e`, then performs an f-test using the function `ftest_e`. Finally, it calculates a p-value and plots the data and the corresponding f-distribution.\n", "\n", "Your specific objective will be to compare the fits of a zeroth-order polynomial and first-order polynomial. Run the code, and try increasing the number of data points that are fit (the default is `data_len = 10`). Also try varying the correlation coefficient. Which of the following best describes the results (select all that apply):\n", "\n", "A) As we increase the number of points that we sample, the p-value decreases, indicating that it is increasingly unlikely that the first-order polynomial is a better fit by chance alone.\\\n", "B) As we increase the number of points that we sample, the correlation between the data becomes more apparent, and the first-order polynomial fit begins to perform better than the zeroth order fit.\\\n", "C) As we increase the correlation coefficient for a fixed number of data points, the p-value decreases because the first-order fit performs better than the zeroth order fit.\\\n", "D) Even if the correlation coefficient is very small, the f-test will always show that the first-order polynomial is a better fit, as long as we sample enough data points.\\\n", "E) The degrees of freedom in the numerator is always 1 when we are comparing two adjacent polynomials."]}, {"cell_type": "code", "execution_count": null, "id": "ae25e326", "metadata": {"tags": ["py", "draft", "learner_chopped"]}, "outputs": [], "source": ["#>>>EXERCISE: L12.2.3\n", "\n", "import lmfit\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "\n", "# Generate random data with slight positive slope\n", "#------------------------------------------------\n", "np.random.seed(0)\n", "data_len = 10\n", "x_e = np.random.random(size=data_len)\n", "y_e = 0.5 * x_e + np.random.normal(loc=0, scale=0.5, size=data_len)\n", "\n", "\n", "# Here we define a fit function and fit the polynomials\n", "#------------------------------------------------------ \n", "def fitModel_e(iX,iY,iFunc):\n", "    model  = lmfit.Model(iFunc)\n", "    p = model.make_params(p0=0,p1=0,p2=0,p3=0,p4=0,p5=0)\n", "    result = model.fit(data=iY,params=p,x=iX)\n", "    #result = lmfit.minimize(binnedLikelihood, params, args=(iX,iY,(iY**0.5),iFunc))\n", "    output = model.eval(params=result.params,x=iX)\n", "    return output\n", "\n", "#polynomial 1 (zeroth order)\n", "ndof1_e = 1\n", "result0_e = fitModel_e(x_e,y_e,pol0)\n", "\n", "#polynomial 2 (first order)\n", "ndof2_e = 2\n", "result1_e = fitModel_e(x_e,y_e,pol1)\n", "\n", "\n", "# Here we define the residuals and f-test\n", "#----------------------------------------\n", "#returns sum of the squared residuals\n", "def residual2_e(iY,iFunc):\n", "    residval = (iY-iFunc)\n", "    return np.sum(residval**2)\n", "    \n", "def ftest_e(iY,f1,f2,indof1,indof2):\n", "    r1=residual2_e(iY,f1)\n", "    r2=residual2_e(iY,f2)\n", "    sigma2group=(r1-r2)/(indof2-indof1)\n", "    sigma2=r2/(len(iY)-indof2)\n", "    return sigma2group/sigma2\n", "\n", "\n", "# Calculate the F-statistic and p-value\n", "#--------------------------------------\n", "#the number of degrees of freedom used for the numerotor is (ndof2-ndof1)\n", "#the number of degrees of freedom used for the denominator is (len(x_e)-ndof2)\n", "dfn_e = ndof2_e-ndof1_e\n", "dfd_e = len(x_e)-ndof2_e\n", "f10_e=ftest_e(y_e,result0_e,result1_e,ndof1_e,ndof2_e)\n", "p_value_e = 1 - stats.f.cdf(f10_e, dfn=dfn_e, dfd=dfd_e)\n", "\n", "print('dfn, dfd:', dfn_e, dfd_e)\n", "print('F-statistic:', f10_e)\n", "print('p-value:', p_value_e)\n", "\n", "\n", "# Plots \n", "#--------------------------------\n", "# Plot the data and fit functions\n", "plt.scatter(x_e, y_e, color='blue', label='Data')\n", "plt.plot(x_e, result0_e, color='red', label='Zeroth Order Fit')\n", "plt.plot(x_e, result1_e, color='green', label='First Order Fit')\n", "plt.xlabel('x')\n", "plt.ylabel('y')\n", "plt.title('Polynomial Fit Comparison')\n", "plt.legend()\n", "plt.show()\n", "\n", "\n", "#Let's see what the f-distribution looks like\n", "x_vals_e = np.linspace(0, 10, 500)\n", "y_vals_e = stats.f.pdf(x_vals_e, dfn_e, dfd_e)\n", "\n", "plt.axvline(x=f10_e, color='black', linestyle='--', label='f-stat')\n", "plt.plot(x_vals_e, y_vals_e, 'r-', lw=2, alpha=0.6, label='f pdf')\n", "\n", "#plot area corresponding to p-value < 0.05\n", "plt.fill_between(x_vals_e, 0, y_vals_e, where=(x_vals_e >= stats.f.ppf((1-0.05), dfn_e, dfd_e)), \n", "                 color='red', alpha=0.2, label = '0.05 > p' )\n", "\n", "plt.xlim(0,10)\n", "plt.ylim(1e-3,)\n", "plt.yscale('log')\n", "plt.legend()\n", "plt.title('f-distribution (dfn=' + str(dfn_e) + ', dfd=' + str(dfd_e) + ')')\n", "plt.xlabel('f value')\n", "plt.ylabel('Probability density')\n", "plt.show()\n"]}, {"cell_type": "markdown", "id": "484e04db", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_12_3'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L12.3 Computation Using the f-statistic</h2>  \n", "\n", "| [Top](#section_12_0) | [Previous Section](#section_12_2) | [Exercises](#exercises_12_3) | [Next Section](#section_12_4) |"]}, {"cell_type": "code", "execution_count": null, "id": "e521b001", "metadata": {"tags": ["learner", "py", "learner_chopped", "lect_03"]}, "outputs": [], "source": ["#>>>RUN: L12.3-runcell01\n", "\n", "import scipy.stats as stats \n", "\n", "#TO MATCH THE VIDEO, USE DATA 'out_2011.txt' \n", "#IF NEEDED, RERUN L12.2-runcell01 WITH THIS FILE\n", "#filename = 'out_2011.txt' \n", "\n", "def residual2(iY,iFunc,iYErr):\n", "    residval = (iY-iFunc)\n", "    return np.sum(residval**2)\n", "    \n", "def ftest(iY,iYerr,f1,f2,ndof1,ndof2):\n", "    r1=residual2(iY,f1,iYerr)\n", "    r2=residual2(iY,f2,iYerr)\n", "    sigma2group=(r1-r2)/(ndof2-ndof1)\n", "    sigma2=r2/(len(iY)-ndof2)\n", "    return sigma2group/sigma2\n", "\n", "f10=ftest(y,y_err,result0,result1,1,2)\n", "f21=ftest(y,y_err,result1,result2,2,3)\n", "f32=ftest(y,y_err,result2,result3,3,4)\n", "f43=ftest(y,y_err,result3,result4,4,5)\n", "f54=ftest(y,y_err,result4,result5,5,6)\n", "\n", "xrange=np.linspace(0,300,100)\n", "farr=1-stats.f.cdf(xrange,1,len(y)-5) #number of bins - 5 floating parameters\n", "fig, ax = plt.subplots(figsize=(9,6))\n", "\n", "ax.axvline(x=f10,linewidth=3,c='b',label='0 to 1')\n", "ax.axvline(x=f21,linewidth=3,c='g',label='1 to 2')\n", "ax.axvline(x=f32,linewidth=3,c='purple',label='2 to 3')\n", "ax.axvline(x=f43,linewidth=3,c='yellow',label='3 to 4')\n", "ax.axvline(x=f54,linewidth=3,c='orange',label='4 to 5')\n", "\n", "ax.set_yscale('log')\n", "plt.plot(xrange,farr,label='f(1,N)')\n", "plt.legend()\n", "plt.xlabel('f-statistic')\n", "plt.ylabel('p-value')\n", "plt.show()\n", "\n", "xrange=np.linspace(0,3,100)\n", "farr=1-stats.f.cdf(xrange,1,len(y)-5) \n", "fig, ax = plt.subplots(figsize=(9,6))\n", "ax.axvline(x=f32,linewidth=3,c='purple',label='2 to 3')\n", "ax.axvline(x=f43,linewidth=3,c='yellow',label='3 to 4')\n", "ax.axvline(x=f54,linewidth=3,c='orange',label='4 to 5')\n", "ax.set_yscale('log')\n", "plt.xlabel('f-statistic')\n", "plt.plot(xrange,farr,label='f(1,N)')\n", "plt.ylabel('p-value')\n", "plt.legend()\n", "plt.show()\n", "\n", "\n", "#PERFORM F-TEST BELOW\n", "#print f-stat and p-value\n", "f10=ftest(y,y_err,result0,result1,1,2)\n", "p_val10 = 1 - stats.f.cdf(f10, dfn=1, dfd=len(y)-2)\n", "print('f-stat 0 to 1:', f10)\n", "print('p-value 1 better than 0 by chance:', p_val10)\n", "print()\n", "f21=ftest(y,y_err,result1,result2,2,3)\n", "p_val21 = 1 - stats.f.cdf(f21, dfn=1, dfd=len(y)-3)\n", "print('f-stat 1 to 2:', f21)\n", "print('p-value 2 better than 1 by chance:', p_val21)\n", "print()\n", "f32=ftest(y,y_err,result2,result3,3,4)\n", "p_val32 = 1 - stats.f.cdf(f32, dfn=1, dfd=len(y)-4)\n", "print('f-stat 2 to 3:', f32)\n", "print('p-value 3 better than 2 by chance:', p_val32)\n", "print()\n", "f43=ftest(y,y_err,result3,result4,4,5)\n", "p_val43 = 1 - stats.f.cdf(f43, dfn=1, dfd=len(y)-5)\n", "print('f-stat 3 to 4:', f43)\n", "print('p-value 4 better than 3 by chance:', p_val43)\n", "print()\n", "f54=ftest(y,y_err,result4,result5,5,6)\n", "p_val54 = 1 - stats.f.cdf(f54, dfn=1, dfd=len(y)-6)\n", "print('f-stat 4 to 5:', f54)\n", "print('p-value 5 better than 4 by chance:', p_val54)"]}, {"cell_type": "code", "execution_count": null, "id": "b88e7cdd", "metadata": {"tags": ["learner", "py", "learner_chopped", "lect_03"]}, "outputs": [], "source": ["#>>>RUN: L12.3-runcell02\n", "\n", "def chi2(iY,iFunc,iYErr,iNDOF):\n", "    resid = (iY-iFunc)/iYErr\n", "    chi2value = np.sum(resid**2)\n", "    print(\"Mean:\",resid.mean(),\"\\tSTD:\",resid.std())\n", "    chi2prob=1-stats.chi2.cdf(chi2value,len(iY)-iNDOF)\n", "    print(\"chi2 prob:\",chi2prob)\n", "    return chi2value/(len(iY)-iNDOF)\n", "\n", "chi2value=chi2(y,result2,y_err,3) #chisquare of a 2nd order polynomial (3 floating parameters)\n", "print(\"Normalized chi2:\",chi2value)\n", "\n", "plt.errorbar(x,y,y_err, lw=2,fmt=\".k\", capsize=0,label=\"data\")\n", "plt.plot(x,result2,label=\"pol2\")\n", "plt.xlabel(\"$m_{\\gamma\\gamma}$\")\n", "plt.ylabel(\"$N_{events}$\")\n", "plt.legend()\n", "plt.show()"]}, {"cell_type": "code", "execution_count": null, "id": "40510734", "metadata": {"tags": ["learner", "py", "learner_chopped", "lect_03"]}, "outputs": [], "source": ["#>>>RUN: L12.3-runcell03\n", "\n", "print('chisq 1st order fit')\n", "print(\"Normalized chi2:\",chi2(y,result1,y_err,2))\n", "print()\n", "\n", "print('chisq 2nd order fit')\n", "print(\"Normalized chi2:\",chi2(y,result2,y_err,3))\n", "print()\n", "\n", "print('chisq 3rd order fit')\n", "print(\"Normalized chi2:\",chi2(y,result3,y_err,4))\n", "print()"]}, {"cell_type": "markdown", "id": "9550ec9b", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='exercises_12_3'></a>     \n", "\n", "| [Top](#section_12_0) | [Restart Section](#section_12_3) | [Next Section](#section_12_4) |\n"]}, {"cell_type": "markdown", "id": "3250b4d1", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-12.3.1</span>\n", "\n", "The p-value for the f-distribution that compares the 0th order polynomial (i.e. a flat line) to the 1st order polynomial (i.e. a linear slope) is smaller than $10^{-100}$. Which of the following statements best characterizes how to interpret this:\n", "\n", "A) There is a $10^{-100}$ probability that the 0th order model provides a better fit to the data purely by chance.\\\n", "B) There is a $10^{-100}$ probability that the 1st order model provides a better fit to the data purely by chance.\\\n", "C) A 0th degree polynomial is a $10^{100}$ times better fit to the data than a 1st degree polynomial.\\\n", "D) A 1st degree polynomial is a $10^{100}$ times better fit to the data than a 0th degree polynomial.\\\n", "E) None of the above."]}, {"cell_type": "markdown", "id": "0eed6d01", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-12.3.2</span>\n", "\n", "Complete the code below to randomly sample data from a Gaussian distribution and fit with various polynomials, then compute the f-test to compare the polynomial fits to this Gaussian data.\n", "\n", "What is the order of the polynomial at which the f-test first tells you to stop? In other words, what is the lowest order polynomial that the f-test indicates is a good fit? Hint: the f-test comparing the next higher order polynomial will not show a significant improvement (if any).\n", "\n", "Consider the options below:\n", "\n", "A) 0\\\n", "B) 1\\\n", "C) 2\\\n", "D) 3\\\n", "E) 4\\\n", "F) 5\n"]}, {"cell_type": "code", "execution_count": null, "id": "8b097693", "metadata": {"tags": ["draft", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>EXERCISE: L12.3.2\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "np.random.seed(10)\n", "gausrandom = np.random.normal(0,10,1000)\n", "y_test,bin_edges = np.histogram(gausrandom,bins=30,range=(-30,30))\n", "x_test = 0.5*(bin_edges[1:] + bin_edges[:-1])\n", "\n", "x_test = x_test[y_test > 0]\n", "y_test = y_test[y_test > 0]\n", "y_test_err = np.sqrt(y_test)\n", "weights_test = 1./np.sqrt(y_test)\n", "\n", "result0 = fitModel(x_test,y_test,weights_test,pol0)\n", "result1 = fitModel(x_test,y_test,weights_test,pol1)\n", "result2 = fitModel(x_test,y_test,weights_test,pol2)\n", "result3 = fitModel(x_test,y_test,weights_test,pol3)\n", "result4 = fitModel(x_test,y_test,weights_test,pol4)\n", "result5 = fitModel(x_test,y_test,weights_test,pol5)\n", "\n", "plt.errorbar(x_test,y_test,y_test_err, lw=2,fmt=\".k\", capsize=0,label=\"data\")\n", "plt.plot(x_test,result0_test,label=\"pol0\")\n", "plt.plot(x_test,result1_test,label=\"pol1\")\n", "plt.plot(x_test,result2_test,label=\"pol2\")\n", "plt.plot(x_test,result3_test,label=\"pol3\")\n", "plt.plot(x_test,result4_test,label=\"pol4\")\n", "plt.plot(x_test,result5_test,label=\"pol5\")\n", "plt.xlabel(\"$m_{\\gamma\\gamma}$\")\n", "plt.ylabel(\"$N_{events}$\")\n", "plt.legend()\n", "plt.show()\n", "\n", "#PERFORM F-TEST BELOW\n", "#YOUR CODE HERE"]}, {"cell_type": "markdown", "id": "eec978ba", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-12.3.3</span>\n", "\n", "Now compute the $\\chi^{2}$ of all of your fits. What do you conclude? Which order polynomial fit would you use? Choose from the options below:\n", "\n", "A) 0\\\n", "B) 1\\\n", "C) 2\\\n", "D) 3\\\n", "E) 4\\\n", "F) 5\\\n", "G) none of the above\n"]}, {"cell_type": "code", "execution_count": null, "id": "3e3c5cc0", "metadata": {"tags": ["draft", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>EXERCISE: L12.3.3\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "###test the chi2"]}, {"cell_type": "markdown", "id": "c58fe9f9", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_12_4'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L12.4 Fitting the Higgs Signal Part 1</h2>  \n", "\n", "| [Top](#section_12_0) | [Previous Section](#section_12_3) | [Exercises](#exercises_12_4) | [Next Section](#section_12_5) |"]}, {"cell_type": "code", "execution_count": null, "id": "739ed23b", "metadata": {"tags": ["learner", "py", "learner_chopped", "lect_04"]}, "outputs": [], "source": ["#>>>RUN: L12.4-runcell01\n", "\n", "def load(iLabel,iRange=False):\n", "    x = np.array([])\n", "    y = np.array([])\n", "    label=iLabel\n", "    with open(label,'r') as csvfile:\n", "        plots = csv.reader(csvfile, delimiter=' ')\n", "        for row in plots:\n", "            if not iRange and (float(row[1]) > 150 or float(row[1]) < 110):\n", "                continue\n", "            x = np.append(x,float(row[1]))\n", "            y = np.append(y,float(row[2]))\n", "            #add poisson uncertainties                                                                                                 \n", "    weights = 1./y**0.5 \n", "    return x,y,y**0.5,weights\n", "\n", "def plot(ax,iLabel):\n", "    x,y,y_err,weights=load(iLabel)\n", "    #Now we plot it. \n", "    ax.errorbar(x,y,y_err, lw=2,fmt=\".k\", capsize=0,label=iLabel)\n", "    #ax.x_label(\"$m_{\\gamma\\gamma}$\")\n", "    #ax.y_label(\"$N_{events}$\")\n", "    ax.legend()\n", "    #ax.show()\n", "    \n", "fig, axs = plt.subplots(2, 3, figsize=(12,8))\n", "#2012 data    \n", "plot(axs[0,0],\"data/L12/out.txt\")\n", "plot(axs[0,1],\"data/L12/out2.txt\")\n", "plot(axs[0,2],\"data/L12/out3.txt\")\n", "plot(axs[1,0],\"data/L12/out4.txt\")\n", "plot(axs[1,1],\"data/L12/out5.txt\")\n", "plt.show()\n", "\n", "fig, axs = plt.subplots(2, 3, figsize=(12,8))\n", "#2011 data    \n", "plot(axs[0,0],\"data/L12/out_2011.txt\")\n", "plot(axs[0,1],\"data/L12/out2_2011.txt\")\n", "plot(axs[0,2],\"data/L12/out3_2011.txt\")\n", "plot(axs[1,0],\"data/L12/out4_2011.txt\")\n", "plot(axs[1,1],\"data/L12/out5_2011.txt\")\n"]}, {"cell_type": "code", "execution_count": null, "id": "ec254f2f", "metadata": {"tags": ["learner", "py", "learner_chopped", "lect_04"]}, "outputs": [], "source": ["#>>>RUN: L12.4-runcell02\n", "\n", "def fitAll(iLabel,iPlot=False):\n", "    x,y,y_err,weights=load(iLabel)\n", "    result0 = fitModel(x,y,weights,pol0)\n", "    result1 = fitModel(x,y,weights,pol1)\n", "    result2 = fitModel(x,y,weights,pol2)\n", "    result3 = fitModel(x,y,weights,pol3)\n", "    result4 = fitModel(x,y,weights,pol4)\n", "    result5 = fitModel(x,y,weights,pol5)\n", "\n", "    if iPlot:\n", "        plt.errorbar(x,y,y_err, lw=2,fmt=\".k\", capsize=0,label=\"data\")\n", "        plt.plot(x,result0,label=\"pol0\")\n", "        plt.plot(x,result1,label=\"pol1\")\n", "        plt.plot(x,result2,label=\"pol2\")\n", "        plt.plot(x,result3,label=\"pol3\")\n", "        plt.plot(x,result4,label=\"pol4\")\n", "        plt.plot(x,result5,label=\"pol5\")\n", "        plt.xlabel(\"$m_{\\gamma\\gamma}$\")\n", "        plt.ylabel(\"$N_{events}$\")\n", "        plt.legend()\n", "        plt.show()\n", "    return x,y,y_err,result0,result1,result2,result3,result4,result5\n", "\n", "def ftestAll(iLabel):\n", "    x,y,y_err,result0,result1,result2,result3,result4,result5=fitAll(iLabel)\n", "    f10=ftest(y,y_err,result0,result1,1,2)\n", "    f21=ftest(y,y_err,result1,result2,2,3)\n", "    f32=ftest(y,y_err,result2,result3,3,4)\n", "    f43=ftest(y,y_err,result3,result4,4,5)\n", "    f54=ftest(y,y_err,result4,result5,4,5)\n", "    print(\"f 1 to 0:\",1-stats.f.cdf(f10,1,len(y)-1))\n", "    print(\"f 2 to 1:\",1-stats.f.cdf(f21,1,len(y)-2))\n", "    print(\"f 3 to 2:\",1-stats.f.cdf(f32,1,len(y)-3))\n", "    print(\"f 4 to 3:\",1-stats.f.cdf(f43,1,len(y)-4))\n", "    print(\"f 5 to 4:\",1-stats.f.cdf(f54,1,len(y)-5))\n", "    print()\n", "    \n", "print('2012 Data: Category 1')\n", "fitAll(\"data/L12/out.txt\",True)\n", "ftestAll(\"data/L12/out.txt\")\n", "\n", "print('2012 Data: Category 2')\n", "fitAll(\"data/L12/out2.txt\",True)\n", "ftestAll(\"data/L12/out2.txt\")"]}, {"cell_type": "code", "execution_count": null, "id": "986f7a10", "metadata": {"scrolled": false, "tags": ["learner", "py", "learner_chopped", "lect_04"]}, "outputs": [], "source": ["#>>>RUN: L12.4-runcell03\n", "\n", "def sigpol3(x,p0,p1,p2,p3,p4,amp,mass,sigma):\n", "    bkg=pol3(x,p0,p1,p2,p3)\n", "    sig=amp*stats.norm.pdf(x,mass,sigma)\n", "    return sig+bkg\n", "\n", "def sigpol4(x,p0,p1,p2,p3,p4,amp,mass,sigma):\n", "    bkg=pol4(x,p0,p1,p2,p3,p4)\n", "    sig=amp*stats.norm.pdf(x,mass,sigma)\n", "    return sig+bkg\n", "\n", "def sigpol5(x,p0,p1,p2,p3,p4,p5,amp,mass,sigma):\n", "    bkg=pol5(x,p0,p1,p2,p3,p4,p5)\n", "    sig=amp*stats.norm.pdf(x,mass,sigma)\n", "    return sig+bkg\n", "\n", "#this function uses fixed iM\n", "def fitModel_new(iX,iY,iWeights,iM,iFunc):\n", "    model  = lmfit.Model(iFunc)\n", "    p = model.make_params(p0=0,p1=0,p2=0,p3=0,p4=0,p5=0,amp=0,mass=iM,sigma=1.2)\n", "    try:\n", "        p[\"mass\"].vary=False\n", "        p[\"sigma\"].vary=False\n", "    except:\n", "      a=1\n", "      #print(\"Mass and Sigma not in fit\")\n", "    result = model.fit(data=iY,params=p,x=iX,weights=iWeights)\n", "    output = model.eval(params=result.params,x=iX)\n", "    return output,result.residual\n", "\n", "def fitSig(iLabel,iM,SBfunc,Bfunc,iPlot=False):\n", "    x,y,y_err,weights=load(iLabel)\n", "    resultSB,likeSB=fitModel_new(x,y,weights,iM,SBfunc)\n", "    resultB, likeB =fitModel_new(x,y,weights,iM,Bfunc)\n", "    if iPlot:\n", "        plt.errorbar(x,y,y_err, lw=2,fmt=\".k\", capsize=0,label=\"data\")\n", "        plt.plot(x,resultSB,label=\"S+B\")\n", "        plt.plot(x,resultB, label=\"B\")\n", "        plt.xlabel(\"$m_{\\gamma\\gamma}$\")\n", "        plt.ylabel(\"$N_{events}$\")\n", "        plt.legend()\n", "        plt.show()\n", "    return np.sum(likeB**2)-np.sum(likeSB**2)\n", "\n", "NLL=fitSig(\"data/L12/out.txt\",125,sigpol4,pol4,True)\n", "print(\"out.txt  2NLL:\",NLL,\"p-value\",1-stats.chi2.cdf(NLL,1))\n", "\n", "NLL=fitSig(\"data/L12/out2.txt\",125,sigpol4,pol4,True)\n", "print(\"out2.txt 2NLL:\",NLL,\"p-value\",1-stats.chi2.cdf(NLL,1))\n", "\n", "NLL=fitSig(\"data/L12/out3.txt\",125,sigpol4,pol4,True)\n", "print(\"out3.txt 2NLL:\",NLL,\"p-value\",1-stats.chi2.cdf(NLL,1))\n", "\n", "NLL=fitSig(\"data/L12/out4.txt\",125,sigpol4,pol4,True)\n", "print(\"out4.txt 2NLL:\",NLL,\"p-value\",1-stats.chi2.cdf(NLL,1))\n", "\n", "NLL=fitSig(\"data/L12/out5.txt\",125,sigpol4,pol4,True)\n", "print(\"out5.txt 2NLL:\",NLL,\"p-value\",1-stats.chi2.cdf(NLL,1))"]}, {"cell_type": "markdown", "id": "372b69dd", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='exercises_12_4'></a>     \n", "\n", "| [Top](#section_12_0) | [Restart Section](#section_12_4) | [Next Section](#section_12_5) |"]}, {"cell_type": "markdown", "id": "1cd07257", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-12.4.1</span>\n", "\n", "Which of the following statements accurately describes how to interpret the p-value in the above calculation? Select all the apply:\n", "\n", "A) The p-value represents the probability that a random fluctuation in the data enables us to fit a peak plus background.\\\n", "B) The p-value represents the probability that a peak plus background is the best fit.\\\n", "C) The p-value represents the probability that the data are best modeled by a background fit only.\\\n", "D) The p-value represents the probability of realizing a particular negative log-likelihood ratio, based on a chi-squared distrubiton.\\\n", "E) None of the above.\n"]}, {"cell_type": "markdown", "id": "ed234ff7", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-12.4.2</span>\n", "\n", "Which category shows the most significant evidence for the Higgs signal?\n", "\n", "A) dataset 1: out.txt\\\n", "B) dataset 2: out2.txt\\\n", "C) dataset 3: out3.txt\\\n", "D) dataset 4: out4.txt\\\n", "E) dataset 5: out5.txt\n", "\n"]}, {"cell_type": "markdown", "id": "7a594d65", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-12.4.3</span>\n", "\n", "When physicists searched for the Higgs boson, they did this blind. That means they did not look at the data. However, they did do f-tests, and even $\\chi^{2}$ goodness of fit tests. Knowing that the f-test for the dataset `out.txt` with a 4th order polynomial background is good, complete the code below to compute the normalized $\\chi^{2}$ for that fit (background only). Report the value as a number with precision 1e-3.\n"]}, {"cell_type": "code", "execution_count": null, "id": "e0a8abd5", "metadata": {"tags": ["py", "learner_chopped", "draft"]}, "outputs": [], "source": ["#>>>EXERCISE: L12.4.3\n", "\n", "def fitModel_background(iX,iY,iWeights,iFunc):\n", "    model  = lmfit.Model(iFunc)\n", "    p = model.make_params(p0=0,p1=0,p2=0,p3=0,p4=0,p5=0)\n", "    result = model.fit(data=iY,params=p,x=iX,weights=iWeights)\n", "    output = model.eval(params=result.params,x=iX)\n", "    return output\n", "\n", "def fitAll_background(iLabel):\n", "    x,y,y_err,weights=load(iLabel)\n", "    result4_background = fitModel_background(x,y,weights,pol4)\n", "    return x,y,y_err,result4_background\n", "\n", "\n", "def chi2testAll_background(iLabel):\n", "    #YOUR CODE HERE\n", "    return\n", "\n", "chi2testAll_background(\"data/L12/out.txt\")"]}, {"cell_type": "markdown", "id": "661710d1", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-12.4.4</span>\n", "\n", "As mentioned above, when physicists searched for the Higgs boson, they performed a blind analysis and did not look at the data. However, even looking only at the significance of the fits, and not the data, could ruin the blind analysis. Which of the following describes how analyzing chi-squared fit values could result in unblinding?\n", "\n", "A) If the chi-squared values for the background-only fit are large in some region, this could indicate that a strong signal is present.\\\n", "B) If the chi-squared values for both the signal-plus-background fit and the background-only fit are both small, then there is definitely no signal present.\\\n", "C) We can always analyze the significance of background-only fits because the influence of the signal will not be present.\n"]}, {"cell_type": "markdown", "id": "693a60a3", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_12_5'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L12.5 Fitting the Higgs Signal Part 2</h2>  \n", "\n", "| [Top](#section_12_0) | [Previous Section](#section_12_4) | [Exercises](#exercises_12_5) | [Next Section](#section_12_6) |"]}, {"cell_type": "code", "execution_count": null, "id": "e1b1a24f", "metadata": {"tags": ["learner", "py", "learner_chopped", "lect_05"]}, "outputs": [], "source": ["#>>>RUN: L12.5-runcell01\n", "\n", "def pvalueCalc(iLabel,pMass,iSBFunc,iBFunc):\n", "    NLL=fitSig(iLabel,pMass,iSBFunc,iBFunc,False)\n", "    NLLp = 1-stats.chi2.cdf(NLL,1)\n", "    return NLLp\n", "\n", "def pvaluePlot(iLabel,iSBFunc,iBFunc):\n", "    pvalue = np.array([])\n", "    massrange=np.linspace(110,150,120)\n", "    for pMass in massrange:\n", "        pvalue = np.append(pvalue,pvalueCalc(iLabel,pMass,iSBFunc,iBFunc))\n", "    return massrange,pvalue\n", "\n", "m0,p0 = pvaluePlot(\"data/L12/out.txt\",sigpol4,pol4)\n", "m1,p1 = pvaluePlot(\"data/L12/out2.txt\",sigpol4,pol4)\n", "m2,p2 = pvaluePlot(\"data/L12/out3.txt\",sigpol4,pol4)\n", "m3,p3 = pvaluePlot(\"data/L12/out4.txt\",sigpol4,pol4)\n", "m4,p4 = pvaluePlot(\"data/L12/out5.txt\",sigpol4,pol4)\n", "\n", "plt.plot(m0,p0,label=\"Category 1\")\n", "plt.plot(m1,p1,label=\"Category 2\")\n", "plt.plot(m2,p2,label=\"Category 3\")\n", "plt.plot(m3,p3,label=\"Category 4\")\n", "plt.plot(m4,p4,label=\"Category 5\")\n", "plt.ylim((0.0001,1))\n", "plt.xlabel(\"$m_{\\gamma\\gamma}$\")\n", "plt.ylabel(\"p-value\")\n", "plt.yscale(\"log\")\n", "plt.legend()\n", "plt.show()"]}, {"cell_type": "markdown", "id": "fd74759f", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='exercises_12_5'></a>     \n", "\n", "| [Top](#section_12_0) | [Restart Section](#section_12_5) | [Next Section](#section_12_6) |\n"]}, {"cell_type": "markdown", "id": "0f1f80f8", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-12.5.1</span>\n", "\n", "In this section, we have fit the data with a 4th order polynomial, and in the next section we will examine fitting the data with a 5th order polynomial.\n", "\n", "In this question, consider what the p-value plot (from runcell `L12.5-runcell01`) would look like using a 3rd order polynomial. Try to make this plot in the code cell provided.\n", "\n", "Which of the following best describes the features of the 3rd order fit, compared to the 4th order fit?\n", "\n", "A) The Higgs bump is enhanced (i.e. it has an even lower p-value).\\\n", "B) The Higgs bump completely disappears.\\\n", "C) The Higgs bump remains, but it not the strongest feature (i.e.  the one with the lowest p-value) in the plot.\\\n", "D) The Higgs bump is not as prominent compared to other features (i.e. the p-values for other features are now closer to that for the Higgs bump).\n"]}, {"cell_type": "code", "execution_count": null, "id": "04717209", "metadata": {"tags": ["draft", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>EXERCISE: L12.5.1\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your answer in the interactive problem online to be graded.\n", "# You may find that copying and editing some lines from L12.5-runcell01 will be helpful\n", "\n"]}, {"cell_type": "markdown", "id": "b6830dd4", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-12.5.2</span>\n", "\n", "What do you expect the p-value plot would look like using a 5th order polynomial (don't try it yet)? Select all answers from the following that you think are possible:\n", "\n", "A) The Higgs bump is enhanced.\\\n", "B) The Higgs bump completely disappears.\\\n", "C) The Higgs bump remains, but it not the strongest feature in the plot.\\\n", "D) The Higgs bump is not as prominent as it was before, but it is still the strongest feature.\n", "\n", "\n"]}, {"cell_type": "markdown", "id": "1748ed70", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_12_6'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L12.6 Fitting Using Higher Order Polynomials</h2>  \n", "\n", "| [Top](#section_12_0) | [Previous Section](#section_12_5) | [Exercises](#exercises_12_6) | [Next Section](#section_12_7) |"]}, {"cell_type": "code", "execution_count": null, "id": "46289a2c", "metadata": {"tags": ["learner", "py", "learner_chopped", "lect_06"]}, "outputs": [], "source": ["#>>>RUN: L12.6-runcell01\n", "\n", "#THESE TWO FUNCTION WERE DEFINED PREVIOUSLY\n", "def pol5(x, p0, p1,p2,p3,p4,p5):\n", "    pols=[p0,p1,p2,p3,p4,p5]\n", "    y = np.polyval(pols,x)\n", "    return y\n", "\n", "def sigpol5(x,p0,p1,p2,p3,p4,p5,amp,mass,sigma):\n", "    bkg=pol5(x,p0,p1,p2,p3,p4,p5)\n", "    sig=amp*stats.norm.pdf(x,mass,sigma)\n", "    return sig+bkg\n", "\n", "m0,p0 = pvaluePlot(\"data/L12/out.txt\",sigpol5,pol5)\n", "m1,p1 = pvaluePlot(\"data/L12/out2.txt\",sigpol5,pol5)\n", "m2,p2 = pvaluePlot(\"data/L12/out3.txt\",sigpol5,pol5)\n", "m3,p3 = pvaluePlot(\"data/L12/out4.txt\",sigpol5,pol5)\n", "m4,p4 = pvaluePlot(\"data/L12/out5.txt\",sigpol5,pol5)\n", "\n", "plt.plot(m0,p0,label=\"Category 1\")\n", "plt.plot(m1,p1,label=\"Category 2\")\n", "plt.plot(m2,p2,label=\"Category 3\")\n", "plt.plot(m3,p3,label=\"Category 4\")\n", "plt.plot(m4,p4,label=\"Category 5\")\n", "plt.ylim((0.0001,1))\n", "plt.xlabel(\"$m_{\\gamma\\gamma}$\")\n", "plt.ylabel(\"p-value\")\n", "plt.yscale(\"log\")\n", "plt.legend()\n", "plt.show()"]}, {"cell_type": "code", "execution_count": null, "id": "65544eb2", "metadata": {"tags": ["learner", "py", "learner_chopped", "lect_06"]}, "outputs": [], "source": ["#>>>RUN: L12.6-runcell02\n", "\n", "#answer\n", "def pol5(x, p0, p1,p2,p3,p4,p5):\n", "    pols=[p0,p1,p2,p3,p4,p5]\n", "    y = np.polyval(pols,x)\n", "    return y\n", "\n", "def sigpol5(x,p0,p1,p2,p3,p4,p5,amp,mass,sigma):\n", "    bkg=pol5(x,p0,p1,p2,p3,p4,p5)\n", "    sig=amp*stats.norm.pdf(x,mass,sigma)\n", "    return sig+bkg\n", "\n", "NLL=fitSig(\"data/L12/out.txt\",125,sigpol5,pol5,True)\n", "\n", "\n", "m0_pol4,p0_pol4 = pvaluePlot(\"data/L12/out.txt\",sigpol4,pol4)\n", "m0_pol5,p0_pol5 = pvaluePlot(\"data/L12/out.txt\",sigpol5,pol5)\n", "\n", "plt.plot(m0_pol4,p0_pol4,label=\"Category 1 4th order\")\n", "plt.plot(m0_pol5,p0_pol5,label=\"Category 1 5th order\")\n", "plt.ylabel(\"p-value\")\n", "plt.yscale(\"log\")\n", "plt.legend()\n", "plt.show()\n", "\n", "#The 5th order polynomial is less sensivitive since there are more degrees of freedom"]}, {"cell_type": "code", "execution_count": null, "id": "788dbc92", "metadata": {"tags": ["learner", "py", "learner_chopped", "lect_06"]}, "outputs": [], "source": ["#>>>RUN: L12.6-runcell03\n", "\n", "def pvalueCalc(iLabel,pMass,iSBFunc,iBFunc):\n", "    logp=0\n", "    for pLabel in iLabel:\n", "        NLL=fitSig(pLabel,pMass,iSBFunc,iBFunc,False)\n", "        NLLp = 1.-stats.chi2.cdf(NLL,1)\n", "        logp = logp - 2.*np.log(NLLp)\n", "    pPVal  = 1-stats.chi2.cdf(logp,2*len(iLabel))\n", "    return pPVal\n", "\n", "files=[\"data/L12/out.txt\",\"data/L12/out2.txt\",\"data/L12/out3.txt\",\"data/L12/out4.txt\",\"data/L12/out5.txt\"]\n", "mC,pC = pvaluePlot(files,sigpol4,pol4)\n", "\n", "for pVal in range(4):\n", "    sigmas = 1-stats.norm.cdf(pVal+1)\n", "    plt.axhline(y=sigmas, color='r', linestyle='-')\n", "plt.plot(m0,p0,label=\"Category 1\")\n", "plt.plot(m1,p1,label=\"Category 2\")\n", "plt.plot(m2,p2,label=\"Category 3\")\n", "plt.plot(m3,p3,label=\"Category 4\")\n", "plt.plot(m4,p4,label=\"Category 5\")\n", "plt.plot(mC,pC,label=\"Combined Category\")\n", "plt.ylim((0.0001,1))\n", "plt.xlabel(\"$m_{\\gamma\\gamma}$\")\n", "plt.ylabel(\"p-value\")\n", "plt.yscale(\"log\")\n", "plt.legend()\n", "plt.show()\n"]}, {"cell_type": "code", "execution_count": null, "id": "009e5e4b", "metadata": {"tags": ["learner", "py", "learner_chopped", "lect_06"]}, "outputs": [], "source": ["#>>>RUN: L12.6-runcell05\n", "\n", "for pVal in range(4):\n", "    sigmas = 1-stats.norm.cdf(pVal+1)\n", "    plt.axhline(y=sigmas, color='r', linestyle='-')\n", "    \n", "plt.plot(mC,pC,label=\"Category 1\")\n", "plt.ylim((0.0001,1))\n", "plt.xlabel(\"$m_{\\gamma\\gamma}$\")\n", "plt.ylabel(\"p-value\")\n", "plt.yscale(\"log\")\n", "plt.legend()\n", "plt.show()"]}, {"cell_type": "markdown", "id": "1bacb197", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='exercises_12_6'></a>     \n", "\n", "| [Top](#section_12_0) | [Restart Section](#section_12_6) | [Next Section](#section_12_7) |\n"]}, {"cell_type": "markdown", "id": "4712495d", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-12.6.1</span>\n", "\n", "Noting that the p-value on the y-axis of the plots shown previously is just a translation of the likelihood, and the p-value is computed from a $\\chi^{2}$ distribution, convert from $\\chi^{2}$ probability back to 2$\\log(\\mathcal{L})$, and from this compute the best fit mass for the Higgs boson with 1 standard deviation uncertainty. Use the results of the combined data, denoted `mC` and `pC`? As always, the \"best fit mass\" is the one corresponding to the lowest $\\chi^2$ and the $1\\sigma$ uncertainty corresponds to the range over which the $\\chi^2$ is within 1 of the minimum value. You may choose to perform this computation in the code cell provided.\n", "\n", "How does the best fit mass that you find from this analysis compare to the accepted value of the Higgs?\n", "\n", "Report your answer as a list of two numbers with precision 1e-1:`[best fit mass, uncertainty]`\n", "\n", "**Note:** You can answer this question by simply looking at the $\\chi^2$ for individual masses in the $mC$ array, but a more accurate answer (less sensitive to the details of the binning) can be found by fitting a parabola to a few points right around the minimum. Use whatever method you like!"]}, {"cell_type": "code", "execution_count": null, "id": "a9b46779", "metadata": {"tags": ["draft", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>EXERCISE: L12.6.1\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "#The strategy here is to realize that p-value plot is also 2*Log(L) of our best fit,\n", "#thus, we just need to go 1 standard deviation from the minimum in likelihood"]}, {"cell_type": "markdown", "id": "a47b3116", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_12_7'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L12.7 Building Interpolated Distributions</h2>  \n", "\n", "| [Top](#section_12_0) | [Previous Section](#section_12_6) | [Exercises](#exercises_12_7) | [Next Section](#section_12_8) |"]}, {"cell_type": "code", "execution_count": null, "id": "f28e9928", "metadata": {"tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L12.7-slides\n", "\n", "from IPython.display import IFrame\n", "IFrame(src='https://mitx-8s50.github.io/slides/L12/slides_L12_08.html', width=970, height=550)"]}, {"cell_type": "code", "execution_count": null, "id": "8ddb27f4", "metadata": {"tags": ["learner", "py", "learner_chopped", "lect_07"]}, "outputs": [], "source": ["#>>>RUN: L12.7-runcell01\n", "\n", "#Now let's load some data and do gaussian kernels with it\n", "x,y,y_err,weights=load(\"data/L12/out_2011.txt\")\n", "\n", "\n", "from scipy import interpolate\n", "tck = interpolate.splrep(x, y) #setup the spline\n", "x2 = np.linspace(110, 150) #range\n", "y2 = interpolate.splev(x2, tck)#apply the spline\n", "\n", "plt.plot(x, y, 'go',label='data')\n", "plt.plot(x2, y2, 'b',label='spline')\n", "plt.xlabel(\"$m_{\\gamma\\gamma}$\")\n", "plt.ylabel(\"$N_{events}$\")\n", "plt.legend()\n", "plt.show()"]}, {"cell_type": "code", "execution_count": null, "id": "027ec626", "metadata": {"tags": ["learner", "py", "learner_chopped", "lect_07"]}, "outputs": [], "source": ["#>>>RUN: L12.7-runcell02\n", "\n", "#spline convolve by hand\n", "def splineconvolve(tck,f2,x,iMin=-15,iMax=15,iN=500):\n", "    step=float((iMax-iMin))/float(iN)\n", "    pInt=0\n", "    for i0 in range(iN):\n", "            pX   = i0*step+iMin\n", "            pVal = interpolate.splev(x-pX,tck)*f2(pX)\n", "            pInt += pVal*step\n", "    return pInt\n", "\n", "def gaussian(x,mean=0,sigma=1):#try changing the sigma value\n", "    return 1./(sigma * np.sqrt(2 * np.pi)) * np.exp( - (x - mean)**2 / (2 * sigma**2)) \n", "\n", "fig, ax = plt.subplots()\n", "x_in=np.linspace(110, 150, 10)\n", "#x_in=np.linspace(115, 145, 10)\n", "#x_in=np.linspace(122, 142, 10)\n", "conv_out=[]\n", "for val in x_in:\n", "    pConv_out=splineconvolve(tck,gaussian,val)\n", "    conv_out.append(pConv_out)\n", "\n", "#now we can plot it\n", "plt.plot(x, y, 'go',label='data')\n", "plt.plot(x2, y2, 'b',label='spline')\n", "plt.plot(x_in,conv_out,c='orange',label='convolution')\n", "plt.xlabel(\"$m_{\\gamma\\gamma}$\")\n", "plt.ylabel(\"$N_{events}$\")\n", "plt.legend()\n", "plt.show()"]}, {"cell_type": "code", "execution_count": null, "id": "2079543e", "metadata": {"tags": ["learner", "py", "learner_chopped", "lect_07"]}, "outputs": [], "source": ["#>>>RUN: L12.7-runcell03\n", "\n", "import george\n", "from george import kernels\n", "\n", "kernel = np.var(y) * kernels.Matern52Kernel(5.0)\n", "#kernel = np.var(y) * kernels.Matern52Kernel(125.0)\n", "gp = george.GP(kernel)\n", "gp.compute(x, y_err)\n", "x_pred = np.linspace(110, 150, 100)\n", "pred, pred_var = gp.predict(y, x_pred, return_var=True)\n", "\n", "plt.fill_between(x_pred, pred - np.sqrt(pred_var), pred + np.sqrt(pred_var),color=\"k\", alpha=0.2,label=\"gaussian Process\")\n", "plt.plot(x_pred, pred, \"k\", lw=1.5, alpha=0.5)\n", "plt.errorbar(x, y, yerr=y_err, fmt=\".k\", capsize=0)\n", "plt.plot(x2, y2, 'b',label='spline')\n", "plt.plot(x_in,conv_out,c='orange',label='convolution')\n", "plt.xlabel(\"$m_{\\gamma\\gamma}$\")\n", "plt.ylabel(\"$N_{events}$\")\n", "plt.legend()\n", "plt.show()"]}, {"cell_type": "markdown", "id": "83d9375f", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='exercises_12_7'></a>     \n", "\n", "| [Top](#section_12_0) | [Restart Section](#section_12_7) | [Next Section](#section_12_8) |"]}, {"cell_type": "markdown", "id": "04a3f8af", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-12.7.1</span>\n", "\n", "As we saw, when we interpolate data we obtain a function that can be used on any set of data points within the interpolated range. We can also augment the interpolation by convolving it with other functions, the most useful of which is a Gaussian.\n", "\n", "Consider how this can be useful, in the context of fitting and hypothesis testing. Which of the following describes the usefulness of creating new functions in this manner? Select all that apply:\n", "\n", "A) We can use a Gaussian convolution to smooth out features of the data when creating a background model.\\\n", "B) We can use the interpolation to create a background model of the data, then add a signal to the interpolation and perform a fit.\\\n", "C) We can use Gaussian convolution to enhance the resolution of the interpolated function, allowing for more precise analysis of the data.\\\n", "D) We can exploit the smoothness of the interpolated function combined with Gaussian convolution to detect and remove outliers in the data before fitting."]}, {"cell_type": "markdown", "id": "d0a69a1a", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_12_8'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L12.8 An Example Fitting Z Boson Data (Ungraded)</h2>     \n", "\n", "\n", "| [Top](#section_12_0) | [Previous Section](#section_12_7) |"]}, {"cell_type": "code", "execution_count": null, "id": "c432c4e8", "metadata": {"tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L12.8-slides\n", "\n", "from IPython.display import IFrame\n", "IFrame(src='https://mitx-8s50.github.io/slides/L12/slides_L12_09.html', width=970, height=550)"]}, {"cell_type": "code", "execution_count": null, "id": "bb1a5e9f", "metadata": {"tags": ["learner", "py", "learner_chopped", "lect_08"]}, "outputs": [], "source": ["#>>>RUN: L12.8-runcell01\n", "\n", "x,y_data,y_err,weights=load(\"data/L12/data.txt\",True)\n", "x,y_mc,y_mc_err,_=load(\"data/L12/zz_narrow.txt\",True)\n", "\n", "tck = interpolate.splrep(x, y_mc)\n", "# y_shift=y_mc*3\n", "# tck = interpolate.splrep(x, y_shift)\n", "\n", "x2 = np.linspace(50, 160,1000)\n", "y2 = interpolate.splev(x2, tck)\n", "#y2 = interpolate.splev(x2, tck)*2.5\n", "\n", "plt.errorbar(x,y_data,yerr=y_err,marker='.',linestyle = 'None', color = 'black')\n", "plt.plot(x2, y2, 'b')\n", "plt.plot(x,y_mc,drawstyle = 'steps-mid')\n", "plt.xlabel(\"$m_{4\\ell}$\")\n", "plt.ylabel(\"$N_{events}$\")\n", "plt.show()"]}, {"cell_type": "code", "execution_count": null, "id": "204630c1", "metadata": {"tags": ["learner", "py", "learner_chopped", "lect_08"]}, "outputs": [], "source": ["#>>>RUN: L12.8-runcell02\n", "\n", "#spline convolve by hand\n", "def splineconvolvegaus(x,mean,sigma,iMin=-15,iMax=15,iN=500):\n", "    step=float((iMax-iMin))/float(iN)\n", "    pInt=0\n", "    for i0 in range(iN):\n", "            pX   = i0*step+iMin\n", "            pVal = interpolate.splev(x-pX,tck)*gaussian(pX,mean,sigma)\n", "            pInt += pVal*step\n", "    return pInt\n", "\n", "def gausconv(x,mean,sigma,amp,a,b):\n", "    #Try fitting just this val first\n", "    val=splineconvolvegaus(x,mean,sigma)*amp\n", "    \n", "    #Next, try uncommenting this line to change the function\n", "    #val=a + b*x + val\n", "    return val\n", "\n", "model  = lmfit.Model(gausconv)\n", "p = model.make_params(mean=0,sigma=1.0,amp=3.0,a=1,b=0)\n", "#try adjusting the sigma values below, and also try floating the fit value\n", "#p[\"sigma\"].value=0.1\n", "#p[\"sigma\"].vary=False\n", "result = model.fit(data=y_data,params=p,x=x,weights=weights)\n", "lmfit.report_fit(result)\n", "result.plot()\n", "plt.show()"]}, {"cell_type": "code", "execution_count": null, "id": "e7b56476", "metadata": {"tags": ["learner", "py", "learner_chopped", "lect_08"]}, "outputs": [], "source": ["#>>>RUN: L12.8-runcell03\n", "\n", "def splineconvolvegaus(tck,f2,x,mean,sigma,iMin=-15,iMax=15,iN=500):\n", "    step=float((iMax-iMin))/float(iN)\n", "    pInt=0\n", "    for i0 in range(iN):\n", "            pX   = i0*step+iMin\n", "            pVal = interpolate.splev(x-pX,tck)*f2(pX,mean,sigma)\n", "            pInt += pVal*step\n", "    return pInt\n", "\n", "def gausconv(x,mean,sigma,sig,baseline,slope):\n", "    val=splineconvolvegaus(tck,gaussian,x,mean,sigma)\n", "    output = baseline+sig*val+slope*x\n", "    return output\n", "\n", "model  = lmfit.Model(gausconv)\n", "p = model.make_params(mean=0,sigma=1,sig=2,baseline=2,slope=0)\n", "p[\"mean\"].vary = False\n", "p[\"sigma\"].vary = True\n", "result = model.fit(data=y_data, params=p, x=x, weights=weights)\n", "lmfit.report_fit(result)\n", "result.plot()\n", "plt.show()\n", "\n", "#Now let's not smear the data\n", "p[\"sigma\"].value = 0.01\n", "p[\"sigma\"].vary = False\n", "result = model.fit(data=y_data, params=p, x=x, weights=weights)\n", "lmfit.report_fit(result)\n", "result.plot()\n", "plt.show()"]}], "metadata": {"celltoolbar": "Tags", "kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}