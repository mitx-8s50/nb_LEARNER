{"cells": [{"cell_type": "markdown", "id": "6d7f5a2c", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<hr style=\"height: 1px;\">\n", "<i>This notebook was authored by the 8.S50x Course Team, Copyright 2022 MIT All Rights Reserved.</i>\n", "<hr style=\"height: 1px;\">\n", "<br>\n", "\n", "<h1>Lesson 14: An Example With LHC Data</h1>\n"]}, {"cell_type": "markdown", "id": "ff95d3bc", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_14_0'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L14.0 Overview</h2>\n"]}, {"cell_type": "markdown", "id": "91dd7386", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<h3>Navigation</h3>\n", "\n", "<table style=\"width:100%\">\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_14_1\">L14.1 Large Hadron Collider Data</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_14_1\">L14.1 Exercises</a></td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_14_2\">L14.2 Loading Data and Defining the Network</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_14_2\">L14.2 Exercises</a></td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_14_3\">L14.3 Training and Testing the Network</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_14_3\">L14.3 Exercises</a></td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_14_4\">L14.4 Adding a Hidden Layer</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_14_4\">L14.4 Exercises</a></td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_14_5\">L14.5 Regularization, Batch Normalization, and Dropout</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_14_5\">L14.5 Exercises</a></td>\n", "    </tr>\n", "</table>\n", "\n"]}, {"cell_type": "markdown", "id": "624022da", "metadata": {"tags": ["learner", "md", "catsoop_00", "learner_chopped"]}, "source": ["<h3>Data</h3>\n", "\n", ">description: CMS Crystal Shower Shape Data<br>\n", ">source: https://zenodo.org/record/8035308 <br>\n", ">attribution: Rankin, Dylan (CMS Collaboration), DOI:10.5281/zenodo.8035308 "]}, {"cell_type": "code", "execution_count": null, "id": "9dab2f77", "metadata": {"tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L14.0-runcell00\n", "\n", "# NOTE: these files are too large to include in the original repository, so you must download them from here:\n", "# https://www.dropbox.com/s/i1dbakzr3pn9twd/xtalTuple_TTbar_PU0.z?dl=0\n", "#\n", "# Ways to download:\n", "#     1. Copy/paste the link (replace =0 with =1 to download automatically)\n", "#     2. Use the wget commands below (works in Colab, but you may need to install wget if using locally)\n", "#\n", "# Location of files:\n", "#     Move the files to the directory data/L14\n", "#\n", "# Using wget: (works in Colab)\n", "#     Upon downloading, the code below will move them to the appropriate directory\n", "\n", "#get the data\n", "!wget -P data/L14 https://www.dropbox.com/s/i1dbakzr3pn9twd/xtalTuple_TTbar_PU0.z?dl=0\n", "!mv data/L14/xtalTuple_TTbar_PU0.z?dl=0 data/L14/xtalTuple_TTbar_PU0.z "]}, {"cell_type": "code", "execution_count": null, "id": "51c9b869", "metadata": {"tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L14.0-runcell01\n", "\n", "#If using notebooks locally, run the following within your conda environment (if not done already)\n", "#conda install pandas\n", "\n", "import numpy as np               #https://numpy.org/doc/stable/\n", "import matplotlib.pyplot as plt  #https://matplotlib.org/3.5.3/api/_as_gen/matplotlib.pyplot.html\n", "import h5py                      #https://docs.h5py.org/en/stable/quick.html#quick\n", "import pandas as pd              #https://pandas.pydata.org/docs/user_guide/index.html\n", "import torch                     #https://pytorch.org/docs/stable/torch.html"]}, {"cell_type": "code", "execution_count": null, "id": "ba828ee6", "metadata": {"tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L14.0-runcell02\n", "\n", "#set plot resolution\n", "%config InlineBackend.figure_format = 'retina'\n", "\n", "#set default figure parameters\n", "plt.rcParams['figure.figsize'] = (9,6)\n", "\n", "medium_size = 12\n", "large_size = 15\n", "\n", "plt.rc('font', size=medium_size)          # default text sizes\n", "plt.rc('xtick', labelsize=medium_size)    # xtick labels\n", "plt.rc('ytick', labelsize=medium_size)    # ytick labels\n", "plt.rc('legend', fontsize=medium_size)    # legend\n", "plt.rc('axes', titlesize=large_size)      # axes title\n", "plt.rc('axes', labelsize=large_size)      # x and y labels\n", "plt.rc('figure', titlesize=large_size)    # figure title\n"]}, {"cell_type": "markdown", "id": "05535af7", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_14_1'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L14.1 Large Hadron Collider Data</h2>  \n", "\n", "| [Top](#section_14_0) | [Previous Section](#section_14_0) | [Exercises](#exercises_14_1) | [Next Section](#section_14_2) |\n"]}, {"cell_type": "code", "execution_count": null, "id": "73dfe0cc", "metadata": {"tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L14.1-slides\n", "\n", "from IPython.display import IFrame\n", "IFrame(src='https://mitx-8s50.github.io/slides/L14/slides_L14_01.html', width=970, height=550)"]}, {"cell_type": "markdown", "id": "71d5e72a", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='exercises_14_1'></a>     \n", "\n", "| [Top](#section_14_0) | [Restart Section](#section_14_1) | [Next Section](#section_14_2) |\n"]}, {"cell_type": "markdown", "id": "2c6d1ed0", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-14.1.1</span>\n", "\n", "The CMS ECAL is intended to identify photons and electrons. However, it is often the case that you can get particles that mimic photons and electrons. In particular, pions can leave large energy deposits in the ECAL. Charged pions will produce a charged track and shower in the calorimeter. These are usually not a problem since they can be identified by the fact that they also deposit energy in the Hadron Calorimeter behind the ECAL.\n", "\n", "On the other hand, neutral pions will decay into two photons that are close to each other (colinear). In fact, they are typically so close together that they look like a single photon. The problem that we would like to solve is the separation of neutral pion decays from photons directly from the original collision. Let's say we are looking for a process that decays to photons, for example the Higgs decay to two well-separated photons. Selecting the Higgs involves selecting two photons on top of backgrounds from *fake* photons. What could a neural network do to remove fake photons? \n", "\n", "A) Reduce the background by eliminating fake events that are produced from pions. This is done by selecting events that have a large probability of containing TWO real photons.\\\n", "B) Do nothing to the background, just help to make suggestions as to what is more likely background.\\\n", "C) Generate a weight for each event quantifying the likelihood that it contains real photons. This weight can be used to look for the Higgs.\\\n", "D) Reduce the background by eliminating fake events that are produced from pions. This is done by selecting events that have a large probability of containing ONE real photon.\n", "\n"]}, {"cell_type": "markdown", "id": "0336942a", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-14.1.2</span>\n", "\n", "If our dominant background comes from pions that decay into two nearby photons, what would allow us to discriminate these cases from real photons? Select all that apply:\n", "\n", "A) Calorimeter shapes that look like two energy blobs in the cells.\\\n", "B) Wider calorimeter shapes.\\\n", "C) A single high energy deposit.\\\n", "D) Calorimeter shapes that are wider in a specific direction.\n", "\n"]}, {"cell_type": "markdown", "id": "26607e2f", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_14_2'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L14.2 Loading Data and Defining the Network</h2>  \n", "\n", "| [Top](#section_14_0) | [Previous Section](#section_14_1) | [Exercises](#exercises_14_2) | [Next Section](#section_14_3) |\n"]}, {"cell_type": "code", "execution_count": null, "id": "466f917d", "metadata": {"tags": ["learner", "py", "lect_02", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L14.2-runcell01\n", "\n", "import h5py\n", "import pandas as pd\n", "\n", "\n", "treename = 'l1pf_egm_reg'\n", "\n", "VARS = ['pt', 'eta', 'phi', 'energy',\n", "  'e2x2', 'e2x5', 'e3x5', 'e5x5', 'e2x2_div_e2x5', 'e2x2_div_e5x5', 'e2x5_div_e5x5',#7\n", "  'hoE', 'bremStrength', 'ecalIso', 'crystalCount',#4\n", "  'lowerSideLobePt','upperSideLobePt',#2\n", "  'phiStripContiguous0', 'phiStripOneHole0', 'phiStripContiguous3p', 'phiStripOneHole3p',#4\n", "  'sihih','sipip','sigetaeta','sigphiphi','sigetaphi',#5\n", "  'e_m2_m2','e_m2_m1','e_m2_p0','e_m2_p1','e_m2_p2',\n", "  'e_m1_m2','e_m1_m1','e_m1_p0','e_m1_p1','e_m1_p2',\n", "  'e_p0_m2','e_p0_m1','e_p0_p0','e_p0_p1','e_p0_p2',\n", "  'e_p1_m2','e_p1_m1','e_p1_p0','e_p1_p1','e_p1_p2',\n", "  'e_p2_m2','e_p2_m1','e_p2_p0','e_p2_p1','e_p2_p2',#^25\n", "  'h_m1_m1','h_m1_p0','h_m1_p1',\n", "  'h_p0_m1','h_p0_p0','h_p0_p1',\n", "  'h_p1_m1','h_p1_p0','h_p1_p1',#^9\n", "  'gen_match']\n", "\n", "filename = 'data/L14/xtalTuple_TTbar_PU0.z'\n", "\n", "h5file = h5py.File(filename, 'r') # open read-only\n", "params = h5file[treename][()]\n", "\n", "df = pd.DataFrame(params,columns=VARS)\n", "\n", "TODROP = [\n", "  'e2x2_div_e2x5', 'e2x2_div_e5x5', 'e2x5_div_e5x5',#7\n", "  'e_m2_m2','e_m2_m1','e_m2_p0','e_m2_p1','e_m2_p2',\n", "  'e_m1_m2','e_m1_m1','e_m1_p0','e_m1_p1','e_m1_p2',\n", "  'e_p0_m2','e_p0_m1','e_p0_p0','e_p0_p1','e_p0_p2',\n", "  'e_p1_m2','e_p1_m1','e_p1_p0','e_p1_p1','e_p1_p2',\n", "  'e_p2_m2','e_p2_m1','e_p2_p0','e_p2_p1','e_p2_p2',#^25\n", "  'h_m1_m1','h_m1_p0','h_m1_p1',\n", "  'h_p0_m1','h_p0_p0','h_p0_p1',\n", "  'h_p1_m1','h_p1_p0','h_p1_p1',#^9\n", "]\n", "\n", "df = df.drop(TODROP, axis=1) #remove custom variables\n", "\n", "#normalize the shower shapes by energy\n", "for ie in ['e2x2', 'e2x5', 'e3x5', 'e5x5']:\n", "    df[ie] /= df['energy']\n", "\n", "#add some labels\n", "df['isPU'] = pd.Series(df['gen_match']==0, index=df.index, dtype='i4')\n", "df['isEG'] = pd.Series(df['gen_match']==1, index=df.index, dtype='i4')\n", "\n", "#now select the dataset based on their transverse momentum (pt)\n", "MINPT = 0.5\n", "MAXPT = 100.\n", "df = df.loc[(df['pt']>MINPT) & (MAXPT>df['pt']) & (1.3>abs(df['eta']))]\n", "df.fillna(0., inplace=True)\n", "\n", "#take a fixed nubmer of events\n", "df0 = df[df['gen_match']==0].head(100000)\n", "df1 = df[df['gen_match']==1].head(10000)\n", "\n", "df = pd.concat([df0, df1], ignore_index=True)\n", "df = df.sample(frac=1).reset_index(drop=True)\n", "col_names = list(df.columns)\n", "\n", "#Now let's check it all\n", "print(df)\n", "print(sum(df['gen_match']==0))\n", "print(sum(df['gen_match']==1))"]}, {"cell_type": "code", "execution_count": null, "id": "233f2c5d", "metadata": {"scrolled": false, "tags": ["learner", "py", "lect_02", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L14.2-runcell02\n", "\n", "col_names = list(df.columns)\n", "print(col_names)\n", "\n", "fig, axs = plt.subplots(len(col_names),1,figsize=(4,4*len(col_names)))\n", "for ix,ax in enumerate(axs):\n", "    ax.hist(df[col_names[ix]][df['gen_match']==0],bins=np.linspace(np.min(df[col_names[ix]]),np.max(df[col_names[ix]]),20),histtype='step',color='r',density=True)\n", "    ax.hist(df[col_names[ix]][df['gen_match']==1],bins=np.linspace(np.min(df[col_names[ix]]),np.max(df[col_names[ix]]),20),histtype='step',color='b',density=True)\n", "    ax.set_xlabel(col_names[ix])\n", "\n", "plt.show()"]}, {"cell_type": "code", "execution_count": null, "id": "0fb25db6", "metadata": {"tags": ["learner", "py", "lect_02", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L14.2-runcell03\n", "\n", "\n", "dataset = df.values\n", "\n", "X = dataset[:,4:-3]\n", "#last 3 columns are labels\n", "ninputs = len(list(df.columns))-3-4\n", "\n", "Y = dataset[:,-1:]\n", "#last column will be used for the label\n", "\n", "test_frac = 0.3\n", "val_frac = 0.2\n", "\n", "alldataset = torch.utils.data.TensorDataset(torch.tensor(X, dtype=torch.float32), torch.tensor(Y, dtype=torch.float32))\n", "\n", "torch.random.manual_seed(42) # fix a random seed for reproducibility\n", "testdataset, trainvaldataset = torch.utils.data.random_split(\n", "    alldataset, [int(len(Y)*test_frac),\n", "              int(len(Y)*(1-test_frac))])\n", "\n", "torch.random.manual_seed(42) # fix a random seed for reproducibility\n", "traindataset, valdataset = torch.utils.data.random_split(\n", "    trainvaldataset, [int(len(Y)*(1.-test_frac)*(1.-val_frac)),\n", "              int(len(Y)*(1.-test_frac)*val_frac)])\n", "\n", "testloader = torch.utils.data.DataLoader(testdataset,\n", "                                          num_workers=6,\n", "                                          batch_size=500,\n", "                                          shuffle=False)\n", "trainloader = torch.utils.data.DataLoader(traindataset,\n", "                                          num_workers=6,\n", "                                          batch_size=500,\n", "                                          shuffle=True)\n", "valloader = torch.utils.data.DataLoader(valdataset,\n", "                                        num_workers=6,\n", "                                        batch_size=500,\n", "                                        shuffle=False)\n"]}, {"cell_type": "code", "execution_count": null, "id": "54a15188", "metadata": {"tags": ["learner", "py", "lect_02", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L14.2-runcell04\n", "\n", "class LR_net(torch.nn.Module):\n", "    def __init__(self):\n", "        super().__init__()\n", "        self.fc1 = torch.nn.Linear(ninputs,1)\n", "        self.output = torch.nn.Sigmoid()\n", "\n", "    def forward(self, x):\n", "        x = self.fc1(x)\n", "        x = self.output(x)\n", "        return x\n", "        \n", "torch.random.manual_seed(42)  # fix a random seed for reproducibility\n", "\n", "model_lr = LR_net()\n", "print(model_lr)\n", "print('----------')\n", "print(model_lr.state_dict())"]}, {"cell_type": "markdown", "id": "ac90e900", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='exercises_14_2'></a>     \n", "\n", "| [Top](#section_14_0) | [Restart Section](#section_14_2) | [Next Section](#section_14_3) |\n"]}, {"cell_type": "markdown", "id": "13f2d256", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-14.2.1</span>\n", "\n", "Consider the variables that you plotted in `L14.2-runcell02`. Among the following variables, which appear to have high discrimination power? In other words, in which plots is the background (red) distinguishable from the egamma (blue), meaning there is a relatively small overlap between the red and blue histograms? Select all that apply.\n", "\n", "A) pt\\\n", "B) eta\\\n", "C) phi\\\n", "D) energy\\\n", "E) e2x2\\\n", "F) sihih\n"]}, {"cell_type": "markdown", "id": "69f30ae9", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-14.2.2</span>\n", "\n", "How are training, validation, and testing data used in machine learning model development?\n", "\n", "A) Training data is used to evaluate the model's performance, validation data is used to select the best hyperparameters, and testing data is used to train the model.\\\n", "B) Training data is used to train the model, validation data is used to tune the hyperparameters, and testing data is used to evaluate the model's performance.\\\n", "C) Training data is used to tune the hyperparameters, validation data is used to evaluate the model's performance, and testing data is used to train the model.\\\n", "D) All three datasets are used interchangeably to train, tune, and evaluate the model."]}, {"cell_type": "markdown", "id": "e9d7add9", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-14.2.3</span>\n", "\n", "In the one-layer neural network that we have defined in this section (`LR_net` from `L14.2-runcell04`), we are using 19 input features. How many weights does this neural network have? Enter your answer as an integer.\n", "\n", "Extra: How is this different from the two-weight model we were using in the last Lesson?"]}, {"cell_type": "markdown", "id": "1d1630a3", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_14_3'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L14.3 Training and Testing the Network</h2>  \n", "\n", "| [Top](#section_14_0) | [Previous Section](#section_14_2) | [Exercises](#exercises_14_3) | [Next Section](#section_14_4) |\n"]}, {"cell_type": "code", "execution_count": null, "id": "eaa7ff3f", "metadata": {"tags": ["learner", "py", "lect_03", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L14.3-runcell01\n", "\n", "criterion = torch.nn.BCELoss()\n", "optimizer_lr = torch.optim.Adam(model_lr.parameters(), lr=0.003) \n", "\n", "history_lr = {'loss':[], 'val_loss':[]}\n", "\n", "for epoch in range(20):\n", "\n", "    current_loss = 0.0 #rezero loss\n", "    \n", "    for i, data in enumerate(trainloader):\n", "\n", "        inputs, labels = data\n", "        \n", "        # zero the parameter gradients\n", "        optimizer_lr.zero_grad()\n", "\n", "        # forward + backward + optimize (training magic)\n", "        # This will use the pytorch autograd feature to adjust the\n", "        ## parameters of our function to minimize the loss\n", "        outputs = model_lr(inputs)\n", "        loss = criterion(outputs, labels)\n", "        loss.backward()\n", "        optimizer_lr.step()\n", "        \n", "        # add loss statistics\n", "        current_loss += loss.item()\n", "        \n", "        if i == len(trainloader)-1:\n", "            current_val_loss = 0.0\n", "            with torch.no_grad():#disable updating gradient\n", "                for iv, vdata in enumerate(valloader):\n", "                    val_inputs, val_labels = vdata\n", "                    val_loss = criterion(model_lr(val_inputs), val_labels)\n", "                    current_val_loss += val_loss.item()\n", "            print('[%d, %4d] loss: %.4f  val loss: %.4f' % \n", "                  (epoch + 1, i + 1, current_loss/float(i+1) , current_val_loss/float(len(valloader))))\n", "\n", "            history_lr['loss'].append(current_loss/float(i+1))\n", "            history_lr['val_loss'].append(current_val_loss/float(len(valloader)))\n", "            \n", "print('Finished Training')\n", "torch.save(model_lr.state_dict(), 'data/L14/lr_model.pt')\n", "print(model_lr.state_dict())"]}, {"cell_type": "code", "execution_count": null, "id": "f2504550", "metadata": {"tags": ["learner", "py", "lect_03", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L14.3-runcell02\n", "\n", "plt.semilogy(history_lr['loss'], label='loss')\n", "plt.semilogy(history_lr['val_loss'], label='val_loss')\n", "plt.legend(loc=\"upper right\")\n", "plt.xlabel('epoch')\n", "plt.ylabel('loss (binary crossentropy)')\n", "plt.show()"]}, {"cell_type": "code", "execution_count": null, "id": "94538aef", "metadata": {"tags": ["learner", "py", "lect_03", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L14.3-runcell03\n", "\n", "def train(model,trainloader,valloader,nepochs=100,lr=0.003,l2reg=0.,patience=5,name=None):\n", "\n", "    criterion = torch.nn.BCELoss()\n", "    \n", "    #NOTE: l2 regularization is set to 0 by default,\n", "    #but we will address this in later sections\n", "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=l2reg) \n", "\n", "    history = {'loss':[], 'val_loss':[]}\n", "\n", "    min_loss = 999999.\n", "    min_epoch = 0\n", "    min_model = model.state_dict()\n", "    should_stop = False\n", "    \n", "    for epoch in range(nepochs):\n", "\n", "        current_loss = 0.0 #rezero loss\n", "\n", "        for i, data in enumerate(trainloader):\n", "\n", "            inputs, labels = data\n", "\n", "            # zero the parameter gradients\n", "            optimizer.zero_grad()\n", "\n", "            # forward + backward + optimize\n", "            # This will use the pytorch autograd feature to adjust the\n", "            ## parameters of our function to minimize the loss\n", "            outputs = model(inputs)\n", "            loss = criterion(outputs, labels)\n", "            loss.backward()\n", "            optimizer.step()\n", "\n", "            # print statistics\n", "            current_loss += loss.item()\n", "\n", "            if i == len(trainloader)-1:\n", "                current_val_loss = 0.0\n", "                with torch.no_grad():#disable updating gradient\n", "                    model.eval() #place model in evaluation state\n", "                                ## necessary for some layer types (like dropout)\n", "                    for iv, vdata in enumerate(valloader):\n", "                        val_inputs, val_labels = vdata\n", "                        val_loss = criterion(model(val_inputs), val_labels)\n", "                        current_val_loss += val_loss.item()\n", "                    model.train() #return to training state\n", "                current_loss = current_loss/float(i+1)\n", "                current_val_loss = current_val_loss/float(len(valloader))\n", "                print('[%d, %4d] loss: %.4f  val loss: %.4f' % \n", "                      (epoch + 1, i + 1, current_loss , current_val_loss))\n", "\n", "                if current_val_loss < min_loss:\n", "                    min_loss = current_val_loss\n", "                    min_model = model.state_dict()\n", "                    min_epoch = epoch\n", "                elif epoch-min_epoch==5:\n", "                    model.load_state_dict(min_model)\n", "                    should_stop = True\n", "                    break\n", "\n", "                history['loss'].append(current_loss)\n", "                history['val_loss'].append(current_val_loss)\n", "                \n", "            if should_stop:\n", "                break\n", "\n", "    print('Finished Training')\n", "    if name is not None:\n", "        filename_save = 'data/L14/' + name + '.pt'\n", "        torch.save(model.state_dict(), filename_save)\n", "    return history"]}, {"cell_type": "code", "execution_count": null, "id": "bd675e99", "metadata": {"tags": ["learner", "py", "lect_03", "learner_chopped"]}, "outputs": [], "source": ["history_lr = train(model_lr,trainloader,valloader,name='lr_model')"]}, {"cell_type": "code", "execution_count": null, "id": "701bc6af", "metadata": {"tags": ["learner", "py", "lect_03", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L14.3-runcell04\n", "\n", "plt.semilogy(history_lr['loss'], label='loss')\n", "plt.semilogy(history_lr['val_loss'], label='val_loss')\n", "plt.legend(loc=\"upper right\")\n", "plt.xlabel('epoch')\n", "plt.ylabel('loss (binary crossentropy)')\n", "plt.show()"]}, {"cell_type": "code", "execution_count": null, "id": "e588d00b", "metadata": {"tags": ["learner", "py", "lect_03", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L14.3-runcell05\n", "\n", "def apply(model, testloader):\n", "    with torch.no_grad():\n", "        model.eval()\n", "        outputs = []\n", "        labels = []\n", "        for data in testloader:\n", "            test_inputs, test_labels = data\n", "            outputs.append(model(test_inputs).numpy())\n", "            labels.append(test_labels.numpy())\n", "        model.train()\n", "\n", "        Y_test_predict = outputs\n", "        Y_test = labels\n", "\n", "    Y_test_predict = np.concatenate(Y_test_predict)\n", "    Y_test = np.concatenate(Y_test)\n", "    \n", "    return Y_test_predict,Y_test\n", "\n", "Y_test_predict_lr, Y_test = apply(model_lr, testloader)\n", "\n", "print(Y_test_predict_lr.shape)\n", "print(Y_test.shape)"]}, {"cell_type": "code", "execution_count": null, "id": "de486bc7", "metadata": {"tags": ["learner", "py", "lect_03", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L14.3-runcell06\n", "\n", "plt.hist(Y_test_predict_lr[Y_test==0],histtype='step',color='r',density=True)\n", "plt.hist(Y_test_predict_lr[Y_test==1],histtype='step',color='b',density=True)\n", "plt.xlabel('Logistic Regression Discriminant')\n", "plt.show()"]}, {"cell_type": "markdown", "id": "a16ea19d", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='exercises_14_3'></a>     \n", "\n", "| [Top](#section_14_0) | [Restart Section](#section_14_3) | [Next Section](#section_14_4) |\n"]}, {"cell_type": "markdown", "id": "aaa7b696", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-14.3.1</span>\n", "\n", "Which of the following features indicates that training is NOT performing successfully? Select all that apply:\n", "\n", "A) The loss for the validation data differs significantly from that for the training data.\\\n", "B) The loss as a function of epoch flattens out for both data sets.\\\n", "C) The loss as a function of epoch is shifted slightly for the validation data compared to the training data.\\\n", "D) The loss as a function of epoch continues to decrease for the training data set, but remains constant for the validation data set."]}, {"cell_type": "markdown", "id": "293d15d7", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-14.3.2</span>\n", "\n", "We can approximate the uncertainty on the loss by assuming that the number of events in both datasets follows a Poisson distribution. Using this concept, complete the code below to calculate the statistical disagreement between the training loss and validation loss. Specifically, write a function that returns the difference between the losses in terms of the number of standard deviations.\n", "\n", "**Extra:** How significant is the difference after the last epoch? Try plotting this as a function of epoch!\n", "\n", "**Note:** The related plot may yield wildly different results, depending on how your network runs. Here we focus on how to write the function, instead of analyzing the output of the plot."]}, {"cell_type": "code", "execution_count": null, "id": "50324d40", "metadata": {"tags": ["draft", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>EXERCISE: L14.3.2\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "def num_stdev(iNTrain,iNVal,loss1_array, loss2_array):\n", "    #convert from list to array\n", "    loss1_array = np.array(loss1_array)\n", "    loss2_array = np.array(loss2_array)\n", "    \n", "    sigma_loss1 = #YOUR CODE HERE (the stdev of the training loss)\n", "    sigma_loss2 = #YOUR CODE HERE (the stdev of the validation loss)\n", "    \n", "    #the combined uncertainty\n", "    sigma_tot = np.sqrt(sigma_loss1**2. + sigma_loss2**2.) \n", "    \n", "    #the difference in losses\n", "    delta = loss2_array-loss1_array\n", "    \n", "    #calculate the difference in terms of number of standard deviations\n", "    diff = abs(delta/sigma_tot) \n", "    \n", "    return diff\n", "\n", "#plot\n", "#----------------------------------------------------\n", "N_train = len(trainloader)*trainloader.batch_size\n", "N_val   = len(valloader)*valloader.batch_size #the number of rows in the data set\n", "diff_sig = num_stdev(N_train,N_val,history_lr['loss'], history_lr['val_loss'])\n", "print(\"Significance of last epoch\",diff_sig[-1])\n", "\n", "\n", "plt.plot(np.arange(len(diff_sig)),diff_sig)\n", "plt.xlabel(\"N-iteration\")\n", "plt.ylabel(\"(train-test)/$\\sigma$\")\n", "plt.show()"]}, {"cell_type": "markdown", "id": "87e047a0", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-14.3.3</span>\n", "\n", "A common way to select events based on a particular final discriminator value from the neural network, is to make normalized histograms (i.e. histograms with the same integral), as is done in the previous examples, and then to select only events above the line where the signal and background histograms cross. For the histogram produced by code cell `L14.3-runcell06`, what fraction of egamma events (blue) or background (red) are above the bin of intersection (you should see this intersection occur at a value of `intersection_bin = 0.20`)?\n", "\n", "Report your answer as a list of numbers with precision 1e-2: `[frac EG, frac PU]`\n"]}, {"cell_type": "code", "execution_count": null, "id": "93aca6a3", "metadata": {"tags": ["draft", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>EXERCISE: L14.3.3\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "#determine fraction of events above intersection\n", "intersection_bin = 0.20\n", "EG_frac = #YOUR CODE HERE\n", "PU_frac = #YOUR CODE HERE\n", "\n", "print(\"EG\", EG_frac)\n", "print(\"PU:\", PU_frac)"]}, {"cell_type": "markdown", "id": "892f0941", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": [">#### Follow-up 14.3.3a (ungraded)\n", ">\n", ">Define a two-weight network, as we had in the last Lesson, and run the training of your two-weight network on the data. What do the resulting weights look like? Can you make a 1D histogram of the separation? Play with the smearing parameter, how do things change?\n", ">\n", ">**NOTE:** Be sure to label your classes, functions, and outputs differently. We will continue to use the results from above, so do not get your previous results confused with your results from this follow-up exercise!"]}, {"cell_type": "code", "execution_count": null, "id": "c59b2ab7", "metadata": {"tags": ["draft", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>EXERCISE: L14.3.3a\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "class LR_net_2(torch.nn.Module):\n", "    #YOUR CODE HERE\n", "\n", "        \n", "model_lr_2 = LR_net_2()\n", "print(model_lr_2)\n", "print('----------')\n", "print(model_lr_2.state_dict())\n", "\n", "\n", "#-----------------\n", "#TRAIN THE NETWORK\n", "#YOUR CODE HERE"]}, {"cell_type": "markdown", "id": "b32c3d56", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_14_4'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L14.4 Adding a Hidden Layer</h2>     \n", "\n", "| [Top](#section_14_0) | [Previous Section](#section_14_3) | [Exercises](#exercises_14_4) | [Next Section](#section_14_5) |\n"]}, {"cell_type": "code", "execution_count": null, "id": "252a28eb", "metadata": {"tags": ["learner", "py", "lect_04", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L14.4-runcell01\n", "\n", "class MLP2_net(torch.nn.Module):\n", "    def __init__(self):\n", "        super().__init__()\n", "        self.fc1 = torch.nn.Linear(ninputs,30)\n", "        self.act1 = torch.nn.ReLU()\n", "        self.fc2 = torch.nn.Linear(30,10)\n", "        self.act2 = torch.nn.ReLU()\n", "        self.fc3 = torch.nn.Linear(10,1)\n", "        self.output = torch.nn.Sigmoid()\n", "\n", "    def forward(self, x):\n", "        x = self.fc1(x)\n", "        x = self.act1(x)\n", "        x = self.fc2(x)\n", "        x = self.act2(x)\n", "        x = self.fc3(x)\n", "        x = self.output(x)\n", "        return x\n", "    \n", "torch.random.manual_seed(42)  # fix a random seed for reproducibility\n", "\n", "model_mlp_2layer = MLP2_net()\n", "print(model_mlp_2layer)"]}, {"cell_type": "code", "execution_count": null, "id": "e526d8b1", "metadata": {"tags": ["learner", "py", "lect_04", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L14.4-runcell02\n", "\n", "history_mlp_2layer = train(model_mlp_2layer,trainloader,valloader,name='mlp_2layer_model')\n", "Y_test_predict_mlp_2layer, Y_test = apply(model_mlp_2layer, testloader)"]}, {"cell_type": "code", "execution_count": null, "id": "aba3384f", "metadata": {"tags": ["learner", "py", "lect_04", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L14.4-runcell03\n", "\n", "plt.semilogy(history_mlp_2layer['loss'], label='loss')\n", "plt.semilogy(history_mlp_2layer['val_loss'], label='val_loss')\n", "plt.legend(loc=\"upper right\")\n", "plt.xlabel('epoch')\n", "plt.ylabel('loss (binary crossentropy)')\n", "plt.show()\n", "\n", "plt.hist(Y_test_predict_mlp_2layer[Y_test==0],histtype='step',color='r',density=True)\n", "plt.hist(Y_test_predict_mlp_2layer[Y_test==1],histtype='step',color='b',density=True)\n", "plt.xlabel('MLP (2 hidden layers) Discriminant')\n", "plt.show()"]}, {"cell_type": "code", "execution_count": null, "id": "cabab175", "metadata": {"tags": ["learner", "py", "lect_04", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L14.4-runcell04\n", "\n", "print(\"Signal\",len(Y_test_predict_mlp_2layer[Y_test==1][Y_test_predict_mlp_2layer[Y_test==1] > 0.10])/len(Y_test_predict_mlp_2layer[Y_test==1]))\n", "print(\"Big:\"  ,len(Y_test_predict_mlp_2layer[Y_test==0][Y_test_predict_mlp_2layer[Y_test==0] > 0.10])/len(Y_test_predict_mlp_2layer[Y_test==0]))"]}, {"cell_type": "code", "execution_count": null, "id": "e9460713", "metadata": {"tags": ["learner", "py", "lect_04", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L14.4-runcell05\n", "\n", "def compute_ROC(labels, predicts, npts=101):\n", "    cutvals = np.linspace(0.,1.,num=npts)\n", "    tot0 = float(len(labels[labels==0]))\n", "    tot1 = float(len(labels[labels==1]))\n", "    tpr = []\n", "    fpr = []\n", "    for c in cutvals:\n", "        fpr.append(float(len(predicts[(labels==0) & (predicts>c)]))/tot0)\n", "        tpr.append(float(len(predicts[(labels==1) & (predicts>c)]))/tot1)\n", "    \n", "    return np.array(fpr),np.array(tpr)\n", "\n", "mlp_2layer_rocpts = compute_ROC(Y_test,Y_test_predict_mlp_2layer)\n", "lr_rocpts = compute_ROC(Y_test,Y_test_predict_lr)\n", "\n", "plt.plot(mlp_2layer_rocpts[0],mlp_2layer_rocpts[1],'g-',label=\"MLP (2 hidden layers)\")\n", "plt.plot(lr_rocpts[0],lr_rocpts[1],'m--',label=\"Logistic Regression\")\n", "plt.title(\"ROC (Receiver Operating Characteristic) Curve\")\n", "plt.xlabel(\"False Positive Rate (FPR) aka Background Efficiency\")\n", "plt.ylabel(\"True Positive Rate (TPR) aka Signal Efficiency\")\n", "plt.legend(loc=\"lower right\")\n", "plt.show()"]}, {"cell_type": "markdown", "id": "96231368", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='exercises_14_4'></a>   \n", "\n", "| [Top](#section_14_0) | [Restart Section](#section_14_4) | [Next Section](#section_14_5) |\n"]}, {"cell_type": "markdown", "id": "37c7fb43", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-14.4.1</span>\n", "\n", "When we compare the performance of two algorithms, we often like to fix the signal efficiency and look at the change in the background rejection rate. For a fixed signal efficiency of 97%, what is the fractional reduction in the false-positive rate between the logistic and MLP networks (i.e. the difference between the two divided by the value for the logistic)? Report your answer as a number with precision 5e-2."]}, {"cell_type": "code", "execution_count": null, "id": "0845f595", "metadata": {"tags": ["draft", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>EXERCISE: L14.4.1\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "\n", "def frac_reduc_fpr(lr_rocpts, mlp_2layer_rocpts, sig_eff=0.97):\n", "    #false-positive-rate (background efficiency)\n", "    #true-positive-rate (signal efficiency):\n", "    lr_fpr = lr_rocpts[0]\n", "    lr_tpr = lr_rocpts[1]\n", "    mlp_2layer_fpr = mlp_2layer_rocpts[0]\n", "    mlp_2layer_tpr = mlp_2layer_rocpts[1]\n", "    \n", "    #find lr_fpr where lr_tpr is closest to sig_eff\n", "    lr_fpr_val = #YOUR CODE HERE\n", "    \n", "    #find mlp_2layer_fpr where lr_tpr is closest to sig_eff\n", "    mlp_2layer_fpr_val = #YOUR CODE HERE\n", "    \n", "    #calculate the fractional reduction in false-positive-rate\n", "    frac_red = #YOUR CODE HERE\n", "    \n", "    return frac_red\n", "\n", "#find where the signal efficiency is 97%\n", "#find difference in logistic vs. MLP\n", "print(\"Fractional Reduction in FPR:\",frac_reduc_fpr(lr_rocpts, mlp_2layer_rocpts, 0.97))"]}, {"cell_type": "markdown", "id": "d11a16fc", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_14_5'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L14.5 Regularization, Batch Normalization, and Dropout</h2>     \n", "\n", "| [Top](#section_14_0) | [Previous Section](#section_14_4) | [Exercises](#exercises_14_5) |\n"]}, {"cell_type": "code", "execution_count": null, "id": "b60a0696", "metadata": {"tags": ["learner", "py", "learner_chopped", "lect_05"]}, "outputs": [], "source": ["#>>>RUN: L14.5-runcell01\n", "\n", "class MLP3_net(torch.nn.Module):\n", "    def __init__(self):\n", "        super().__init__()\n", "        self.fc1 = torch.nn.Linear(ninputs,50)\n", "        self.act1 = torch.nn.ReLU()\n", "        self.fc2 = torch.nn.Linear(50,30)\n", "        self.act2 = torch.nn.ReLU()\n", "        self.fc3 = torch.nn.Linear(30,10)\n", "        self.act3 = torch.nn.ReLU()\n", "        self.fc4 = torch.nn.Linear(10,1)\n", "        self.output = torch.nn.Sigmoid()\n", "\n", "    def forward(self, x):\n", "        x = self.fc1(x)\n", "        x = self.act1(x)\n", "        x = self.fc2(x)\n", "        x = self.act2(x)\n", "        x = self.fc3(x)\n", "        x = self.act3(x)\n", "        x = self.fc4(x)\n", "        x = self.output(x)\n", "        return x\n", "    \n", "torch.random.manual_seed(42)  # fix a random seed for reproducibility\n", "model_mlp_3layer = MLP3_net()\n", "print(model_mlp_3layer)"]}, {"cell_type": "code", "execution_count": null, "id": "b0ca1ba8", "metadata": {"tags": ["learner", "py", "learner_chopped", "lect_05"]}, "outputs": [], "source": ["#>>>RUN: L14.5-runcell02\n", "\n", "# NOTE: we add the l2 regularization to the optimzer by setting l2reg=0.0001, which modifies the loss.\n", "# The l2 regularization is already implemented in the train() function, with default value l2reg=0.\n", "\n", "#you may choose to call the model again, if you have already run it\n", "#model_mlp_3layer.load_state_dict(torch.load('mlp_3layer_model.pt'))\n", "\n", "history_mlp_3layer = train(model_mlp_3layer,trainloader,valloader,l2reg=0.0001,name='mlp_3layer_model')\n", "Y_test_predict_mlp_3layer, Y_test = apply(model_mlp_3layer, testloader)"]}, {"cell_type": "code", "execution_count": null, "id": "df6a39b3", "metadata": {"tags": ["learner", "py", "learner_chopped", "lect_05"]}, "outputs": [], "source": ["#>>>RUN: L14.5-runcell03\n", "\n", "mlp_2layer_rocpts = compute_ROC(Y_test,Y_test_predict_mlp_2layer)\n", "mlp_3layer_rocpts = compute_ROC(Y_test,Y_test_predict_mlp_3layer)\n", "lr_rocpts = compute_ROC(Y_test,Y_test_predict_lr)\n", "\n", "plt.plot(mlp_2layer_rocpts[0],mlp_2layer_rocpts[1],'g-',label=\"MLP (2 hidden layers)\")\n", "plt.plot(mlp_3layer_rocpts[0],mlp_3layer_rocpts[1],'--',color='orange',label=\"MLP (3 hidden layers)\")\n", "plt.plot(lr_rocpts[0],lr_rocpts[1],'m--',label=\"Logistic Regression\")\n", "plt.title(\"ROC (Receiver Operating Characteristic) Curve\")\n", "plt.xlabel(\"False Positive Rate (FPR) aka Background Efficiency\")\n", "plt.ylabel(\"True Positive Rate (TPR) aka Signal Efficiency\")\n", "plt.legend(loc=\"lower right\")\n", "plt.show()"]}, {"cell_type": "code", "execution_count": null, "id": "4556e191", "metadata": {"tags": ["learner", "py", "learner_chopped", "lect_05"]}, "outputs": [], "source": ["#>>>RUN: L14.5-runcell04\n", "\n", "mlp_2layer_rocpts = compute_ROC(Y_test,Y_test_predict_mlp_2layer,101)\n", "mlp_3layer_rocpts = compute_ROC(Y_test,Y_test_predict_mlp_3layer,101)\n", "lr_rocpts = compute_ROC(Y_test,Y_test_predict_lr,101)\n", "\n", "plt.plot(1./mlp_2layer_rocpts[0],mlp_2layer_rocpts[1],'g-',label=\"MLP (2 hidden layers)\")\n", "plt.plot(1./mlp_3layer_rocpts[0],mlp_3layer_rocpts[1],'--',color='orange',label=\"MLP (3 hidden layers)\")\n", "plt.plot(1./lr_rocpts[0],lr_rocpts[1],'m--',label=\"Logistic Regression\")\n", "plt.xlim([-1, 2500])\n", "plt.title(\"ROC (Receiver Operating Characteristic) Curve\")\n", "plt.xlabel(\"1/(Background Efficiency)\")\n", "plt.ylabel(\"True Positive Rate (TPR) aka Signal Efficiency\")\n", "plt.legend(loc=\"upper right\")\n", "plt.show()"]}, {"cell_type": "code", "execution_count": null, "id": "668597db", "metadata": {"tags": ["learner", "py", "learner_chopped", "lect_05"]}, "outputs": [], "source": ["#>>>RUN: L14.5-runcell05\n", "\n", "class MLP3_BN_net(torch.nn.Module):\n", "    def __init__(self):\n", "        super().__init__()\n", "        self.bn0 = torch.nn.BatchNorm1d(ninputs)\n", "        self.fc1 = torch.nn.Linear(ninputs,50)\n", "        self.act1 = torch.nn.ReLU()\n", "        self.bn1 = torch.nn.BatchNorm1d(50)\n", "        self.fc2 = torch.nn.Linear(50,30)\n", "        self.act2 = torch.nn.ReLU()\n", "        self.bn2 = torch.nn.BatchNorm1d(30)\n", "        self.fc3 = torch.nn.Linear(30,10)\n", "        self.act3 = torch.nn.ReLU()\n", "        self.bn3 = torch.nn.BatchNorm1d(10)\n", "        self.fc4 = torch.nn.Linear(10,1)\n", "        self.output = torch.nn.Sigmoid()\n", "\n", "    def forward(self, x):\n", "        x = self.bn0(x)\n", "        x = self.fc1(x)\n", "        x = self.act1(x)\n", "        x = self.bn1(x)\n", "        x = self.fc2(x)\n", "        x = self.act2(x)\n", "        x = self.bn2(x)\n", "        x = self.fc3(x)\n", "        x = self.act3(x)\n", "        x = self.bn3(x)\n", "        x = self.fc4(x)\n", "        x = self.output(x)\n", "        return x\n", "    \n", "class MLP3_Drop_net(torch.nn.Module):\n", "    def __init__(self):\n", "        super().__init__()\n", "        self.fc1 = torch.nn.Linear(ninputs,50)\n", "        self.act1 = torch.nn.ReLU()\n", "        self.drop1 = torch.nn.Dropout(0.1)\n", "        self.fc2 = torch.nn.Linear(50,30)\n", "        self.act2 = torch.nn.ReLU()\n", "        self.drop2 = torch.nn.Dropout(0.1)\n", "        self.fc3 = torch.nn.Linear(30,10)\n", "        self.act3 = torch.nn.ReLU()\n", "        self.drop3 = torch.nn.Dropout(0.1)\n", "        self.fc4 = torch.nn.Linear(10,1)\n", "        self.output = torch.nn.Sigmoid()\n", "\n", "    def forward(self, x):\n", "        x = self.fc1(x)\n", "        x = self.act1(x)\n", "        x = self.drop1(x)\n", "        x = self.fc2(x)\n", "        x = self.act2(x)\n", "        x = self.drop2(x)\n", "        x = self.fc3(x)\n", "        x = self.act3(x)\n", "        x = self.drop3(x)\n", "        x = self.fc4(x)\n", "        x = self.output(x)\n", "        return x\n", "\n", "torch.random.manual_seed(42)  # fix a random seed for reproducibility\n", "model_mlp_3layer_bn = MLP3_BN_net()\n", "print(model_mlp_3layer_bn)\n", "\n", "torch.random.manual_seed(42)  # fix a random seed for reproducibility\n", "model_mlp_3layer_drop = MLP3_Drop_net()\n", "print(model_mlp_3layer_drop)"]}, {"cell_type": "code", "execution_count": null, "id": "9a280529", "metadata": {"tags": ["learner", "py", "learner_chopped", "lect_05"]}, "outputs": [], "source": ["#>>>RUN: L14.5-runcell06\n", "\n", "history_mlp_3layer_bn = train(model_mlp_3layer_bn,trainloader,valloader,name='mlp_3layer_bn_model')\n", "Y_test_predict_mlp_3layer_bn, Y_test = apply(model_mlp_3layer_bn, testloader)"]}, {"cell_type": "code", "execution_count": null, "id": "be525e1d", "metadata": {"tags": ["learner", "py", "learner_chopped", "lect_05"]}, "outputs": [], "source": ["#>>>RUN: L14.5-runcell07\n", "\n", "history_mlp_3layer_drop = train(model_mlp_3layer_drop,trainloader,valloader,name='mlp_3layer_drop_model')\n", "Y_test_predict_mlp_3layer_drop, Y_test = apply(model_mlp_3layer_drop, testloader)"]}, {"cell_type": "code", "execution_count": null, "id": "9bfb384c", "metadata": {"tags": ["learner", "py", "learner_chopped", "lect_05"]}, "outputs": [], "source": ["#>>>RUN: L14.5-runcell08\n", "\n", "mlp_3layer_rocpts = compute_ROC(Y_test,Y_test_predict_mlp_3layer,101)\n", "mlp_3layer_bn_rocpts = compute_ROC(Y_test,Y_test_predict_mlp_3layer_bn,101)\n", "mlp_3layer_drop_rocpts = compute_ROC(Y_test,Y_test_predict_mlp_3layer_drop,101)\n", "\n", "fig, (ax1, ax2) = plt.subplots(1,2,figsize=(12,4))\n", "\n", "ax1.plot(mlp_3layer_rocpts[0],mlp_3layer_rocpts[1],'--',color='orange',label=\"MLP (3 hidden layers)\")\n", "ax1.plot(mlp_3layer_bn_rocpts[0],mlp_3layer_bn_rocpts[1],'--',color='brown',label=\"MLP (3 hidden layers w/ BN)\")\n", "ax1.plot(mlp_3layer_drop_rocpts[0],mlp_3layer_drop_rocpts[1],'--',color='cyan',label=\"MLP (3 hidden layers w/ Dropout)\")\n", "ax1.set_title(\"ROC (Receiver Operating Characteristic) Curve\")\n", "ax1.set_xlabel(\"Bkg Eff\")\n", "ax1.set_ylabel(\"Sig Eff\")\n", "ax1.legend(loc=\"lower right\")\n", "\n", "ax2.plot(1./mlp_3layer_rocpts[0],mlp_3layer_rocpts[1],'--',color='orange',label=\"MLP (3 hidden layers)\")\n", "ax2.plot(1./mlp_3layer_bn_rocpts[0],mlp_3layer_bn_rocpts[1],'--',color='brown',label=\"MLP (3 hidden layers w/ BN)\")\n", "ax2.plot(1./mlp_3layer_drop_rocpts[0],mlp_3layer_drop_rocpts[1],'--',color='cyan',label=\"MLP (3 hidden layers w/ Dropout)\")\n", "ax2.set_title(\"ROC (Receiver Operating Characteristic) Curve\")\n", "ax2.set_xlabel(\"1/Bkg Eff\")\n", "ax2.set_xlim([-1, 2500])\n", "ax2.set_ylabel(\"Sig Eff\")\n", "ax2.legend(loc=\"upper right\")\n", "\n", "plt.show()"]}, {"cell_type": "markdown", "id": "5f3df180", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='exercises_14_5'></a>   \n", "\n", "| [Top](#section_14_0) | [Restart Section](#section_14_5) |\n"]}, {"cell_type": "markdown", "id": "36a59ef1", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-14.5.1</span>\n", "\n", "Which of the following statements is true about regularization in machine learning?\n", "\n", "A) Regularization is a technique used to increase the complexity of a model to improve its performance on new data.\\\n", "B) Regularization is a technique used to prevent overfitting by employing a variety of methods, including adding a penalty term to the loss function.\\\n", "C) Regularization is a technique used to reduce the size of the training data to improve generalization performance.\\\n", "D) Regularization is a technique used to randomly drop out neurons in a neural network during training.\\\n", "E) Regularization is a technique that normalizes the mean and variance of the activations of each layer in a neural network.\n"]}, {"cell_type": "markdown", "id": "b32128e7", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-14.5.2</span>\n", "\n", "For a fixed signal efficiency of 97%, what is the fractional reduction in rejection rate going from a 3 hidden layer model to ones adding either batch normalization or dropout? Complete the code below to do this calculation, then report your answer as a list of numbers with precision 1e-2: `[reduction with batch norm, reduction with dropout]`\n", "\n", "**HINT:** Use the function that you defined previously."]}, {"cell_type": "code", "execution_count": null, "id": "b4afa3ea", "metadata": {"tags": ["draft", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>EXERCISE: L14.5.2\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "def frac_reduc_fpr(array1, array2, sig_eff=0.97):\n", "    #false-positive-rate (background efficiency)\n", "    #true-positive-rate (signal efficiency):\n", "    array1_fpr = array1[0]\n", "    array1_tpr = array1[1]\n", "    array2_fpr = array2[0]\n", "    array2_tpr = array2[1]\n", "    \n", "    #find array_1_fpr where array_1_tpr is closest to sig_eff\n", "    array1_fpr_val = #YOUR CODE HERE\n", "    \n", "    #find array_2_fpr where array_2_tpr is closest to sig_eff\n", "    array2_fpr_val = #YOUR CODE HERE\n", "    \n", "    #calculate the fractional reduction in false-positive-rate\n", "    frac_red = #YOUR CODE HERE\n", "    \n", "    return frac_red\n", "\n", "\n", "#find where the signal efficiency is 97%\n", "print(\"Fractional Reduction from 3Layer to Batch Norm :\",frac_reduc_fpr(mlp_3layer_rocpts, mlp_3layer_bn_rocpts, 0.97))\n", "print(\"Fractional Reduction from 3Layer to Dropout:\",frac_reduc_fpr(mlp_3layer_rocpts, mlp_3layer_drop_rocpts, 0.97))"]}, {"cell_type": "markdown", "id": "f9e6b695", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-14.5.3</span>\n", "\n", "Try including both batch normalization *and* dropout. Do you get even better performance? Complete the code below to define and run this new model, then select the best answer from the following:\n", "\n", "A) Yes, it's definitely better to combine both regularization methods.\\\n", "B) No, combining methods was worse than models using either one separately.\\\n", "C) The combined model was maybe better than one model, but not better than both.       "]}, {"cell_type": "code", "execution_count": null, "id": "4d8184e4", "metadata": {"tags": ["draft", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>EXERCISE: L14.5.3\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "class MLP3_BN_Dropout_net(torch.nn.Module):\n", "    #YOUR CODE HERE\n", "\n", "    \n", "torch.random.manual_seed(42)  # fix a random seed for reproducibility\n", "model_mlp_3layer_bn_drop = MLP3_BN_Dropout_net()\n", "print(model_mlp_3layer_bn)\n", "\n", "history_mlp_3layer_bn_drop = train(model_mlp_3layer_bn_drop,trainloader,valloader,name='mlp_3layer_bn_drop_model',nepochs=100)\n", "Y_test_predict_mlp_3layer_bn_drop, Y_test = apply(model_mlp_3layer_bn_drop, testloader)\n", "mlp_3layer_bn_drop_rocpts = compute_ROC(Y_test,Y_test_predict_mlp_3layer_bn_drop,501)\n", "    \n", "#find where the signal efficiency is 97%\n", "print(\"Fractional Reduction from Batch Norm to Combined Model:\",frac_reduc_fpr(mlp_3layer_bn_rocpts, mlp_3layer_bn_drop_rocpts, 0.97))\n", "print(\"Fractional Reduction from Dropout to Combined Model:\",frac_reduc_fpr(mlp_3layer_drop_rocpts, mlp_3layer_bn_drop_rocpts, 0.97))"]}], "metadata": {"celltoolbar": "Tags", "kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}