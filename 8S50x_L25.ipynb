{"cells": [{"cell_type": "markdown", "id": "4930b1e3", "metadata": {"tags": ["md", "learner", "learner_chopped"]}, "source": ["<hr style=\"height: 1px;\">\n", "<i>This notebook was authored by the 8.S50x Course Team, Copyright 2022 MIT All Rights Reserved.</i>\n", "<hr style=\"height: 1px;\">\n", "<br>\n", "\n", "<h1>Lesson 25: Anomaly Detection</h1>"]}, {"cell_type": "markdown", "id": "9f23fcdd", "metadata": {"tags": ["md", "learner", "learner_chopped"]}, "source": ["<a name='section_25_0'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L25.0 Overview</h2>\n"]}, {"cell_type": "markdown", "id": "22ed1587", "metadata": {"tags": ["md", "learner", "learner_chopped"]}, "source": ["<h3>Navigation</h3>\n", "\n", "<table style=\"width:100%\">\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_25_1\">25.1 The Gaia Experiment</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_25_1\">L25.1 Exercises</a></td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_25_2\">25.2 Building Projections with Gaia</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_25_2\">L25.2 Exercises</a></td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_25_3\">25.3 Anomaly Detection with Gaia</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_25_3\">L25.3 Exercises</a></td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_25_4\">25.4 Anomaly Detection with Lots of Gaia Data</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_25_4\">L25.4 Exercises</a></td>\n", "    </tr>\n", "</table>"]}, {"cell_type": "markdown", "id": "f887cc78", "metadata": {"tags": ["catsoop_00", "learner", "md"]}, "source": ["<h3>Slides</h3>\n", "\n", "You can access the slides related to this lecture at the following link: <a href=\"https://github.com/mitx-8s50/slides/raw/main/module3_slides/L25_slides.pdf\" target=\"_blank\">L25 Slides</a>"]}, {"cell_type": "markdown", "id": "9e26101a", "metadata": {"tags": ["catsoop_00", "learner"]}, "source": ["<h3>Learning Objectives</h3>\n", "\n", "This lecture is going to review AI based anomaly detection using an astophysics dataset. For this dataset we are going to use data from the Gaia satellite. The Gaia dataset is a rich dataset that has previously been used for anomaly detection, as detailed in the papers below:\n", "\n", ">source: https://arxiv.org/abs/2303.01529<br>\n", ">attribution: Shih et al., arXiv:2303.01529 [astro-ph.GA] (2023)\n", "\n", ">source: https://arxiv.org/abs/2104.12789 <br>\n", ">attribution: Shih et al., arXiv:2104.12789 [astro-ph.GA] (2021)  \n", "\n", "\n", "\n", "Now lets load a toolkit to process Gaia data"]}, {"cell_type": "markdown", "id": "b673edd5", "metadata": {"tags": ["md", "learner"]}, "source": ["<h3>Data</h3>\n", "\n", "Download the directory where we will save data."]}, {"cell_type": "markdown", "id": "23407f65", "metadata": {"tags": ["md", "learner"]}, "source": ["<h3>Importing Libraries</h3>\n", "\n", "Before beginning, run the cell below to import the relevant libraries for this notebook. "]}, {"cell_type": "code", "execution_count": null, "id": "f1a51921", "metadata": {"tags": ["py", "learner", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L25.0-runcell02\n", "\n", "# astropy imports\n", "import astropy.coordinates as coord\n", "from astropy.table import QTable\n", "import astropy.units as u\n", "from astroquery.gaia import Gaia\n", "\n", "# Third-party imports\n", "import matplotlib as mpl\n", "import matplotlib.pyplot as plt\n", "import numpy as np\n", "%matplotlib inline\n", "\n", "# gala imports\n", "import gala.coordinates as gc\n", "import gala.dynamics as gd\n", "import gala.potential as gp\n", "from gala.units import galactic\n", "\n", "#ML imports\n", "import torch\n", "import torch.nn as nn\n", "from torch.utils.data import Dataset\n", "from torch.autograd import Variable\n"]}, {"cell_type": "markdown", "id": "775e22c9", "metadata": {"tags": ["md", "learner"]}, "source": ["<h3>Setting Default Figure Parameters</h3>\n", "\n", "The following code cell sets default values for figure parameters.\n"]}, {"cell_type": "code", "execution_count": null, "id": "c8b9d625", "metadata": {"tags": ["py", "learner", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L25.0-runcell03\n", "\n", "#set plot resolution\n", "%config InlineBackend.figure_format = 'retina'\n", "\n", "#set default figure parameters\n", "plt.rcParams['figure.figsize'] = (9,6)\n", "\n", "medium_size = 12\n", "large_size = 15\n", "\n", "plt.rc('font', size=medium_size)          # default text sizes\n", "plt.rc('xtick', labelsize=medium_size)    # xtick labels\n", "plt.rc('ytick', labelsize=medium_size)    # ytick labels\n", "plt.rc('legend', fontsize=medium_size)    # legend\n", "plt.rc('axes', titlesize=large_size)      # axes title\n", "plt.rc('axes', labelsize=large_size)      # x and y labels\n", "plt.rc('figure', titlesize=large_size)    # figure title"]}, {"cell_type": "markdown", "id": "bcb48e25", "metadata": {"tags": ["md", "learner", "learner_chopped"]}, "source": ["<a name='section_25_1'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\"> L25.1 The Gaia Experiment </h2>  \n", "\n", "| [Top](#section_25_0) | [Previous Section](#section_25_0) | [Exercises](#exercises_25_1) | [Next Section](#section_25_2) |"]}, {"cell_type": "markdown", "id": "05890daf", "metadata": {"tags": ["learner"]}, "source": ["*The material in this section is discussed in the video **<a href=\"https://courses.mitxonline.mit.edu/learn/course/course-v1:MITxT+8.S50.3x+3T2023/block-v1:MITxT+8.S50.3x+3T2023+type@sequential+block@seq_LS25/block-v1:MITxT+8.S50.3x+3T2023+type@vertical+block@vert_LS25_vid1\" target=\"_blank\">HERE</a>.** You are encouraged to watch that video and use this notebook concurrently.*"]}, {"cell_type": "markdown", "id": "317b9cbd", "metadata": {"tags": ["lect_01", "learner"]}, "source": ["<h3>Overview</h3>\n", "\n", "The Gaia experiment is a satellite experiment launched by the European Space Agency (more information can be found <a href=\"https://www.esa.int/Science_Exploration/Space_Science/Gaia\" target=\"_blank\">here</a> and <a href=\"https://www.esa.int/Science_Exploration/Space_Science/Gaia_overview\" target=\"_blank\">here</a>). This experiment has been running for the last 10 years and has been focused on agglomerating nearby star data. There have been 3 data releases since the onset of the experiment, with the first data coming from 2016. This experiment has had a large impact on our knowledge of the local galaxy by enabling a detailed catalog of nearby stars in the Milky Way.\n"]}, {"cell_type": "markdown", "id": "c1f23f6a", "metadata": {"tags": ["lect_01", "learner"]}, "source": ["<h3>Loading the Data</h3>\n", "\n", "Before we do anything, we need to load Gaia data. To do this, we make a query to the Gaia database selecting the first 4096 stars that have a parallax significance > 10, a parallax > 10, and for which the velocity has been calculated. Fortunately, this is relatively easy to do since its an SQL style query that we can perform in python. Requiring a high parallax significance means that the parallax is very well determined by the measurements. As explained below, requiring a larger parallax selects stars that are relatively close to the Sun.\n", "\n", "We are going to extract all the info that the Gaia database provide by default, namely:\n", "\n", "  * Coordinates (ra,dec)\n", "  * Parallax (which yields distance)\n", "  * Radial velocity\n", "  * Magnitude of the light through 3 filters (general, blue and red)\n", "  \n", "With this information, we can build a good understanding of stellar data.  One other small note is that we will use Gaia Data Release 3 (GaiaDR3), giving us the latest greatest data to study. Later on, we can download more data, but we'll start small for now.\n", "\n", ">source:\\\n", ">This work has made use of data from the European Space Agency (ESA) mission *Gaia* https://www.cosmos.esa.int/gaia, processed by the *Gaia* Data Processing and Analysis Consortium (DPAC, https://www.cosmos.esa.int/web/gaia/dpac/consortium). Funding for the DPAC has been provided by national institutions, in particular the institutions participating in the *Gaia* Multilateral Agreement.\n", ">\n", ">related papers:\\\n", ">Gaia Collaboration, T. Prusti, J.H.J. de Bruijne, et al. (2016b) The Gaia mission. A&A 595, pp. A1.\\\n", ">Gaia Collaboration, A. Vallenari, A. G. A. Brown, et al. (2023j) Gaia Data Release 3. Summary of the content and survey properties. A&A 674, pp. A1.\\\n", ">C. Babusiaux, C. Fabricius, S. Khanna, et al. (2023) Gaia Data Release 3. Catalogue validation. A&A 674, pp. A32.\n"]}, {"cell_type": "code", "execution_count": null, "id": "8fe9cb80", "metadata": {"tags": ["lect_01", "learner", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L25.1-runcell01\n", "\n", "query_text = '''SELECT TOP 4096 ra, dec, parallax, pmra, pmdec, radial_velocity,\n", "phot_g_mean_mag, phot_bp_mean_mag, phot_rp_mean_mag\n", "FROM gaiadr3.gaia_source\n", "WHERE parallax_over_error > 10 AND\n", "    parallax > 10 AND\n", "    radial_velocity IS NOT null\n", "ORDER BY random_index\n", "'''\n", "\n", "job = Gaia.launch_job(query_text)\n", "gaia_data = job.get_results()\n", "gaia_data.write('data/L25/gaia_data1.fits',overwrite=True)\n", "\n", "#Note, if you return to this section after closing the kernel,\n", "#you can load the data again using the following code\n", "gaia_data = QTable.read('data/L25/gaia_data1.fits')\n", "\n", "\n", "print(\"Total Events:\",len(gaia_data))\n", "print()\n", "print(gaia_data[:4])\n", "\n", "#Note, the columns in our dataset are defined by our query above.\n", "#To print one column, use the command gaia_data['column_name'], e.g.: gaia_data['parallax']"]}, {"cell_type": "markdown", "id": "09536d67", "metadata": {"tags": ["lect_01", "learner"]}, "source": ["Let's consider the units here. Firstly, astronomical distances have units of parsecs (1 parsec = 3.25 light years). By definition, a parsec is the distance to an object with a paralax of one arcsecond (for reference, the moon subtends an angle of roughly 0.5 degrees in the sky, or about 1800 arcseconds).\n", "\n", "In the dataset that we are using, the units of parallax are reported in milliarcseconds (mas), for which we use the variable $\\theta_{\\rm mas}$. To convert from $\\theta_{\\rm mas}$ to a distance in parsecs is, $d_{\\rm parsec}$, one should divide, as follows:\n", "\n", "$$\n", "d_{\\rm parsec} = \\frac{1}{\\theta_{\\rm mas}\\times10^{-3}}\n", "$$\n", "\n", "\n", "Furthermore, you will also notice in the above dataset, 2 variables use radial velocity, which is the proper motion in milliarcseconds per year. This is given in angular coordinates ra and dec, but we can use parallax to obtain the radial velocity in milliarcseconds per year.\n", "\n", "[GSFS The paragraph above makes no sense to me. First of all, the current printout doesn't show any velocities. Second, there can only be one radial velocity. I assume that this should refer instead to tangential velocity. Third, converting velocity in milliarcseconds per year into milliarcseconds per year. I assume it should instead convert milliarcseconds per year into parsecs per year. Finally, it makes more sense geometrically to say that we use the distance to convert angular velocity to linear velocity (although technically one can get the distance from the parallax). Here's my suggested replacement:]\n", "\n", "This dataset contains 2 variables for tangential velocity, which is the proper motion in angular coordinates ra and dec, using units of milliarcseconds per year. We can use the distance (found from the parallax) to obtain the tangential velocity in parsecs per year.\n", "\n", "The library `astropy` has lots of nice tools to do this conversion for us. One very simple conversion is to calculate the distance."]}, {"cell_type": "code", "execution_count": null, "id": "f442ac54", "metadata": {"tags": ["lect_01", "learner", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L25.1-runcell02\n", "\n", "dist = coord.Distance(parallax=u.Quantity(gaia_data['parallax']))\n", "print(\"Min:\",dist.min(), \"Max:\",dist.max())\n", "print(dist[0],1e3/gaia_data['parallax'][0])\n", "\n", "plt.hist(dist)\n", "plt.xlabel(\"distance (pcs)\")\n", "plt.ylabel(\"N\")\n", "plt.show()"]}, {"cell_type": "markdown", "id": "3f30bf64", "metadata": {"tags": ["lect_01", "learner"]}, "source": ["You may be surprised that the sun appears to be in a \"hole\" with few stars nearby and lots of stars farther away. However, each bin in this histogram represents a spherical shell with a radius $r$ and width $\\Delta r$, whose volume is $4\\pi r^2 \\Delta r$. So, a uniform density would manifest itself as a quadratic rise in the number of stars as the radius increases.\n", "\n", "Given the position of the Solar System in the Milky way, the angular coordinates ra and dec of the star, and its distance, we can transform the star's location into Galactocentric coordinates. The function used for this conversion also transforms the velocities.\n"]}, {"cell_type": "code", "execution_count": null, "id": "b9b1e98b", "metadata": {"tags": ["lect_01", "learner", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L25.1-runcell03\n", "\n", "c = coord.SkyCoord(ra=gaia_data['ra'], dec=gaia_data['dec'],distance=dist,\n", "                   pm_ra_cosdec=gaia_data['pmra'], pm_dec=gaia_data['pmdec'],\n", "                   radial_velocity=gaia_data['radial_velocity'])\n", "\n", "print(c[:4],'\\n')\n", "print()\n", "\n", "#Now we can translate it to galactic coordinates\n", "print(c.galactic[:4])\n", "coord.Galactocentric()"]}, {"cell_type": "markdown", "id": "b0afa2a9", "metadata": {"tags": ["lect_01", "learner"]}, "source": ["Notice that the distances to the stars don't change when changing coordinate systems. The function used above only changes the angular coordinates into the galactic frame. In order to move the center of our coordinate system, we need to do one more transform.\n", "\n", "To define the Galactocentric coordinate system, the position of the Sun is assumed to be on the $x$ axis. That is, the $x$ axis points from the position of the Sun projected along the Galactic midplane to the Galactic center. The $y$ axis points roughly towards Galactic longitude 90$^{\\circ}$ and the $z$ axis points roughly towards the North Galactic Pole."]}, {"cell_type": "code", "execution_count": null, "id": "ea485305", "metadata": {"tags": ["lect_01", "learner", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L25.1-runcell04\n", "\n", "galcen = c.transform_to(coord.Galactocentric(z_sun=0*u.pc, galcen_distance=8.1*u.kpc))\n", "print(galcen[:4],'\\n')\n", "plt.hist(galcen.z.value, bins=np.linspace(-110, 110, 32),alpha=0.5,label='z')\n", "#plt.hist(galcen.x.value, bins=np.linspace(-110, 110, 32),alpha=0.5)\n", "plt.hist(galcen.y.value, bins=np.linspace(-110, 110, 32),alpha=0.5,label='y')\n", "plt.xlabel('Corrdinate position (about Galactic Plane) [{0:latex_inline}]'.format(galcen.z.unit));\n", "plt.legend()\n", "plt.show()\n", "\n", "plt.hist(galcen.x.value, alpha=0.5)\n", "plt.xlabel('X-position (nearby stars) [{0:latex_inline}]'.format(galcen.x.unit))\n", "plt.legend()\n", "plt.show()\n"]}, {"cell_type": "markdown", "id": "09910589", "metadata": {"tags": ["lect_01", "learner"]}, "source": ["Defining the $x$ axis as pointing to the position of the Sun means that it is located at $(y, z)=(0,0)$, so nearby stars are centered around those coordinates. However, the Sun is very far from the Galactic center, so nearby stars have $x$ coordinates that have a peak roughly similar in shape and width to those for the 2 other coordinates, but with a center which is offset quite a bit from $0$.\n", "\n", "We can also look at the transformed velocities of our stars (below).\n"]}, {"cell_type": "code", "execution_count": null, "id": "6cf57210", "metadata": {"tags": ["lect_01", "learner", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L25.1-runcell05\n", "\n", "fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n", "ax.plot(galcen.v_x.value, galcen.v_y.value,marker='.', linestyle='none', alpha=0.5)\n", "ax.set_xlim(-125, 125)\n", "ax.set_ylim(200-125, 200+125)\n", "ax.set_xlabel('vx [{0:latex_inline}]'.format(u.km/u.s))\n", "ax.set_ylabel('vy [{0:latex_inline}]'.format(u.km/u.s))\n", "plt.show()\n", "\n", "fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n", "plt.plot(galcen.v_x.value, galcen.v_z.value,marker='.', linestyle='none', alpha=0.5)\n", "plt.xlabel('vx [{0:latex_inline}]'.format(u.km/u.s))\n", "plt.ylabel('vz [{0:latex_inline}]'.format(u.km/u.s))\n", "plt.xlim(-125, 125)\n", "plt.ylim(-125, 125)\n", "plt.show()\n"]}, {"cell_type": "markdown", "id": "f350e1ba", "metadata": {"tags": ["lect_01", "learner"]}, "source": ["Notice that the $x$ and $z$ velocities are centered around zero, but the $y$ velocities are not, indicating an average motion in a specific direction. This is the result of the rotation of the Galaxy. The Sun, and all of the stars around it, are moving together in a tangential direction around the Galactic center which, given the definition of the coordinate system, means the $y$ direction.\n", "\n", "Now that we have transformed things, let's do some basic analysis of the different stars to try to understand their properties. The Gaia satellite is capable of taking spectra in 3 different frequency ranges called general, blue, and red. General denotes the magnitude using an inclusive filter, while the blue and red tell us about the magnitude with filters of those two colors. This information is enough for us to start categorizing the stars into  different populations.\n", "\n", "Around 100 years ago, astronomers started classifying the different populations of stars and stumbled upon a classification using a diagram known as the <a href=\"https://en.wikipedia.org/wiki/Hertzsprung%E2%80%93Russell_diagram\" target=\"_blank\">Hertzsprung-Russell diagram</a>. The strategy is to look at the intensity of the star as a function of color (or more accurately as a function of the frequency of the light). It was found that most stars are concentrated in a few specific regions on a plot of intensity versus color. Stars that are bluer tend to be larger and younger. Stars that are redder tend to be older and smaller. Most stars are found along a specific trajectory, called the \"Main Sequence\".\n", "\n", "There are two regions of anomalous stars that are not along this main trajectory. Those that are not very bright but have a blue tint are known as white dwarfs. Likewise, bright red stars are known as red giants. Stars along the main sequence will evolve into red giants, and then into white dwarfs, which is the final stage in stellar evolution.\n", "\n", "Stars are placed on the H-R diagram by taking their magnitudes and correcting for the distance modulus (i.e., $1/r^{2}$ in intensity drop) to find the vertical coordinate on the plot. For the horizontal coordinate, each star's color is found by computing the difference in luminosity between the blue and red filtered light.\n"]}, {"cell_type": "code", "execution_count": null, "id": "421aa070", "metadata": {"tags": ["lect_01", "learner", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L25.1-runcell06\n", "\n", "M_G = gaia_data['phot_g_mean_mag'] - dist.distmod #corrected Star Magnitude (general-distance mod)\n", "BP_RP = gaia_data['phot_bp_mean_mag'] - gaia_data['phot_rp_mean_mag'] #Blue filter - read filter\n", "\n", "plt.plot(BP_RP.value, M_G.value, marker='.', linestyle='none', alpha=0.3)\n", "\n", "plt.xlim(0, 3)\n", "plt.ylim(11, 1)\n", "\n", "#expand range\n", "#plt.xlim(0, 3.75)\n", "#plt.ylim(13.5, 0)\n", "\n", "plt.xlabel('$G_{BP}-G_{RP}$')\n", "plt.ylabel('$M_{G}$')\n", "plt.show()"]}, {"cell_type": "markdown", "id": "3d6dd0d8", "metadata": {"tags": ["lect_01", "learner"]}, "source": ["These local stars are predominantly along the main sequence, but you do see a few red giants at about $(1-1.5,0-4)$. There may also be several white dwarfs around $(1.0-2.0,10)$\n", "\n", "Let's have some fun and see what we can do with this data. One simple thing is to look at where the big bright stars are compared to the small red stars. Specifically, we will characterize a group of large stars using a `BP_RP` color in the range `[0.5,0.7]` and a magnitude in the range `[2,3.75]`. The group of low mass stars will be characterized with a `BP_RP` color in the range `[2,2.4]` and a magnitude in the range `[8.2,9.7]`. First, we'll show exactly where these stars are on our H-R diagram.\n"]}, {"cell_type": "code", "execution_count": null, "id": "54ed5279", "metadata": {"tags": ["lect_01", "learner", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L25.1-runcell07\n", "\n", "np.seterr(invalid=\"ignore\")\n", "#Red+0.7 > Blue > Red+0.5 and 2 < Star magnitude < 3.75\n", "hi_mass_mask = ((BP_RP > 0.5*u.mag) & (BP_RP < 0.7*u.mag) & (M_G > 2*u.mag) & (M_G < 3.75*u.mag))\n", "#                &                 (np.abs(galcen.v_y - 220*u.km/u.s) < 50*u.km/u.s))\n", "\n", "#Red+2.4 > Blue > Red+2 and 8.2 < Star magnitude < 9.7\n", "lo_mass_mask = ((BP_RP > 2*u.mag) & (BP_RP < 2.4*u.mag) & (M_G > 8.2*u.mag) & (M_G < 9.7*u.mag))\n", "#                &                (np.abs(galcen.v_y - 220*u.km/u.s) < 50*u.km/u.s))\n", "\n", "hi_mass_color = 'tab:purple'\n", "lo_mass_color = 'tab:red'\n", "hi_mass_label = 'high mass'\n", "lo_mass_label = 'low mass'\n", "milky_way = gp.MilkyWayPotential()\n", "milky_way\n", "\n", "plt.plot(BP_RP.value, M_G.value, marker='.', linestyle='none', alpha=0.1)\n", "\n", "for mask, color, label in zip([lo_mass_mask, hi_mass_mask],[lo_mass_color, hi_mass_color], [lo_mass_label, hi_mass_label]):\n", "    plt.plot(BP_RP[mask].value, M_G[mask].value, marker='.', linestyle='none', \n", "            alpha=0.5, color=color, label=label)\n", "\n", "plt.xlim(0, 3)\n", "plt.ylim(11, 1)\n", "\n", "plt.xlabel('$G_{BP}-G_{RP}$')\n", "plt.ylabel('$M_{G}$')\n", "plt.legend()\n", "plt.show()\n", "     \n", "     "]}, {"cell_type": "markdown", "id": "6ebe75fb", "metadata": {"tags": ["lect_01", "learner"]}, "source": ["Next, look at the where these stars are found in the galaxy. This plot shows the location of all stars as fainter dots."]}, {"cell_type": "code", "execution_count": null, "id": "3631000d", "metadata": {"tags": ["lect_01", "learner", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L25.1-runcell08\n", "\n", "fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n", "\n", "ax.plot(galcen[1==1].x.value, galcen[1==1].y.value,marker='.', linestyle='none', alpha=0.02,color='blue')\n", "ax.plot(galcen[lo_mass_mask].x.value, galcen[lo_mass_mask].y.value,marker='.', linestyle='none', alpha=0.5,color=lo_mass_color, label=lo_mass_label)\n", "ax.plot(galcen[hi_mass_mask].x.value, galcen[hi_mass_mask].y.value,marker='.', linestyle='none', alpha=0.5,color=hi_mass_color, label=hi_mass_label)\n", "ax.set_xlabel('x [{0:latex_inline}]'.format(galcen.x.unit));\n", "ax.set_ylabel('y [{0:latex_inline}]'.format(galcen.y.unit));\n", "plt.legend()\n", "plt.show()"]}, {"cell_type": "markdown", "id": "c54c185e", "metadata": {"tags": ["lect_01", "learner"]}, "source": ["At first glance, there is no obvious deviation between the different populations visible here. However, I think it's been made clear in this class that simply plotting points doesn't reveal much about broad variations. To look further, we'll plot the distribution of radial and $z$ velocities.\n", "\n", "[GSFS I don't understand this description given what's in the code cell. The first plot seems to be $\\sqrt{y^2+z^2}$ (so not a velocity at all), while the second is $\\sqrt{v_y^2+v_x^2}$ (which is radial only in the $x-y$ plane).]  \n"]}, {"cell_type": "code", "execution_count": null, "id": "3bc0efe4", "metadata": {"tags": ["lect_01", "learner", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L25.1-runcell09\n", "\n", "r_ce = np.sqrt(galcen[1==1]        .y.value**2 + galcen[1==1]        .z.value**2)\n", "r_lo = np.sqrt(galcen[lo_mass_mask].y.value**2 + galcen[lo_mass_mask].z.value**2)\n", "r_hi = np.sqrt(galcen[hi_mass_mask].y.value**2 + galcen[hi_mass_mask].z.value**2)\n", "\n", "hist_ce, bin_edges = np.histogram(r_ce,bins=10, density=True)\n", "hist_lo, bin_edges = np.histogram(r_lo, bins=bin_edges, density=True)\n", "hist_hi, bin_edges = np.histogram(r_hi, bins=bin_edges, density=True)\n", "bin_center = 0.5*(bin_edges[:-1] + bin_edges[1:])\n", "sc=1./(len(galcen))\n", "sl=1./(len(galcen[lo_mass_mask]))\n", "sh=1./(len(galcen[hi_mass_mask]))\n", "plt.errorbar(bin_center,hist_ce,yerr=sc*hist_ce**0.5,alpha=0.5,drawstyle=\"steps-mid\",marker='.',color='blue',label='all')\n", "plt.errorbar(bin_center,hist_lo,yerr=sl*hist_lo**0.5,alpha=0.5,drawstyle=\"steps-mid\",marker='.',color=lo_mass_color,label=lo_mass_label)\n", "plt.errorbar(bin_center,hist_hi,yerr=sh*hist_hi**0.5,alpha=0.5,drawstyle=\"steps-mid\",marker='.',color=hi_mass_color,label=hi_mass_label)\n", "plt.xlabel('y[pc]')\n", "plt.legend()\n", "plt.show()\n", "\n", "\n", "vr_ce = np.sqrt(galcen[1==1]        .v_y.value**2 + galcen[1==1]        .v_x.value**2)\n", "vr_lo = np.sqrt(galcen[lo_mass_mask].v_y.value**2 + galcen[lo_mass_mask].v_x.value**2)\n", "vr_hi = np.sqrt(galcen[hi_mass_mask].v_y.value**2 + galcen[hi_mass_mask].v_x.value**2)\n", "\n", "hist_ce, bin_edges = np.histogram(vr_ce,bins=20, density=True)\n", "hist_lo, bin_edges = np.histogram(vr_lo, bins=bin_edges, density=True)\n", "hist_hi, bin_edges = np.histogram(vr_hi, bins=bin_edges, density=True)\n", "bin_center = 0.5*(bin_edges[:-1] + bin_edges[1:])\n", "sc=1./(len(galcen))\n", "sl=1./(len(galcen[lo_mass_mask]))\n", "sh=1./(len(galcen[hi_mass_mask]))\n", "plt.errorbar(bin_center,hist_ce,yerr=sc*hist_ce**0.5,alpha=0.5,drawstyle=\"steps-mid\",marker='.',color='blue',label='all')\n", "plt.errorbar(bin_center,hist_lo,yerr=sl*hist_lo**0.5,alpha=0.5,drawstyle=\"steps-mid\",marker='.',color=lo_mass_color,label=lo_mass_label)\n", "plt.errorbar(bin_center,hist_hi,yerr=sh*hist_hi**0.5,alpha=0.5,drawstyle=\"steps-mid\",marker='.',color=hi_mass_color,label=hi_mass_label)\n", "plt.xlabel('v$_{r}$[km/s]')\n", "plt.legend()\n", "plt.show()\n", "\n", "\n", "vz_ce = galcen[1==1]        .v_z.value\n", "vz_lo = galcen[lo_mass_mask].v_z.value\n", "vz_hi = galcen[hi_mass_mask].v_z.value\n", "\n", "hist_ce, bin_edges = np.histogram(vz_ce,bins=20, density=True)\n", "hist_lo, bin_edges = np.histogram(vz_lo, bins=bin_edges, density=True)\n", "hist_hi, bin_edges = np.histogram(vz_hi, bins=bin_edges, density=True)\n", "bin_center = 0.5*(bin_edges[:-1] + bin_edges[1:])\n", "sc=1./(len(galcen))\n", "sl=1./(len(galcen[lo_mass_mask]))\n", "sh=1./(len(galcen[hi_mass_mask]))\n", "plt.errorbar(bin_center,hist_ce,yerr=sc*hist_ce**0.5,alpha=0.5,drawstyle=\"steps-mid\",marker='.',color='blue',label='all')\n", "plt.errorbar(bin_center,hist_lo,yerr=sl*hist_lo**0.5,alpha=0.5,drawstyle=\"steps-mid\",marker='.',color=lo_mass_color,label=lo_mass_label)\n", "plt.errorbar(bin_center,hist_hi,yerr=sh*hist_hi**0.5,alpha=0.5,drawstyle=\"steps-mid\",marker='.',color=hi_mass_color,label=hi_mass_label)\n", "plt.xlabel('v$_{z}$[km/s]')\n", "plt.legend()\n", "plt.show()\n"]}, {"cell_type": "markdown", "id": "34831a1d", "metadata": {"tags": ["lect_01", "learner"]}, "source": ["As before, these distributions do not appear by-eye to be different from each other."]}, {"cell_type": "markdown", "id": "45901919", "metadata": {"tags": ["md", "learner", "learner_chopped"]}, "source": ["<a name='exercises_25_1'></a>     \n", "\n", "| [Top](#section_25_0) | [Restart Section](#section_25_1) | [Next Section](#section_25_2) |\n"]}, {"cell_type": "markdown", "id": "0f6ab667", "metadata": {"tags": ["md", "learner", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Exercise 25.1.1</span>\n", "\n", "We've compared the distributions of high mass stars and low mass stars. Now, partition the intermediate mass stars and compare their distributions.\n", "\n", "Specifically, define two group of stars, `intermed1`, with a `BP_RP` color in the range `[1,1.5]` and a magnitude in the range `[5,7.5]`, and `intermed2`, with a `BP_RP` color in the range `[1.5,2]` and a magnitude in the range `[6.5,8.5]`.\n", "\n", "Hint: It might be convenient to wrap some of the cells above into a function, which takes the masks, colors, and labels as inputs.\n", "\n", "\n", "What do you see?\n", "\n", "A) There appears to be no difference in the distributions.\n", "\n", "B) These distributions are now clearly different.\n", "\n", "\n", "<br>"]}, {"cell_type": "code", "execution_count": null, "id": "3ce153ce", "metadata": {"tags": ["learner_chopped", "py", "draft"]}, "outputs": [], "source": ["#>>>EXERCISE: L25.1.1\n", "\n", "np.seterr(invalid=\"ignore\")\n", "\n", "#DEFINE MASKS\n", "intermed1_mask = #YOUR CODE HERE\n", "intermed2_mask = #YOUR CODE HERE\n", "\n", "intermed1_color = 'tab:green'\n", "intermed2_color = 'tab:orange'\n", "intermed1_label = 'intermed1'\n", "intermed2_label = 'intermed2'\n", "\n", "\n", "#YOUR CODE HERE (APPLY MASKS, MAKE PLOTS)\n", "\n", "     "]}, {"cell_type": "markdown", "id": "0b10231d", "metadata": {"tags": ["md", "learner", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Exercise 25.1.2</span>\n", "\n", "What if we consider some of the other variables? Select objects with extreme radial velocity, and see how they look in the HR diagram.  Specifically, select events with radial velocity < 100 km/s and compare with radial velocity > 290 km/s. Where do these objects appear on the HR diagram? Select ALL that apply.\n", "\n", "A) The low velocity objects appear roughly evenly distributed in color and magnitude.\n", "    \n", "B) The low velocity objects appear to be bluer (younger) stars.\n", "\n", "C) The low velocity objects appear to be redder (older) stars.\n", "\n", "D) The high velocity objects appear roughly evenly distributed in color and magnitude.\n", "\n", "E) The high velocity objects appear to be bluer (younger) stars.\n", "\n", "F) The high velocity objects appear to be redder (older) stars.\n", "\n", "\n", "<br>"]}, {"cell_type": "code", "execution_count": null, "id": "4d2f6f6b", "metadata": {"tags": ["py", "draft", "learner_chopped"]}, "outputs": [], "source": ["#>>>EXERCISE: L25.1.2\n", "\n", "#DEFINE MASKS\n", "np.seterr(invalid=\"ignore\")\n", "\n", "lo_vel_mask = #YOUR CODE HERE\n", "hi_vel_mask = #YOUR CODE HERE\n", "\n", "hi_vel_color = 'tab:purple'\n", "lo_vel_color = 'tab:orange'\n", "hi_vel_label = 'high velocity'\n", "lo_vel_label = 'low velocity'\n", "\n", "#MAKE PLOT OF HR DIAGRAM\n", "#suggestion: use larger marker style or size to see clearly\n", "\n", "     "]}, {"cell_type": "markdown", "id": "b59e4c78", "metadata": {"tags": ["md", "learner", "learner_chopped"]}, "source": ["<a name='section_25_2'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\"> L25.2 Building Projections with Gaia </h2>  \n", "\n", "| [Top](#section_25_0) | [Previous Section](#section_25_1) | [Exercises](#exercises_25_2) | [Next Section](#section_25_3) |"]}, {"cell_type": "markdown", "id": "d61e1e2f", "metadata": {"tags": ["learner"]}, "source": ["*The material in this section is discussed in the video **<a href=\"https://courses.mitxonline.mit.edu/learn/course/course-v1:MITxT+8.S50.3x+3T2023/block-v1:MITxT+8.S50.3x+3T2023+type@sequential+block@seq_LS25/block-v1:MITxT+8.S50.3x+3T2023+type@vertical+block@vert_LS25_vid2\" target=\"_blank\">HERE</a>.** You are encouraged to watch that video and use this notebook concurrently.*"]}, {"cell_type": "markdown", "id": "c75720e7", "metadata": {"tags": ["lect_02", "learner"]}, "source": ["<h3>Overview</h3>\n", "\n", "Now, we would like to see if we can understand some differences using the Gaia data toolkit, mostly because the toolkit is so well developed, and we would like to show you the extent of what you can do easily. Let's take our same data that doesn't have may obvious differences and see if we can start to conjure a differnece in the data.\n", "\n", "What we are going to do is take our previous data set and evolve it for a long period of time using a stepping integrator. This integrator will not step all stars in the galaxy, but will instead use a constant radial mass profile (i.e., mass distribution as a function of radius), consistent with that observed for the Milky Way.\n", "\n", "Let's go ahead and define it. What we will end up plotting is a histogram of z-positions for all stars, before and after evolving in time 500 million years. Again, we are comparing high-mass stars (before and after evolution) with low-mass stars (before and after evolution).\n", "\n", "This will start to show us if some stars are going in or out of the plane of the galaxy!"]}, {"cell_type": "code", "execution_count": null, "id": "2f485591", "metadata": {"tags": ["lect_02", "learner", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L25.2-runcell01\n", "\n", "H = gp.Hamiltonian(milky_way)\n", "w0_hi = gd.PhaseSpacePosition(galcen[hi_mass_mask].cartesian)\n", "w0_lo = gd.PhaseSpacePosition(galcen[lo_mass_mask].cartesian)\n", "\n", "orbits_hi = H.integrate_orbit(w0_hi, dt=1*u.Myr, t1=0*u.Myr, t2=500*u.Myr)\n", "orbits_lo = H.integrate_orbit(w0_lo, dt=1*u.Myr, t1=0*u.Myr, t2=500*u.Myr)\n", "\n", "w0_hlo,bin_edges = np.histogram(w0_lo.z.value,bins=10,density=True)\n", "w0_hhi,bin_edges = np.histogram(w0_hi.z.value,bins=bin_edges,density=True)\n", "bin_center = 0.5*(bin_edges[:-1] + bin_edges[1:])\n", "\n", "print(orbits_lo[-1,0],w0_lo[0])\n", "o0_hlo,bin_edges = np.histogram(1e3*orbits_lo[-1,:].z.value,bins=bin_edges,density=True)\n", "o0_hhi,bin_edges = np.histogram(1e3*orbits_hi[-1,:].z.value,bins=bin_edges,density=True)\n", "\n", "plt.plot(bin_center,w0_hlo,alpha=0.5,marker='.',drawstyle=\"steps-mid\",label='z-start low mass')\n", "plt.plot(bin_center,w0_hhi,alpha=0.5,marker='.',drawstyle=\"steps-mid\",label='z-start high mass')\n", "\n", "plt.plot(bin_center,o0_hlo,alpha=0.5,marker='.',drawstyle=\"steps-mid\",label='z-end low mass')\n", "plt.plot(bin_center,o0_hhi,alpha=0.5,marker='.',drawstyle=\"steps-mid\",label='z-end high mass')\n", "plt.legend()\n", "plt.xlabel('z[pc]')\n", "plt.ylabel('N')\n", "\n", "plt.show()"]}, {"cell_type": "markdown", "id": "f25e0a36", "metadata": {"tags": ["lect_02", "learner"]}, "source": ["Now let's plot the orbit of some stars as function of time over the 500 Myr, to see how things look. We will just choose the one high-mass star and low-mass star to compare the two orbits.\n"]}, {"cell_type": "code", "execution_count": null, "id": "5c092b81", "metadata": {"tags": ["lect_02", "learner", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L25.2-runcell02\n", "\n", "fig = orbits_hi[0:500, 0].plot(color=hi_mass_color)\n", "_   = orbits_lo[0:500, 0].plot(axes=fig.axes, color=lo_mass_color)"]}, {"cell_type": "markdown", "id": "7835054a", "metadata": {"tags": ["lect_02", "learner"]}, "source": ["So, the stars tend to stay in the plane of the galaxy at $z\\approx 0$, while orbiting around the galactic center at $(x, y)=(0,0)$.\n", "\n", "Let's also plot the initial and final positions of the evolved stars.\n"]}, {"cell_type": "code", "execution_count": null, "id": "5625ab8c", "metadata": {"tags": ["lect_02", "learner", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L25.2-runcell03\n", "\n", "plt.plot(1e-3*galcen[1==1].x.value, 1e-3*galcen[1==1].y.value,marker='.', linestyle='none', alpha=0.2,color='blue')\n", "plt.plot(1e-3*w0_lo.x.value,1e-3*w0_lo.y.value,marker='.', linestyle='none', alpha=0.5,color='green',label='start')\n", "plt.plot(orbits_lo[-1].x.value,orbits_lo[-1].y.value,marker='.', linestyle='none', alpha=0.5,color=lo_mass_color,label='low mass')\n", "plt.plot(orbits_hi[-1].x.value,orbits_hi[-1].y.value,marker='.', linestyle='none', alpha=0.5,color=hi_mass_color,label='high mass')\n", "plt.xlabel('x[kpc]')\n", "plt.ylabel('y[kpc]')\n", "plt.legend()\n", "plt.show()\n", "\n", "plt.plot(1e-3*galcen[1==1].z.value, 1e-3*galcen[1==1].y.value,marker='.', linestyle='none', alpha=0.2,color='blue')#,label='origin')\n", "plt.plot(1e-3*w0_lo.z.value,1e-3*w0_lo.y.value,marker='.', linestyle='none', alpha=0.5,color='green',label='start')\n", "plt.plot(orbits_lo[-1].z.value,orbits_lo[-1].y.value,marker='.', linestyle='none', alpha=0.5,color=lo_mass_color,label='low mass')\n", "plt.plot(orbits_hi[-1].z.value,orbits_hi[-1].y.value,marker='.', linestyle='none', alpha=0.5,color=hi_mass_color,label='high mass')\n", "plt.xlabel('z[kpc]')\n", "plt.ylabel('y[kpc]')\n", "plt.legend()\n", "plt.show()"]}, {"cell_type": "markdown", "id": "88aec10e", "metadata": {"tags": ["lect_02", "learner"]}, "source": ["\n", "We now start to see that there are some interesting features in these results. Look in particular at the evolved $x$ and $y$ positions. This is a reflection of the velocities of the nearby stars based on their type. Also, note, the stars that we selected are not comoving, so we do not expect them to all follow the same orbits. Remember, we are just analyzing stars within a nearby distance to the sun.\n", "\n", "\n", "Another way to visualize the orbits is in cylindrical coordinates. The we can see the variation in z and radius $\\rho$ (instead of cartesian position). In the first plot, we again take the first star in each group (high-mass vs. low-mass) and plot its trajectory over the 500 Myr evolution. In the second plot, we show the final position of every star in the selected groups.\n"]}, {"cell_type": "code", "execution_count": null, "id": "61a67f66", "metadata": {"scrolled": false, "tags": ["lect_02", "learner", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L25.2-runcell04\n", "\n", "fig = orbits_hi[:, 0].cylindrical.plot(['rho', 'z'], \n", "                                       color=hi_mass_color,\n", "                                       label='high mass')\n", "_ = orbits_lo[:, 0].cylindrical.plot(['rho', 'z'], color=lo_mass_color,\n", "                                     axes=fig.axes,\n", "                                     label='low mass')\n", "\n", "fig.axes[0].legend(loc='upper left')\n", "fig.axes[0].set_ylim(-0.3, 0.3)\n", "plt.show()\n", "\n", "plt.plot(1e-3*galcen[1==1].cylindrical.rho, 1e-3*galcen[1==1].z.value,marker='.', linestyle='none', alpha=0.2,color='blue')\n", "plt.plot(1e-3*w0_lo.cylindrical.rho,        1e-3*w0_lo.z.value,marker='.', linestyle='none', alpha=0.5,color='green')\n", "plt.plot(orbits_lo[-1].cylindrical.rho,orbits_lo[-1].z.value,marker='.', linestyle='none', alpha=0.5,color=lo_mass_color)\n", "plt.plot(orbits_hi[-1].cylindrical.rho,orbits_hi[-1].z.value,marker='.', linestyle='none', alpha=0.5,color=hi_mass_color)\n", "plt.xlabel(r'$\\rho$ [kpc]')\n", "plt.ylabel('z [kpc]')\n", "plt.show()"]}, {"cell_type": "markdown", "id": "ffc519c9", "metadata": {"tags": ["lect_02", "learner"]}, "source": ["We can try to maximize the trends that we see above by computing more complex observables. You can see that the purple stars tend to have large z values and spread out in $\\rho$ more. We can further separate these out by looking at the projected $z_{\\rm max}$ and the orbital eccentricity (a measure of how far an orbit deviates from a circle, with $ecc=0$ indicating a perfect circle and $ecc=1$ indicating an ellipse that is so elongated that it forms a line)."]}, {"cell_type": "code", "execution_count": null, "id": "df4a17e2", "metadata": {"tags": ["lect_02", "learner", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L25.2-runcell05\n", "\n", "zmax_hi = orbits_hi.zmax(approximate=True)\n", "zmax_lo = orbits_lo.zmax(approximate=True)\n", "bins = np.linspace(0, 2, 50)\n", "\n", "plt.hist(zmax_hi.value, bins=bins, alpha=0.4, density=True, label='high-mass', color=hi_mass_color)\n", "plt.hist(zmax_lo.value, bins=bins, alpha=0.4, density=True, label='low-mass',color=lo_mass_color);\n", "plt.legend(loc='best', fontsize=14)\n", "print(\"Mean high: \", zmax_hi.value.mean(),\"Mean Low:\",zmax_lo.mean())\n", "\n", "plt.yscale('log')\n", "plt.xlabel(r\" zmax\" + \" [{0:latex}]\".format(zmax_hi.unit))\n", "plt.show()\n", "\n", "zmax_hi = orbits_hi.eccentricity()\n", "zmax_lo = orbits_lo.eccentricity()\n", "print(\"Ecc high: \", zmax_hi.value.mean(),\"Ecc Low:\",zmax_lo.mean())\n", "\n", "bins = np.linspace(0, 2, 50) #bins = np.linspace(0, 0.75, 50)\n", "plt.hist(zmax_hi.value, bins=bins, alpha=0.4, density=True, label='high-mass', color=hi_mass_color)\n", "plt.hist(zmax_lo.value, bins=bins, alpha=0.4, density=True, label='low-mass',color=lo_mass_color);\n", "plt.legend(loc='best', fontsize=14)\n", "plt.yscale('log')\n", "plt.xlabel('Eccentricity')\n", "plt.show()"]}, {"cell_type": "markdown", "id": "dc7df043", "metadata": {"tags": ["lect_02", "learner"]}, "source": ["<h3>Observations</h3>\n", "\n", "The critical question from this all is what have we observed? We've seen that the low mass stars are moving much more out of the galactic plane than the high mass stars, and similarly the low mass stars have a greater eccentricity.\n", "\n", "Another piece to this puzzle is the fact that the low mass stars are old (they've been burning for billions of years), and the high mass stars are young.\n", "\n", "\n", "In this case, we have observed two types of stars: Population I and Population II. You can read more about this <a href=\"https://en.wikipedia.org/wiki/Stellar_population\" target=\"_blank\">here</a>. "]}, {"cell_type": "markdown", "id": "572fafab", "metadata": {"tags": ["md", "learner", "learner_chopped"]}, "source": ["<a name='exercises_25_2'></a>     \n", "\n", "| [Top](#section_25_0) | [Restart Section](#section_25_2) | [Next Section](#section_25_3) |\n"]}, {"cell_type": "markdown", "id": "e6a79be7", "metadata": {"tags": ["md", "learner", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Exercise 25.2.1</span>\n", "\n", "Let's make a histogram of another characteristic, namely the orbital period. Plot the distribution of orbital periods for these populations of stars. Note you will need the `estimate_period(radial=True)` function from <a href=\"https://gala.adrian.pw/en/latest/api/gala.dynamics.Orbit.html#gala.dynamics.Orbit.estimate_period\" target=\"_blank\">here</a>. \n", "\n", "As a rough measurement of how regular the periods are for each group, calculate the standard deviation of the orbital periods for the high-mass vs. low-mass stars. Report your answer as a list of numbers with `[per_hi_stdev,per_lo_stdev]`, with precision 1 Myr.\n", "\n", "\n", "<br>"]}, {"cell_type": "code", "execution_count": null, "id": "fe88cb92", "metadata": {"tags": ["py", "draft", "learner_chopped"]}, "outputs": [], "source": ["#>>>EXERCISE: L25.2.1\n", "\n", "per_hi = #YOUR CODE HERE\n", "per_lo = #YOUR CODE HERE\n", "\n", "#CALCULATE THE STDEV OF EACH\n", "#YOUR CODE HERE\n", "\n", "#PLOT THE HISTOGRAM\n", "bins = np.linspace(100, 250, 50)\n", "plt.hist(per_hi.value, bins=bins, alpha=0.4, density=True, label='high-mass', color=hi_mass_color)\n", "plt.hist(per_lo.value, bins=bins, alpha=0.4, density=True, label='low-mass',color=lo_mass_color);\n", "plt.legend(loc='best', fontsize=14)\n", "plt.yscale('log')\n", "plt.xlabel('period')\n", "plt.show()"]}, {"cell_type": "markdown", "id": "f7040e0a", "metadata": {"tags": ["md", "learner", "learner_chopped"]}, "source": ["<a name='section_25_3'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\"> L25.3 Anomaly Detection with Gaia </h2>  \n", "\n", "| [Top](#section_25_0) | [Previous Section](#section_25_2) | [Exercises](#exercises_25_3) | [Next Section](#section_25_4) |"]}, {"cell_type": "markdown", "id": "b9029090", "metadata": {"tags": ["learner"]}, "source": ["*The material in this section is discussed in the video **<a href=\"https://courses.mitxonline.mit.edu/learn/course/course-v1:MITxT+8.S50.3x+3T2023/block-v1:MITxT+8.S50.3x+3T2023+type@sequential+block@seq_LS25/block-v1:MITxT+8.S50.3x+3T2023+type@vertical+block@vert_LS25_vid3\" target=\"_blank\">HERE</a>.** You are encouraged to watch that video and use this notebook concurrently.*"]}, {"cell_type": "markdown", "id": "69ce784e", "metadata": {"tags": ["lect_03", "learner"]}, "source": ["<h3>Overview</h3>\n", "\n", "Now let's do something more interesting with machine learning to see if we can detect more anomalies in the Gaia data! First, we will train an autoencoder on part of the dataset. Then, we'll apply this to another part of the dataset.  Our strategy will be to see if we can find some of the most anomalous stars in the dataset. We can then use the above observable exploration to see if the AI is telling us something meaningful.\n", "\n", "To do this, we will want to identify stars that are not exactly like the others. The idea is going to be to construct an autoencoder that can look for anomalous stars amongst the dataset. To do that we will make a torch dataset and we are going to use variables that don't make our location in the galaxy too special. What we will do is use:\n", "\n", " * galactic coordinates $\\vec{r} = r_{1}, r_{2}, r_{3}$\n", " * galactic velocity coordinates $\\vec{v_{r}} = \\dot{r}_{1}, \\dot{r}_{2}, \\dot{r}_{3}$\n", " * Distance corrected magnitudes\n", "\n", "Naturally, to make the data clean, we will remove the `nans` (where there are no values in the data) and split it up into testing and training data sets."]}, {"cell_type": "code", "execution_count": null, "id": "72c9793d", "metadata": {"tags": ["lect_03", "learner", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L25.3-runcell01\n", "\n", "def prepData(igaia_data,idist,igalcen,iSplit=0.5):\n", "    gaia_vars=['phot_g_mean_mag','phot_bp_mean_mag','phot_rp_mean_mag']\n", "    var0=(igaia_data[gaia_vars[0]]-idist.distmod).value\n", "    var1=(igaia_data[gaia_vars[1]]-idist.distmod).value\n", "    var2=(igaia_data[gaia_vars[2]]-idist.distmod).value\n", "    var3=igalcen.x.value\n", "    var4=igalcen.y.value\n", "    var5=igalcen.z.value\n", "    var6=igalcen.v_x.value\n", "    var7=igalcen.v_y.value\n", "    var8=igalcen.v_z.value\n", "\n", "    #processed_data = np.vstack((var0,var1,var2,var3,var4,var5,var6,var7,var8))\n", "    processed_data = np.vstack((var0,var1,var2,var3,var4,var5,var6,var7,var8))\n", "    processed_data = processed_data.T\n", "    processed_data = processed_data[~np.isnan(processed_data).any(axis=1)]\n", "    processed_data_raw = processed_data.copy()\n", "\n", "    #normalize the data\n", "    processed_data /= np.std(processed_data,axis=0)\n", "    processed_data -= np.mean(processed_data,axis=0)\n", "    processed_data = processed_data[~np.isnan(processed_data).any(axis=1)]\n", "    \n", "    #pytorch the layer\n", "    tprocessed_data = torch.tensor(processed_data).float()\n", "    processed_data_raw = processed_data_raw[~torch.any(tprocessed_data.isnan(),dim=1)]\n", "    galcen_clean       = igalcen[~torch.any(tprocessed_data.isnan(),dim=1)]\n", "    #split\n", "    maxindex       = int(len(processed_data)*iSplit)\n", "    trainset       = torch.tensor(processed_data[0:maxindex]).float()\n", "    trainset       = trainset[~torch.any(trainset.isnan(),dim=1)]\n", "    testset        = torch.tensor(processed_data[maxindex:len(processed_data)]).float()\n", "    testset        = testset[~torch.any(testset.isnan(),dim=1)]\n", "    print(processed_data_raw.shape,testset.shape,trainset.shape)\n", "    return testset,trainset,processed_data_raw,tprocessed_data,galcen_clean\n", "\n", "btestset,btrainset,bprocessed_data_raw,btprocessed_data,bgalcen_clean=prepData(gaia_data,dist,galcen)"]}, {"cell_type": "markdown", "id": "602a7211", "metadata": {"tags": ["lect_03", "learner"]}, "source": ["Now, we'll make a multilayer perceptron autoencoder with this dataset. This autoencoder will go to dimension 20 and then shrink down to a latent space of 2. The idea is that, in performing this compression, the autoencoder will learn the most relevant features of the data. Again, the features that we will give it are the three positions, three velocities, and three distance-corrected spectral magnitudes.\n"]}, {"cell_type": "code", "execution_count": null, "id": "e6c8a4c7", "metadata": {"tags": ["lect_03", "learner", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L25.3-runcell02\n", "\n", "class MLP(nn.Module):\n", "    def __init__(self,n_inputs,n_outputs):\n", "        super(MLP, self).__init__()\n", "        self.layers = nn.Sequential(\n", "            nn.Linear(n_inputs, 20),\n", "            nn.ReLU(),\n", "            nn.Linear(20, 6),\n", "            nn.ReLU(),\n", "            nn.Linear(6, 2),\n", "            nn.ReLU(),\n", "            nn.Linear(2, 6),\n", "            nn.ReLU(),\n", "            nn.Linear(6, 20),\n", "            nn.ReLU(),\n", "            nn.Linear(20, n_outputs),\n", "        )\n", "        \n", "    def forward(self, x):        \n", "        x = self.layers(x)\n", "        return x\n", "\n", "def train(x,y,net,loss_func,opt,sched,nepochs):\n", "    net.train(True)\n", "    for epoch in range(nepochs):\n", "        prediction = net(x)\n", "        opt.zero_grad()\n", "        loss = loss_func(prediction,y) \n", "        loss.backward() \n", "        opt.step()\n", "        if epoch % 500 == 0: \n", "            print('[%d] loss: %.4f ' % (epoch + 1, loss.item()  ))\n", "    #sched.step()\n", "    return    \n", "\n", "basicmodel     = MLP(btrainset.shape[1],btrainset.shape[1])\n", "optimizer = torch.optim.Adam(basicmodel.parameters(), lr=0.01)\n", "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.1, last_epoch=-1, verbose=False)\n", "loss_fn   =  nn.MSELoss(reduction='sum')"]}, {"cell_type": "markdown", "id": "e9c4a44d", "metadata": {"tags": ["lect_03", "learner"]}, "source": ["Let's use our dataset to train this."]}, {"cell_type": "code", "execution_count": null, "id": "03589744", "metadata": {"tags": ["lect_03", "learner", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L25.3-runcell03\n", "\n", "train(btrainset,btrainset,basicmodel,loss_fn,optimizer,scheduler,10001)"]}, {"cell_type": "markdown", "id": "bf375160", "metadata": {"tags": ["lect_03", "learner"]}, "source": ["Now that we have trained the model, let's look at its overall performance to understand what is going on. First, take a look at the loss. In the context of anomaly detection, the loss has a particular meaning. Any inputs with high loss represent anomalous events, since they don't match what the network has learned. These inputs (stars in our case) may be quite different from the majority of those in the dataset!"]}, {"cell_type": "code", "execution_count": null, "id": "a17ac6b8", "metadata": {"scrolled": false, "tags": ["lect_03", "learner", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L25.3-runcell04\n", "\n", "basicmodel.train(False)\n", "boutput=basicmodel(btestset)\n", "btestloss=torch.sum((btestset-boutput)**2,axis=1)\n", "plt.hist(btestloss[btestloss < 100].detach().numpy(),density=True)\n", "plt.yscale('log')\n", "plt.xlabel('loss')\n", "plt.ylabel('pdf')\n", "plt.show()\n", "\n", "varlabels=['Mag','B-Mag','R-Mag','x','y','z','vx','vy','vz']\n", "\n", "fig, ax = plt.subplots(3, 3, figsize=(20, 20))\n", "for var in range(btestset.shape[1]):\n", "    _,bins,_=ax[var//3,var % 3].hist(btestset[:,var].detach().numpy(),density=True,alpha=0.5,label='Input')\n", "    ax[var//3,var % 3].hist(boutput [:,var].detach().numpy(),density=True,alpha=0.5,bins=bins,label='Output')\n", "    ax[var//3,var % 3].set_xlabel(varlabels[var])\n", "    ax[var//3,var % 3].legend()"]}, {"cell_type": "markdown", "id": "983b91f7", "metadata": {"tags": ["lect_03", "learner"]}, "source": ["We can put a cut on the loss and look to see where the most anomalous regions are in the data. Specifically, we'll select all objects with a loss above 40 (but you can try changing the cut if you want!).\n", "\n", "The colorscale in the plots below represent the normalized loss, with higher loss values (i.e., more anomalous objects) represented by the orange end of the spectrum."]}, {"cell_type": "code", "execution_count": null, "id": "d60e7a03", "metadata": {"scrolled": false, "tags": ["lect_03", "learner", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L25.3-runcell05\n", "\n", "def plotAnomaly(iCut,iRaw,iLoss,igalcen,maxlosscolor=20):\n", "    loss = np.minimum(iLoss,maxlosscolor)\n", "    anomalies=(iLoss > iCut)\n", "    baseidex=len(iRaw)-len(iLoss)\n", "    btestdata_raw = iRaw[baseidex:]\n", "    anomaly_raw  = btestdata_raw[anomalies]\n", "    btestgalcen   = igalcen[baseidex:]\n", "    anomaly_galcen = btestgalcen[anomalies]\n", "    loss = loss/np.max(loss)\n", "    if maxlosscolor != 20:\n", "        loss=np.ones(loss.shape)\n", "    print(loss.shape,btestdata_raw[:,2].shape)\n", "    scat=plt.scatter(btestdata_raw[:,2]-btestdata_raw[:,1],btestdata_raw[:,0], marker='.',c=loss,cmap=\"viridis\")\n", "    plt.plot(anomaly_raw[:,2]-anomaly_raw[:,1],anomaly_raw[:,0], marker='.', linestyle='none',c='orange')\n", "    plt.xlabel('$G_{BP}-G_{RP}$')\n", "    plt.ylabel('$M_{G}$')\n", "    plt.ylim(15,0)\n", "    plt.xlim(1,-5)\n", "    plt.colorbar(scat)\n", "    plt.show()\n", "    \n", "    scat=plt.scatter(btestdata_raw[:,3],btestdata_raw[:,4],c=loss, marker='.',cmap=\"viridis\")\n", "    plt.plot(anomaly_raw[:,3],anomaly_raw[:,4], marker='.', linestyle='none',c='orange')\n", "    plt.xlabel(\"x[pc]\")\n", "    plt.ylabel(\"y[pc]\")\n", "    plt.colorbar(scat)\n", "    plt.show()\n", "\n", "    scat=plt.scatter(btestdata_raw[:,3],btestdata_raw[:,5],c=loss, marker='.', cmap=\"viridis\")\n", "    plt.plot(anomaly_raw[:,3],anomaly_raw[:,5], marker='.', linestyle='none',c='orange')\n", "    plt.xlabel(\"x[pc]\")\n", "    plt.ylabel(\"z[pc]\")\n", "    plt.colorbar(scat)\n", "    plt.show()\n", "\n", "    scat=plt.scatter(btestdata_raw[:,6],btestdata_raw[:,7],c=loss, marker='.', cmap=\"viridis\")\n", "    plt.plot(anomaly_raw[:,6],anomaly_raw[:,7], marker='.', linestyle='none',c='orange')\n", "    plt.xlabel(\"vx[pc]\")\n", "    plt.ylabel(\"vy[pc]\")\n", "    plt.colorbar(scat)\n", "    plt.show()\n", "\n", "    scat=plt.scatter(btestdata_raw[:,6],btestdata_raw[:,8],c=loss, marker='.',cmap=\"viridis\")\n", "    plt.plot(anomaly_raw[:,6],anomaly_raw[:,8], marker='.', linestyle='none')\n", "    plt.xlabel(\"vx[pc]\")\n", "    plt.ylabel(\"vz[pc]\")\n", "    plt.show()\n", "\n", "    print(len(btestdata_raw),len(iRaw),len(igalcen),len(btestgalcen))\n", "    H = gp.Hamiltonian(milky_way)\n", "    w0_anom = gd.PhaseSpacePosition(anomaly_galcen.cartesian)\n", "    orbits_anom = H.integrate_orbit(w0_anom, dt=1*u.Myr, t1=0*u.Myr, t2=100*u.Myr)\n", "    w0_all  = gd.PhaseSpacePosition(bgalcen_clean[1==1].cartesian)\n", "    orbits_all  = H.integrate_orbit(w0_all,  dt=1*u.Myr, t1=0*u.Myr, t2=100*u.Myr)\n", "\n", "    zmax_all = orbits_all.zmax(approximate=True)\n", "    zmax_anom = orbits_anom.zmax(approximate=True)\n", "    print(\"ZMax mean:\",zmax_anom[~np.isnan(zmax_anom.value)].mean(),len(zmax_anom),\"default:\",zmax_all.mean())\n", "    bins = np.linspace(0, 10, 50)\n", "    plt.hist(zmax_all.value, bins=bins, alpha=0.4, density=True, label='all')\n", "    plt.hist(zmax_anom.value, bins=bins, alpha=0.4, density=True, label='anom')\n", "    plt.legend(loc='best', fontsize=14)\n", "    plt.yscale('log')\n", "    plt.xlabel(r\" zmax\" + \" [{0:latex}]\".format(zmax_all.unit))\n", "    plt.show()\n", "\n", "    zmax_all  = orbits_all.eccentricity()\n", "    zmax_anom = orbits_anom.eccentricity()\n", "    print(\"Ecc mean:\",zmax_anom[~np.isnan(zmax_anom.value)].mean(),len(zmax_anom),\"default:\",zmax_all.mean())\n", "    bins = np.linspace(0, 3, 50)\n", "    plt.hist(zmax_all.value,  bins=bins, alpha=0.4, density=True, label='all')\n", "    plt.hist(zmax_anom.value, bins=bins, alpha=0.4, density=True, label='anom')\n", "    plt.legend(loc='best', fontsize=14)\n", "    plt.yscale('log')\n", "    plt.xlabel('Eccentricity')\n", "    plt.show()\n", "\n", "\n", "plotAnomaly(10,bprocessed_data_raw,btestloss.detach().numpy(),bgalcen_clean)"]}, {"cell_type": "markdown", "id": "c6ff90ee", "metadata": {"tags": ["lect_03", "learner"]}, "source": ["I think its clear that we are really finding some of the weirdest stars, which are very far from the galactic plane!"]}, {"cell_type": "markdown", "id": "bed54b47", "metadata": {"tags": ["md", "learner", "learner_chopped"]}, "source": ["<a name='exercises_25_3'></a>     \n", "\n", "| [Top](#section_25_0) | [Restart Section](#section_25_3) | [Next Section](#section_25_4) |\n"]}, {"cell_type": "markdown", "id": "16860f69", "metadata": {"tags": ["md", "learner", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Exercise 25.3.1</span>\n", "\n", "We saw that anomalous stars tended to have large $|v_{x}|$ values. Let's try to cut on the events with $|v_{x}| > 100$ and thereby effectively reverse engineer the anomaly detection. \n", "\n", "In the function `plotAnomaly(iCut,iRaw,iLoss,igalcen,maxlosscolor=20)`, the anomalous objects have `iLoss` values that are above the threshold defined by `iCut`. So to accomplish our goal, let's define an array of `iLoss` values to be the velocities  $|v_{x}|$, then use the appropriate `iCut`.\n", "\n", "Consider the options below for defining this cut on the data, and choose the best one. Hint, look at the definitions within the `prepData` function at the beginning of this section.\n", "\n", "A) `cutvals=np.abs(bprocessed_data_raw[:,0][2032:]).copy()`\\\n", "B) `cutvals=np.abs(bprocessed_data_raw[:,1][2032:]).copy()`\\\n", "C) `cutvals=np.abs(bprocessed_data_raw[:,2][2032:]).copy()`\\\n", "D) `cutvals=np.abs(bprocessed_data_raw[:,3][2032:]).copy()`\\\n", "E) `cutvals=np.abs(bprocessed_data_raw[:,4][2032:]).copy()`\\\n", "F) `cutvals=np.abs(bprocessed_data_raw[:,5][2032:]).copy()`\\\n", "G) `cutvals=np.abs(bprocessed_data_raw[:,6][2032:]).copy()`\\\n", "H) `cutvals=np.abs(bprocessed_data_raw[:,7][2032:]).copy()`\\\n", "I) `cutvals=np.abs(bprocessed_data_raw[:,8][2032:]).copy()`\n", "\n", "\n", "Upon completing your selection above, try to run the code `plotAnomaly(100,bprocessed_data_raw,cutvals,bgalcen_clean,maxlosscolor=150)` and compare the HR diagram to what we saw before. Do you think this does a good job?\n", "\n", "<br>"]}, {"cell_type": "code", "execution_count": null, "id": "b6c127f9", "metadata": {"tags": ["py", "draft", "learner_chopped"]}, "outputs": [], "source": ["#>>>EXERCISE: L25.3.1\n", "\n", "cutvals=#YOUR CODE HERE\n", "plotAnomaly(100,bprocessed_data_raw,cutvals,bgalcen_clean,maxlosscolor=150)"]}, {"cell_type": "markdown", "id": "9aabde57", "metadata": {"tags": ["md", "learner", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Exercise 25.3.2</span>\n", "\n", "Search for anomalies without using the red, blue, and general filters. Hint, to do this you could either redfine the `prepData` function, or perform the relavant selections on the existing data (e.g., `btestset[:,3:9]`). How do things compare? Select ALL that apply.\n", "\n", "A) The selected anomalies are identical.\n", "\n", "B) The selected anomalies are totally different.\n", "\n", "C) Some anomalies are the same, but the selection is different from before.\n", "\n", "D) The loss function is nearly identical.\n", "\n", "E) The loss function has a narrower tail, so fewer anomalies are selected.\n", "\n", "F) The loss function has a wider tail, so more anomalies are selected.\n", "\n", "<br>"]}, {"cell_type": "markdown", "id": "863cec5b", "metadata": {"tags": ["md", "learner", "learner_chopped"]}, "source": ["<a name='section_25_4'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\"> L25.4 Anomaly Detection with Lots of Gaia Data</h2>  \n", "\n", "| [Top](#section_25_0) | [Previous Section](#section_25_3) | [Exercises](#exercises_25_4) |"]}, {"cell_type": "markdown", "id": "ac7eb143", "metadata": {"tags": ["learner"]}, "source": ["*The material in this section is discussed in the video **<a href=\"https://courses.mitxonline.mit.edu/learn/course/course-v1:MITxT+8.S50.3x+3T2023/block-v1:MITxT+8.S50.3x+3T2023+type@sequential+block@seq_LS25/block-v1:MITxT+8.S50.3x+3T2023+type@vertical+block@vert_LS25_vid4\" target=\"_blank\">HERE</a>.** You are encouraged to watch that video and use this notebook concurrently.*"]}, {"cell_type": "markdown", "id": "4c0cf721", "metadata": {"tags": ["lect_04", "learner"]}, "source": ["<h3>Overview</h3>\n", "\n", "Recall that what we've done so far only used a sampling of 2k nearest stars. What if we do this on a much larger scale, namely 170k stars which generate a dataset file of size approximately 9MB.\n"]}, {"cell_type": "code", "execution_count": null, "id": "a2a1092a", "metadata": {"tags": ["lect_04", "learner", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L25.4-runcell01\n", "\n", "#note, our query looks for 1e6 stars, but only finds about 170k\n", "query_text = '''SELECT TOP 1000000 ra, dec, parallax, pmra, pmdec, radial_velocity,\n", "phot_g_mean_mag, phot_bp_mean_mag, phot_rp_mean_mag\n", "FROM gaiadr3.gaia_source\n", "WHERE parallax_over_error > 10 AND\n", "    parallax > 10 AND\n", "    radial_velocity IS NOT null\n", "ORDER BY random_index\n", "'''\n", "\n", "job = Gaia.launch_job(query_text)\n", "gaia_data = job.get_results()\n", "gaia_data.write('data/L25/gaia_data2.fits',overwrite=True)\n", "\n", "#Note, if you return to this section after closing the kernel,\n", "#you can load the data again using the following code\n", "gaia_data = QTable.read('data/L25/gaia_data2.fits')\n", "print(\"Total Events:\",len(gaia_data))\n"]}, {"cell_type": "markdown", "id": "1dfc7c14", "metadata": {"tags": ["lect_04", "learner"]}, "source": ["How do these stars look on an H-R diagram?"]}, {"cell_type": "code", "execution_count": null, "id": "a2d78a09", "metadata": {"tags": ["lect_04", "learner", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L25.4-runcell02\n", "\n", "dist = coord.Distance(parallax=u.Quantity(gaia_data['parallax']))\n", "M_G = gaia_data['phot_g_mean_mag'] - dist.distmod\n", "BP_RP = gaia_data['phot_bp_mean_mag'] - gaia_data['phot_rp_mean_mag']\n", "\n", "plt.plot(BP_RP.value, M_G.value, marker='.', linestyle='none', alpha=0.3)\n", "\n", "plt.xlim(-1, 5)\n", "plt.ylim(17, -2)\n", "\n", "plt.xlabel('$G_{BP}-G_{RP}$')\n", "plt.ylabel('$M_{G}$')\n", "plt.show()"]}, {"cell_type": "markdown", "id": "287a5198", "metadata": {"tags": ["lect_04", "learner"]}, "source": ["With this larger sample, the red giants at the top are much more prominent and you start to more clearly see some points down at the bottom that correspond to the white dwarfs. As before, we'll split this dataset in two and train an autoencoder, **this time using 4 latent dimensions.**\n", "\n", "As before, we are performing a compression that forces the autoencoder to learn the most relevant features of the data. Again, the features that we will give it are the three positions, three velocities, and three distance-corrected spectral magnitudes.\n", "\n", "Let's define everything."]}, {"cell_type": "code", "execution_count": null, "id": "1726da7c", "metadata": {"tags": ["lect_04", "learner", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L25.4-runcell03\n", "\n", "#redefine as above\n", "def prepData(igaia_data,idist,igalcen,iSplit=0.5):\n", "    gaia_vars=['phot_g_mean_mag','phot_bp_mean_mag','phot_rp_mean_mag']\n", "    var0=(igaia_data[gaia_vars[0]]-idist.distmod).value\n", "    var1=(igaia_data[gaia_vars[1]]-idist.distmod).value\n", "    var2=(igaia_data[gaia_vars[2]]-idist.distmod).value\n", "    var3=igalcen.x.value\n", "    var4=igalcen.y.value\n", "    var5=igalcen.z.value\n", "    var6=igalcen.v_x.value\n", "    var7=igalcen.v_y.value\n", "    var8=igalcen.v_z.value\n", "\n", "    #processed_data = np.vstack((var0,var1,var2,var3,var4,var5,var6,var7,var8))\n", "    processed_data = np.vstack((var0,var1,var2,var3,var4,var5,var6,var7,var8))\n", "    processed_data = processed_data.T\n", "    processed_data = processed_data[~np.isnan(processed_data).any(axis=1)]\n", "    processed_data_raw = processed_data.copy()\n", "\n", "    #normalize the data\n", "    processed_data /= np.std(processed_data,axis=0)\n", "    processed_data -= np.mean(processed_data,axis=0)\n", "    processed_data = processed_data[~np.isnan(processed_data).any(axis=1)]\n", "    \n", "    #pytorch the layer\n", "    tprocessed_data = torch.tensor(processed_data).float()\n", "    processed_data_raw = processed_data_raw[~torch.any(tprocessed_data.isnan(),dim=1)]\n", "    galcen_clean       = igalcen[~torch.any(tprocessed_data.isnan(),dim=1)]\n", "    #split\n", "    maxindex       = int(len(processed_data)*iSplit)\n", "    trainset       = torch.tensor(processed_data[0:maxindex]).float()\n", "    trainset       = trainset[~torch.any(trainset.isnan(),dim=1)]\n", "    testset        = torch.tensor(processed_data[maxindex:len(processed_data)]).float()\n", "    testset        = testset[~torch.any(testset.isnan(),dim=1)]\n", "    print(processed_data_raw.shape,testset.shape,trainset.shape)\n", "    return testset,trainset,processed_data_raw,tprocessed_data,galcen_clean\n", "\n", "c = coord.SkyCoord(ra=gaia_data['ra'], dec=gaia_data['dec'],distance=dist,pm_ra_cosdec=gaia_data['pmra'], pm_dec=gaia_data['pmdec'],\n", "                   radial_velocity=gaia_data['radial_velocity'])\n", "galcen = c.transform_to(coord.Galactocentric(z_sun=0*u.pc, galcen_distance=8.1*u.kpc))\n", "\n", "testset,trainset,processed_data_raw,tprocessed_data,galcen_clean=prepData(gaia_data,dist,galcen)"]}, {"cell_type": "code", "execution_count": null, "id": "ea4a2977", "metadata": {"tags": ["lect_04", "learner", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L25.4-runcell04\n", "\n", "class MLP(nn.Module):\n", "    def __init__(self,n_inputs,n_outputs):\n", "        super(MLP, self).__init__()\n", "        self.layers = nn.Sequential(\n", "            nn.Linear(n_inputs, 100),\n", "            nn.ReLU(),\n", "            nn.Linear(100, 20),\n", "            nn.ReLU(),\n", "            nn.Linear(20, 4),\n", "            nn.ReLU(),\n", "            nn.Linear(4, 20),\n", "            nn.ReLU(),\n", "            nn.Linear(20, 100),\n", "            nn.ReLU(),\n", "            nn.Linear(100, n_outputs),\n", "        )\n", "        \n", "    def forward(self, x):        \n", "        x = self.layers(x)\n", "        return x\n", "\n", "def train(x,y,net,loss_func,opt,sched,nepochs):\n", "    net.train(True)\n", "    for epoch in range(nepochs):\n", "        prediction = net(x)\n", "        opt.zero_grad()\n", "        loss = loss_func(prediction,y) \n", "        loss.backward() \n", "        opt.step()\n", "        if epoch % 500 == 0: \n", "            print('[%d] loss: %.4f ' % (epoch + 1, loss.item()  ))\n", "    #sched.step()\n", "    return    \n", "\n", "model     = MLP(trainset.shape[1],trainset.shape[1])\n", "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n", "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.1, last_epoch=-1, verbose=False)\n", "loss_fn   =  nn.MSELoss(reduction='sum')"]}, {"cell_type": "markdown", "id": "e7002612", "metadata": {"tags": ["lect_04", "learner"]}, "source": ["Then, we train on half of the data. This will take a significant amount of time (perhaps more than 30 min, but probably not more than an hour - timed in Colab)."]}, {"cell_type": "code", "execution_count": null, "id": "88cd12ea", "metadata": {"tags": ["lect_04", "learner", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L25.4-runcell05\n", "\n", "train(trainset,trainset,model,loss_fn,optimizer,scheduler,10001)\n", "#train(trainset,trainset,model,loss_fn,optimizer,scheduler,5001) #train for shorter amount of epochs"]}, {"cell_type": "code", "execution_count": null, "id": "e58e17c2", "metadata": {"tags": ["lect_04", "learner", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L25.4-runcell06\n", "\n", "model.train(False)\n", "output=model(testset)\n", "testloss=torch.sum((testset-output)**2,axis=1)\n", "plt.hist(testloss[testloss < 100].detach().numpy(),density=True)\n", "plt.yscale('log')\n", "plt.xlabel('loss')\n", "plt.ylabel('pdf')\n", "plt.show()\n", "\n", "varlabels=['Mag','B-Mag','R-Mag','x','y','z','vx','vy','vz']\n", "\n", "fig, ax = plt.subplots(3, 3, figsize=(20, 20))\n", "for var in range(testset.shape[1]):\n", "    _,bins,_=ax[var//3,var % 3].hist(testset[:,var].detach().numpy(),density=True,alpha=0.5,label='Input')\n", "    ax[var//3,var % 3].hist(output [:,var].detach().numpy(),density=True,alpha=0.5,bins=bins,label='Output')\n", "    ax[var//3,var % 3].set_xlabel(varlabels[var])\n", "    ax[var//3,var % 3].legend()"]}, {"cell_type": "markdown", "id": "b7fd37fe", "metadata": {"tags": ["lect_04", "learner"]}, "source": ["As before, we'll look for anomalies in this scenario, and study the dynamics of these objects, first checking if anything appears strange by-eye."]}, {"cell_type": "code", "execution_count": null, "id": "907e3f37", "metadata": {"scrolled": false, "tags": ["lect_04", "learner", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L25.4-runcell07\n", "\n", "milky_way = gp.MilkyWayPotential()\n", "H = gp.Hamiltonian(milky_way)\n", "w0_all  = gd.PhaseSpacePosition(galcen_clean[1==1].cartesian)\n", "orbits_all  = H.integrate_orbit(w0_all,  dt=1*u.Myr, t1=0*u.Myr, t2=100*u.Myr)\n", "#plotAnomaly(45,processed_data_raw,testloss,galcen_clean,orbits_all)"]}, {"cell_type": "code", "execution_count": null, "id": "82de276f", "metadata": {"tags": ["lect_04", "learner", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L25.4-runcell08\n", "\n", "anomalies=(testloss > 45.)\n", "baseidex=len(processed_data_raw)-len(testloss)\n", "testgalcen   = galcen_clean[baseidex:]\n", "anomaly_galcen = testgalcen[anomalies]\n", "\n", "w0_anom = gd.PhaseSpacePosition(anomaly_galcen.cartesian)\n", "orbits_anom = H.integrate_orbit(w0_anom, dt=1*u.Myr, t1=0*u.Myr, t2=100*u.Myr)\n", "\n", "#redefine, as above\n", "hi_mass_color = 'tab:purple'\n", "lo_mass_color = 'tab:red'\n", "hi_mass_label = 'high mass'\n", "lo_mass_label = 'low mass'\n", "\n", "fig = orbits_all[0:200, 0].plot(color=hi_mass_color,label='normal')\n", "_   = orbits_all[0:200, 1].plot(axes=fig.axes,color=hi_mass_color)\n", "_   = orbits_all[0:200, 2].plot(axes=fig.axes,color=hi_mass_color)\n", "_   = orbits_anom[0:200, 0].plot(axes=fig.axes,label='anom 1')\n", "_   = orbits_anom[0:200, 1].plot(axes=fig.axes,label='anom 2')\n", "_   = orbits_anom[0:200, 2].plot(axes=fig.axes,label='anom 3')\n", "_   = orbits_anom[0:200, 3].plot(axes=fig.axes,label='anom 4',color=lo_mass_color)\n", "plt.legend()\n", "\n", "\n", "zmax=orbits_anom.zmax(approximate=True)\n", "print(zmax)"]}, {"cell_type": "markdown", "id": "c880f595", "metadata": {"tags": ["lect_04", "learner"]}, "source": ["Wow! Some of these objects appear to have trajectories that make them shoot out of the galactic plane. That's certainly not how most stars behave!\n", "\n", "Now, let's again cut on the loss and look at the characteristics of the anomalous population."]}, {"cell_type": "code", "execution_count": null, "id": "21146213", "metadata": {"scrolled": false, "tags": ["lect_04", "learner", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L25.4-runcell09\n", "\n", "def plotAnomalyBasic(iCut,iRaw,iLoss,igalcen,iOrbitsAll):\n", "    loss = np.minimum(iLoss.detach().numpy(),20)\n", "    anomalies=(iLoss > iCut)\n", "    baseidex=len(iRaw)-len(iLoss)\n", "    testdata_raw = iRaw[baseidex:]\n", "    anomaly_raw  = testdata_raw[anomalies]\n", "    testgalcen   = igalcen[baseidex:]\n", "    anomaly_galcen = testgalcen[anomalies]\n", "\n", "    scat=plt.scatter(-1.*(testdata_raw[:,2]-testdata_raw[:,1]),-1.*testdata_raw[:,0],c=loss)\n", "    plt.plot(-1.*(anomaly_raw[:,2]-anomaly_raw[:,1]),-1.*anomaly_raw[:,0], marker='.',c='orange', linestyle='none')\n", "    plt.xlabel('$G_{BP}-G_{RP}$')\n", "    plt.ylabel('$M_{G}$')\n", "    plt.colorbar(scat)\n", "    plt.show()\n", "\n", "    scat=plt.scatter(testdata_raw[:,3],testdata_raw[:,4], marker='.', c=loss)\n", "    plt.plot(anomaly_raw[:,3],anomaly_raw[:,4], marker='.',c='orange', linestyle='none')\n", "    plt.xlabel(\"x[pc]\")\n", "    plt.ylabel(\"y[pc]\")\n", "    plt.colorbar(scat)\n", "    plt.show()\n", "\n", "    scat=plt.scatter(testdata_raw[:,3],testdata_raw[:,5], marker='.', c=loss)\n", "    plt.plot(anomaly_raw[:,3],anomaly_raw[:,5], marker='.',c='orange', linestyle='none')\n", "    plt.xlabel(\"x[pc]\")\n", "    plt.ylabel(\"z[pc]\")\n", "    plt.colorbar(scat)\n", "    plt.show()\n", "\n", "    scat=plt.scatter(testdata_raw[:,6],testdata_raw[:,7], marker='.', c=loss)\n", "    plt.plot(anomaly_raw[:,6],anomaly_raw[:,7], marker='.',c='orange', linestyle='none')\n", "    plt.xlabel(\"vx[pc]\")\n", "    plt.ylabel(\"vy[pc]\")\n", "    plt.colorbar(scat)\n", "    plt.show()\n", "\n", "    scat=plt.scatter(testdata_raw[:,6],testdata_raw[:,8], marker='.', c=loss)\n", "    plt.plot(anomaly_raw[:,6],anomaly_raw[:,8], marker='.',c='orange', linestyle='none')\n", "    plt.xlabel(\"vx[pc]\")\n", "    plt.ylabel(\"vz[pc]\")\n", "    plt.colorbar(scat)\n", "    plt.show()\n", "\n", "def plotAnomalyComplex(iCut,iRaw,iLoss,igalcen,iOrbitsAll,iEccAll,iZAll,iT2):\n", "    anomalies=(iLoss > iCut)\n", "    baseidex=len(iRaw)-len(iLoss)\n", "    testdata_raw = iRaw[baseidex:]\n", "    anomaly_raw  = testdata_raw[anomalies]\n", "    testgalcen   = igalcen[baseidex:]\n", "    anomaly_galcen = testgalcen[anomalies]\n", "\n", "    print(len(testdata_raw),len(iRaw),len(igalcen),len(testgalcen))\n", "    H = gp.Hamiltonian(milky_way)\n", "    w0_anom = gd.PhaseSpacePosition(anomaly_galcen.cartesian)\n", "    orbits_anom = H.integrate_orbit(w0_anom, dt=1*u.Myr, t1=0*u.Myr, t2=iT2)\n", "\n", "    #zmax_all = iOrbitsAll.zmax(approximate=True)\n", "    zmax_anom = orbits_anom.zmax(approximate=True)\n", "    print(\"ZMax mean:\",zmax_anom[~np.isnan(zmax_anom.value)].mean(),len(zmax_anom),\"default:\",zmax_all.mean())\n", "    #print(zmax_anom,\"!! Anom 1\")\n", "    bins = np.linspace(0, 10, 50)\n", "    plt.hist(iZAll.value, bins=bins, alpha=0.4, density=True, label='all')\n", "    plt.hist(zmax_anom.value, bins=bins, alpha=0.4, density=True, label='anom')\n", "    plt.legend(loc='best', fontsize=14)\n", "    plt.yscale('log')\n", "    plt.xlabel(r\" zmax\" + \" [{0:latex}]\".format(zmax_all.unit))\n", "    plt.show()\n", "\n", "    #zmax_all  = orbits_all.eccentricity()\n", "    zmax_anom = orbits_anom.eccentricity()\n", "    print(\"Ecc mean:\",zmax_anom[~np.isnan(zmax_anom.value)].mean(),len(zmax_anom),\"default:\",zmax_all.mean())\n", "    bins = np.linspace(0, 10, 50)\n", "    plt.hist(iEccAll.value,  bins=bins, alpha=0.4, density=True, label='all')\n", "    plt.hist(zmax_anom.value, bins=bins, alpha=0.4, density=True, label='anom')\n", "    plt.legend(loc='best', fontsize=14)\n", "    plt.yscale('log')\n", "    plt.xlabel('Eccentricity')\n", "    plt.show()\n", "\n", "plotAnomalyBasic(45,processed_data_raw,testloss,galcen_clean,orbits_all)"]}, {"cell_type": "code", "execution_count": null, "id": "6fdeba0f", "metadata": {"tags": ["lect_04", "learner", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L25.4-runcell10\n", "\n", "#Note, this will take some time to run\n", "zmax_all = orbits_all.zmax(approximate=True)\n", "ecc_all  = orbits_all.eccentricity()\n", "plotAnomalyComplex(20,processed_data_raw,testloss,galcen_clean,orbits_all,ecc_all,zmax_all,iT2=100*u.Myr)"]}, {"cell_type": "markdown", "id": "0448149a", "metadata": {"tags": ["lect_04", "learner"]}, "source": ["These results make it clear that we are selecting stars with very weird orbital paths. We could go into even more detail and investigate these some more.\n", "\n", "However, let's train a different type of anomaly detection. We can do a VAE and see if anything else is different."]}, {"cell_type": "markdown", "id": "3be3073d", "metadata": {"tags": ["md", "learner", "learner_chopped"]}, "source": ["<a name='exercises_25_4'></a>     \n", "\n", "| [Top](#section_25_0) | [Restart Section](#section_25_4) |\n"]}, {"cell_type": "markdown", "id": "d622dc4a", "metadata": {"tags": ["md", "learner", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Exercise 25.4.1</span>\n", "\n", "We performed the analysis above using a multilayer perceptron (MLP). Consider what similarities or differences might arise if we instead used a variational autoencoder (VAE). Select ALL statements that accurately describe these machine learning methods.\n", "\n", "A) A VAE would provide a probabilistic representation of the data, while an MLP would provide a deterministic mapping from input to output.\n", "\n", "B) Both VAE and MLP can be used for anomaly detection, but a VAE can generate new samples similar to the training data, which an MLP cannot do.\n", "\n", "C) A VAE would learn a continuous latent space, while an MLP would not.\n", "\n", "\n", "**Afterwards, perform the same analysis as above with a VAE, using 4 latent dimensions.** What do you find? Note, you can check the solution to this problem to see the code that we used.\n", "\n", "<br>"]}], "metadata": {"celltoolbar": "Tags", "kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}