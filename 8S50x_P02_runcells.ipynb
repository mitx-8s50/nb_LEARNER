{"cells": [{"cell_type": "markdown", "id": "168bf389", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<hr style=\"height: 1px;\">\n", "<i>This notebook was authored by the 8.S50x Course Team, Copyright 2022 MIT All Rights Reserved.</i>\n", "<hr style=\"height: 1px;\">\n", "<br>\n", "\n", "<h1>Guided Problem Set 2: Error Propagation</h1>\n"]}, {"cell_type": "markdown", "id": "6d7e33ca", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_2_0'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">P2.0 Overview</h2>\n"]}, {"cell_type": "markdown", "id": "3d97b41e", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<h3>Navigation</h3>\n", "\n", "<table style=\"width:100%\">\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_2_1\">P2.1 Error Propagation - A Simple Example</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#problems_2_1\">P2.1 Problems</a></td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_2_2\">P2.2 Error Propagation - A More Complicated Example</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#problems_2_2\">P2.2 Problems</a></td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_2_3\">P2.3 Johnson Noise</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#problems_2_3\">P2.3 Problems</a></td>\n", "    </tr>\n", "</table>"]}, {"cell_type": "markdown", "id": "2f808567", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<h3>New Library</h3>\n", "\n", "**Using Jupyter Notebook Locally**\n", "\n", "Starting in this section of the course, you will need access to the `lmfit` library. If you are running Juypter locally, you don't need to do anything, as this library was installed during the initial setup of your 8.S50x environment. So, you can ignore the first code cell below and jump right to the import code.\n", "\n", "If you didn't perform this installation (or others), then activate your 8.S50x conda environment and execute the following installations:\n", "\n", "<pre>\n", "conda install lmfit\n", "</pre>\n", "\n", "\n", "**Using Colab**\n", "\n", "However, if you are running this notebook in a Colab environment, the procedure is slightly different. Unlike the libraries that were used previously, `lmfit` is not included in the default Colab environment. To do this installation, you must run the `!pip install lmfit` command in the code cell below."]}, {"cell_type": "code", "execution_count": null, "id": "2c01cf77", "metadata": {"tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: P2.0-runcell00\n", "\n", "!pip3 install lmfit "]}, {"cell_type": "code", "execution_count": null, "id": "ea182157", "metadata": {"tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: P2.0-runcell01\n", "\n", "import numpy as np                 #https://numpy.org/doc/stable/\n", "import matplotlib.pyplot as plt    #https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.html\n", "import scipy.stats                 #https://docs.scipy.org/doc/scipy/reference/stats.html\n", "from scipy.integrate import trapz  #https://docs.scipy.org/doc/scipy-0.18.1/reference/generated/scipy.integrate.trapz.html\n", "from lmfit.models import Model     #https://lmfit.github.io/lmfit-py/model.html"]}, {"cell_type": "code", "execution_count": null, "id": "783e9667", "metadata": {"tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: P2.0-runcell02\n", "\n", "#set plot resolution\n", "%config InlineBackend.figure_format = 'retina'\n", "\n", "#set default figure parameters\n", "plt.rcParams['figure.figsize'] = (9,6)\n", "\n", "medium_size = 12\n", "large_size = 15\n", "\n", "plt.rc('font', size=medium_size)          # default text sizes\n", "plt.rc('xtick', labelsize=medium_size)    # xtick labels\n", "plt.rc('ytick', labelsize=medium_size)    # ytick labels\n", "plt.rc('legend', fontsize=medium_size)    # legend\n", "plt.rc('axes', titlesize=large_size)      # axes title\n", "plt.rc('axes', labelsize=large_size)      # x and y labels\n", "plt.rc('figure', titlesize=large_size)    # figure title"]}, {"cell_type": "markdown", "id": "118b0074", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_2_1'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">P2.1 Error Propagation - A Simple Example</h2>    \n", "\n", "| [Top](#section_2_0) | [Previous Section](#section_2_0) | [Problems](#problems_2_1) | [Next Section](#section_2_2) |\n"]}, {"cell_type": "code", "execution_count": null, "id": "c868f597", "metadata": {"tags": ["learner", "py", "catsoop_01", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: P2.1-runcell01\n", "\n", "def f(x, y):\n", "    return x + y\n", "\n", "def delta_f(delta_x, delta_y):\n", "    return np.sqrt((delta_x**2.)+(delta_y**2.))\n", "\n", "x_val = 5.\n", "x_err = 2.\n", "\n", "y_val = 9.\n", "y_err = 2.\n", "\n", "print(\"f(x) = %f +/- %f\" % (f(x_val, y_val), delta_f(x_err, y_err)))\n"]}, {"cell_type": "code", "execution_count": null, "id": "ee9f6444", "metadata": {"tags": ["learner", "py", "catsoop_01", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: P2.1-runcell02\n", "\n", "np.random.seed(2)\n", "N_SAMPLES = 10000\n", "N_BINS = 100\n", "x_samples = np.random.normal(loc = x_val, scale = x_err, size = N_SAMPLES)\n", "y_samples = np.random.normal(loc = y_val, scale = y_err, size = N_SAMPLES)\n", "\n", "f_samples = f(x_samples, y_samples)\n", "\n", "print(\"observed: f(x) = %f +/- %f\" % (np.mean(f_samples), np.std(f_samples)))\n", "print(\"expected: f(x) = %f +/- %f\" % (f(x_val, y_val), delta_f(x_err, y_err)))\n", "\n", "#MAKING A PLOT\n", "counts, bin_edges = np.histogram(f_samples, bins = N_BINS, density = True)\n", "bin_centers = 0.5*(bin_edges[:-1]+bin_edges[1:])\n", "\n", "#Plotting the data\n", "#Alternatively use: #plt.plot(bin_centers,counts)\n", "plt.step(bin_edges[1:],counts)\n", "\n", "#Plotting Gaussian with mean and std given by  f and delta_f\n", "plt.plot(bin_centers, scipy.stats.norm.pdf(bin_centers, loc = f(x_val, y_val), scale = delta_f(x_err, y_err)))"]}, {"cell_type": "markdown", "id": "4b791c59", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='problems_2_1'></a>     \n", "\n", "| [Top](#section_2_0) | [Restart Section](#section_2_1) | [Next Section](#section_2_2) |\n"]}, {"cell_type": "markdown", "id": "1492d4f0", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Problem 2.1.1</span>\n", "\n", "Let $g(x_1,x_2,x_3...x_n) = x_1 + x_2 + x_3 + ... + x_n$ be a function of $n$ variables, where each variable has an error of 5. Complete the code below to manually compute the error of $g$ with 2 variables, 101 variables, and 5012 variables. \n", "\n", "\n", "Hint: Begin with the definition for error propagation. What does it reduce to when the `n` errors are the same?\n", "\n", "$$\\Delta g(x_1, x_2, x_3...x_n) = \\sqrt{(\\partial g / \\partial x_1)^2(\\Delta x_1)^2 + (\\partial g / \\partial x_2)^2(\\Delta x_2)^2 +...+(\\partial g / \\partial x_1)^2(\\Delta x_1)^2}$$"]}, {"cell_type": "code", "execution_count": null, "id": "fec58084", "metadata": {"tags": ["draft", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>PROBLEM: P2.1.1\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "def delta_g(delta_x, n):\n", "  ####################\n", "  # Insert Code Here #\n", "  ####################\n", "  return \n", "\n", "x_err = 5.\n", "\n", "n=2\n", "print(\"g_err(n=2) = %f\" % (delta_g(x_err, n)))\n", "n=101\n", "print(\"g_err(n=101) = %f\" % (delta_g(x_err, n)))\n", "n=5012\n", "print(\"g_err(n=5012) = %f\" % (delta_g(x_err, n)))\n"]}, {"cell_type": "markdown", "id": "f764e4db", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": [">#### Follow-up 2.1.1a (ungraded)\n", ">   \n", ">Why is it important to minimize the number of variables that contribute to your uncertainty? A lot of functions in physics analysis are dependent on not just a linear combination of terms, but perhaps an exponential one. What would the error of this look like with $n$ variables?\n"]}, {"cell_type": "markdown", "id": "9f1952f3", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_2_2'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">P2.2 Error Propagation - A More Complicated Example</h2>    \n", "\n", "| [Top](#section_2_0) | [Previous Section](#section_2_1) | [Problems](#problems_2_2) | [Next Section](#section_2_3) |\n"]}, {"cell_type": "markdown", "id": "d4d962c5", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='problems_2_2'></a>    \n", "\n", "### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Problem 2.2.1</span>\n", "\n", "Now let's take $h(x,y) = (\\sqrt{|x|} + \\sqrt{|y|})\\cdot (x - y)$\n", "\n", "Fill in the following code cell to compute the error on `h(x,y)` using the same values of $x$ and $y$ (and their respective errors) from the example code in P2.1. Take the stdev of `h(x,y)` and add it to the mean of `h(x,y)` to get an upper bound that is \"one error away\". What is this value?\n", "\n", "Enter your answer as a number with precision 1e-1.\n", "\n", "**NOTE:** You must specifically use the random seed and number of samples defined in the code below, for comparison with the answer checker."]}, {"cell_type": "code", "execution_count": null, "id": "c33cba14", "metadata": {"tags": ["draft", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>PROBLEM: P2.2.1\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "np.random.seed(2)\n", "N_SAMPLES = 100000\n", "N_BINS = 100\n", "x_val = 5.\n", "x_err = 2.\n", "y_val = 9.\n", "y_err = 2.\n", "x_samples = np.random.normal(loc = x_val, scale = x_err, size = N_SAMPLES)\n", "y_samples = np.random.normal(loc = y_val, scale = y_err, size = N_SAMPLES)\n", "\n", "def h(x,y):\n", "    return (np.sqrt(np.abs(x))+np.sqrt(np.abs(y)))*(x-y)\n", "\n", "\n", "####################\n", "# Insert Code Here #\n", "####################\n", "\n", "def h_val_err(ix, iy):\n", "    h_samples = None # Placeholder Value - Fill in the correct line\n", "    h_val = None # Placeholder Value - Fill in the correct line\n", "    h_err = None # Placeholder Value - Fill in the correct line\n", "    return h_val, h_err\n", "\n", "\n", "####################\n", "\n", "\n", "h_val, h_err = h_val_err(x_samples, y_samples)\n", "\n", "print(\"observed: h(x) upper bound\", h_val + h_err)"]}, {"cell_type": "markdown", "id": "1bbfe7a7", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Problem 2.2.2</span>\n", "\n", "Write a function that computes the error in `h(x,y)`. What is the expected error, based on the propagation of error formula? Does this match the error that you obtained in the previous problem?\n", "\n", "\n", "Use the values given previously to evaluate `delta_h(x_val,y_val,delta_x,delta_y)`:\n", "\n", "<pre>\n", "x_val = 5.\n", "x_err = 2.\n", "y_val = 9.\n", "y_err = 2.\n", "</pre>\n", "\n", "Enter your answer as a number with precision 1e-2."]}, {"cell_type": "code", "execution_count": null, "id": "b26e293f", "metadata": {"scrolled": false, "tags": ["py", "draft", "learner_chopped"]}, "outputs": [], "source": ["#>>>PROBLEM: P2.2.2\n", "\n", "#defining delta_h\n", "####################\n", "\n", "def delta_h(x_val,y_val,delta_x,delta_y):\n", "    x_deriv = 0 #YOUR CODE HERE\n", "    y_deriv = 0 #YOUR CODE HERE\n", "    return np.sqrt(x_deriv**2. * delta_x**2. + y_deriv**2. * delta_y**2.)\n", "\n", "\n", "print(\"observed: h(x) = %f +/- %f\" % (h_val, h_err))\n", "print(\"expected: h(x) = %f +/- %f\" % (h(x_val, y_val), delta_h(x_val,y_val,x_err,y_err)))"]}, {"cell_type": "markdown", "id": "d297a029", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": [">#### Follow-up 2.2.2a (ungraded)\n", ">   \n", ">Approximate the data as a Gaussian with mean and variance. How well does this Gaussian compare with the data? How does this Gaussian depend on the mean and variance of the underlying distributions?\n"]}, {"cell_type": "markdown", "id": "67f17f68", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_2_3'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">P2.3 Johnson Noise</h2>   \n", "\n", "| [Top](#section_2_0) | [Previous Section](#section_2_2) | [Problems](#problems_2_3) |\n"]}, {"cell_type": "code", "execution_count": null, "id": "5dbd4019", "metadata": {"tags": ["learner", "py", "catsoop_03", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: P2.3-runcell01\n", "\n", "frequency = np.array([   200.,    300.,    400.,    500.,    600.,    700.,    800.,\n", "          900.,   1000.,   1100.,   1200.,   1300.,   1400.,   1500.,\n", "         1700.,   2000.,   3000.,   4000.,   5000.,   7000.,  10000.,\n", "        13000.,  15000.,  17000.,  20000.,  25000.,  30000.,  35000.,\n", "        40000.,  45000.,  50000.,  55000.,  60000.,  65000.,  70000.,\n", "        75000.,  80000.,  85000.,  90000.,  95000., 100000.])\n", "\n", "gain = np.array([  1.56572199,   7.56008454,  24.23507344,  58.36646477,\n", "       119.11924863, 215.75587662, 354.79343025, 517.34083494,\n", "       679.81395988, 805.18954729, 877.53623188, 944.14612835,\n", "       951.12203586, 981.66551215, 976.08071562, 971.57565072,\n", "       991.33195051, 974.54482165, 968.02100388, 970.96127868,\n", "       972.70192708, 980.9122768 , 983.62597547, 981.85446382,\n", "       964.75994752, 984.27991886, 959.44478862, 975.87335094,\n", "       906.24841379, 831.8699187 , 695.5940221 , 562.69096627,\n", "       426.50959034, 328.93671408, 248.14630158, 198.16023325,\n", "       150.59357167, 121.00349255, 100.86777721,  79.42663031,\n", "        63.20952534])\n", "\n", "gain_uncertainty = np.array([5.21317443e-03, 3.11522352e-02, 1.17453781e-01, 1.54063502e-01,\n", "       1.27335068e+00, 1.27124575e+00, 1.62862522e+00, 8.07632112e-01,\n", "       1.39800408e+00, 1.52872753e+00, 9.26100943e-01, 2.07700290e+00,\n", "       2.41624111e+00, 2.48737608e+00, 2.66446131e+00, 6.30956544e+00,\n", "       2.48543922e+00, 5.85031911e+00, 5.36245736e+00, 5.03316166e+00,\n", "       5.96042863e+00, 1.80119083e+00, 2.19189309e+00, 4.76416499e+00,\n", "       2.60518705e+00, 8.91016625e-01, 8.68517783e-01, 7.60893395e-02,\n", "       1.12595429e+00, 9.59211786e-01, 2.11207039e+00, 1.54206027e+00,\n", "       6.15658573e-01, 2.21068956e+00, 1.93131996e+00, 1.17159272e+00,\n", "       1.02084395e+00, 6.45939329e-01, 1.15822783e+00, 1.50426555e-01,\n", "       2.64213908e-01])\n", "\n", "resistance = np.array([477.1e3, 810e3, 99.7e3, 502.3e3, 10.03e3]) \n", "resistance_uncertainty = np.array([0.2e3, 2e3, 0.2e3, 0.3e3, 0.3e3])\n", "\n", "capacitance = 125e-12\n", "capacitance_uncertainty = 14e-12\n", "\n", "#measured Johnson Noise and uncertainty\n", "#note that this is the value of the left side of the equation shown above\n", "v2rmsd4t = np.array([2.57337556e-08, 1.96214066e-08, 2.21758082e-08, 2.38320749e-08,\n", "       7.31633110e-09])\n", "v2rmsd4t_uncertainty = np.array([1.25267830e-09, 1.46644504e-09, 1.08426579e-09, 1.77538860e-09,\n", "       2.07583938e-10])\n"]}, {"cell_type": "code", "execution_count": null, "id": "e0bdb35a", "metadata": {"tags": ["learner", "py", "catsoop_03", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: P2.3-runcell02\n", "\n", "from scipy.integrate import trapz\n", "\n", "def mc_compute(freq, gain, gain_error, r, rerr, cap, cap_err, n_samp):\n", "    samples = []\n", "    for k in range(n_samp):\n", "        mc_gain = gain + np.random.normal(len(gain))*gain_error\n", "        mc_r = r + rerr*np.random.normal(1)\n", "        mc_cap = cap + cap_err*np.random.normal(1)\n", "        mc_integrand = mc_gain**2.0/(1+ (2*np.pi*mc_r*mc_cap*freq)**2.0)\n", "        mc_int = scipy.integrate.trapz(mc_integrand, freq)\n", "        samples.append(mc_r*mc_int)\n", "    return np.array(samples)"]}, {"cell_type": "code", "execution_count": null, "id": "e329e0ce", "metadata": {"tags": ["learner", "py", "catsoop_03", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: P2.3-runcell03\n", "\n", "rgr = []\n", "rgr_unc = []\n", "for k in range(5):\n", "    samples = mc_compute(frequency, gain, gain_uncertainty, resistance[k], resistance_uncertainty[k], capacitance, capacitance_uncertainty,100)\n", "    rgr.append(np.mean(samples))\n", "    rgr_unc.append(np.std(samples))\n", "rgr = np.array(rgr)  \n", "rgr_unc = np.array(rgr_unc)"]}, {"cell_type": "code", "execution_count": null, "id": "bd7062a1", "metadata": {"tags": ["learner", "py", "catsoop_03", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: P2.3-runcell04\n", "\n", "plt.errorbar(rgr, v2rmsd4t, yerr = v2rmsd4t_uncertainty, xerr= rgr_unc, fmt = 'o' )\n", "plt.show()"]}, {"cell_type": "code", "execution_count": null, "id": "54efaf35", "metadata": {"tags": ["learner", "py", "catsoop_03", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: P2.3-runcell05\n", "\n", "from lmfit.models import Model\n", "\n", "def linear_model(x, k, b):\n", "    return x/k+b\n", "\n", "lmod = Model(linear_model)\n", "lmod.set_param_hint(name = 'k', value = 1)\n", "lmod.set_param_hint(name = 'b', value = 1)\n", "result = lmod.fit(1e-15*rgr, x = v2rmsd4t*1e8, weights = 1/(rgr_unc*1e-15))\n", "print(result.fit_report())\n", "result.plot_fit()\n", "plt.show()"]}, {"cell_type": "code", "execution_count": null, "id": "bf612f99", "metadata": {"tags": ["learner", "py", "learner_chopped", "catsoop_03"]}, "outputs": [], "source": ["#>>>RUN: P2.3-runcell06\n", "\n", "m_earth = 5.9722 * 10**24 \n", "m_earth_unc = 6*10^22\n", "\n", "r_earth = 6.371 * 10**6\n", "r_earth_unc = 100000\n", "\n", "#length and unc\n", "l = [.1, .2, .3, .4, .5, .6, .7, .8, .9, 1.0]\n", "l_unc = [.01, .02, .03, .03, .03, .04, .03, .04, .05, .04]\n", "\n", "T = [0.6468, 0.9317, 1.1352, 1.4553, 1.6181, 1.782,  1.969,  2.068,  2.211,  2.255 ]\n", "#Note, we are assuming that the experiment is run for a very large number of pendulum swings\n", "#so that the uncertainty in the period is negligible\n", "\n", "\n", "pi2dTsqrd = ((2*np.pi)/np.array(T))**2"]}, {"cell_type": "markdown", "id": "985ec672", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='problems_2_3'></a>   \n", "\n", "| [Top](#section_2_0) | [Restart Section](#section_2_3) |\n"]}, {"cell_type": "markdown", "id": "3df42c28", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Problem 2.3.1</span>\n", "\n", "Using an approach similar to the Johnson noise example, calculate the error on Newton's gravitational constant in this experiment as a percent. Enter your answer as a percentage with precision 1e-3 (for instance an answer of 7.25% would be entered as 7.250). Use the code below as a starting point.\n"]}, {"cell_type": "code", "execution_count": null, "id": "3f754b30", "metadata": {"tags": ["draft", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>PROBLEM: P2.3.1\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "np.random.seed(2)\n", "from lmfit.models import Model\n", "\n", "def compute(m_earth, m_earth_unc, r_earth, r_earth_unc, l, l_unc, n_samp):\n", "    samples = []\n", "    ########################\n", "    ### INSERT CODE HERE ###\n", "    ########################\n", "    return np.array(samples)\n", "\n", "\n", "def get_rgr_data(m_earth, m_earth_unc, r_earth, r_earth_unc, l, l_unc, n_samp):\n", "    rgr = []\n", "    rgr_unc = []\n", "    for k in range(10):\n", "        ########################\n", "        ### INSERT CODE HERE ###\n", "        ########################\n", "        \n", "    rgr = np.array(rgr)  \n", "    rgr_unc = np.array(rgr_unc)\n", "    return rgr, rgr_unc\n", "\n", "\n", "def linear_model(x, G, b):\n", "    #the fit function again includes a non-zero intercept\n", "    return ######INSERT CODE HERE########\n", "\n", "\n", "n_samp = 100\n", "rgr, rgr_unc = get_rgr_data(m_earth, m_earth_unc, r_earth, r_earth_unc, l, l_unc, n_samp)\n", "lmod = Model(linear_model)\n", "lmod.set_param_hint(name = 'G', value = 1)\n", "lmod.set_param_hint(name = 'b', value = 0.1)\n", "result = lmod.fit(1e-11*rgr, x = pi2dTsqrd, weights = 1/(rgr_unc*1e-11))\n", "print(result.fit_report())\n", "result.plot_fit()\n", "plt.show()"]}, {"cell_type": "markdown", "id": "ba702954", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": [">#### Follow-up 2.3.1a (ungraded)\n", ">\n", ">Compare your calculation of $G$ using the numbers given above to the current accepted value of Newton's gravitational constant. Where did the majority of your error come from?"]}], "metadata": {"celltoolbar": "Tags", "kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}