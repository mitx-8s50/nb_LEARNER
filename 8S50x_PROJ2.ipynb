{"cells": [{"cell_type": "markdown", "id": "4c879cea", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<hr style=\"height: 1px;\">\n", "<i>This notebook was authored by the 8.S50x Course Team, Copyright 2022 MIT All Rights Reserved.</i>\n", "<hr style=\"height: 1px;\">\n", "<br>\n", "\n", "<h1>Project 2 - Part I: Measuring Properties of the W Boson from LHC Data</h1>\n"]}, {"cell_type": "markdown", "id": "88e7c942", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_2_0'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">PROJ2.0 Overview</h2>\n"]}, {"cell_type": "markdown", "id": "038120ad", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<h3>Navigation</h3>\n", "\n", "<table style=\"width:100%\">\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_2_1\">PROJ2.1 Introduction and Data Exploration</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#problems_2_1\">PROJ2.1 Checkpoints</a></td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_2_2\">PROJ2.2 Event Selection and Background Mitigation</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#problems_2_2\">PROJ2.2 Checkpoints</a></td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_2_3\">PROJ2.3 Beginning to Look for the W Signal in the Data</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#problems_2_3\">PROJ2.3 Checkpoints</a></td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_2_4\">PROJ2.4 Refining our Selection to Look for the W Signal in the Data</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#problems_2_4\">PROJ2.4 Checkpoints</a></td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_2_5\">PROJ2.5 Fit for W Peak</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#problems_2_5\">PROJ2.5 Checkpoints</a></td>\n", "    </tr>\n", "</table>"]}, {"cell_type": "markdown", "id": "ce052a59", "metadata": {"tags": ["learner", "catsoop_00", "md"]}, "source": ["<h3>Learning Objectives</h3>\n", "\n", "In this lab we will investigate W bosons produced in the LHC's 8 TeV proton proton collisions. These samples were produced some years ago, in a fun experiment that opened up the option of performing low mass resonance searches at the LHC. The studies done then have led to a wealth of results from both LHC experiments, ATLAS and CMS.\n", "\n", "Specifically, in this part of the Project, we will explore the following objectives:\n", "\n", "- Downloading and understanding the data\n", "- Learning about event selection and background mitigation\n", "- Examining simulated data\n", "- Fitting the W peak to find the W boson mass\n", "\n"]}, {"cell_type": "markdown", "id": "d0c4ac89", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Slides</h3>\n", "\n", "Run the code below to view the slides for this section, which are discussed in the related video. You can also open the slides in a separate window <a href=\"https://mitx-8s50.github.io/slides/L09/slides_L09_09.html\" target=\"_blank\">HERE</a>."]}, {"cell_type": "code", "execution_count": null, "id": "9e278ce5", "metadata": {"tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: PROJ2.0-slides\n", "\n", "from IPython.display import IFrame\n", "IFrame(src='https://mitx-8s50.github.io/slides/L09/slides_L09_09.html', width=970, height=550)"]}, {"cell_type": "markdown", "id": "0e495862", "metadata": {"tags": ["learner", "md", "catsoop_00", "learner_chopped"]}, "source": ["<h3>Data</h3>\n", "\n", ">description: Boosted Single Jet dataset at 8TeV<br>\n", ">source: https://zenodo.org/record/8035318 <br>\n", ">attribution: Philip Harris (CMS Collaboration), DOI:10.5281/zenodo.8035318 "]}, {"cell_type": "code", "execution_count": null, "id": "2bc7f81e", "metadata": {"tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: PROJ2.0-runcell00\n", "\n", "# NOTE: these files are too large to include in the original repository,\n", "# so you must download them using the options below\n", "#\n", "# Ways to download:\n", "#     1. Copy/paste the link (replace =0 with =1 to download automatically)\n", "#     2. Use the wget commands below (works in Colab, but you may need to install wget if using locally)\n", "#\n", "# Location of files:\n", "#     Move the files to the directory 'data'\n", "#\n", "# Using wget: (works in Colab)\n", "#     Upon downloading, the code below will move them to the appropriate directory\n", "\n", "#3GB Data Set: data1\n", "!wget https://www.dropbox.com/s/bcyab2lljie72aj/data.tgz?dl=0\n", "!mv data.tgz?dl=0 data.tgz #rename\n", "!tar -xvf data.tgz #extract the data\n", "!rm data.tgz #clean the downloaded file\n", "\n", "#130MB Data Set: data2\n", "!wget https://www.dropbox.com/s/p756oa4mfw17lfw/data.zip?dl=0\n", "!mv data.zip?dl=0 data.zip #rename\n", "!unzip data.zip #extract the data\n", "!rm data.zip #clean the downloaded file"]}, {"cell_type": "markdown", "id": "f6bf7ec4", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Importing Libraries</h3>\n", "\n", "Before beginning, run the cell below to import the relevant libraries for this notebook."]}, {"cell_type": "code", "execution_count": null, "id": "9f8e7998", "metadata": {"tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: PROJ2.0-runcell01\n", "\n", "# pre-requisites\n", "# uproot High energy physics python file format: https://masonproffitt.github.io/uproot-tutorial/aio.html\n", "#!pip install uproot #install uproot if you have not done this already\n", "import uproot\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "from scipy.optimize import curve_fit\n", "import os,sys\n", "\n", "#!pip install lmfit #install lmfit if you have not done this already\n", "import lmfit as lm\n", "\n", "#!pip install mplhep #install mplhep if you have not done this already\n", "# plotting style for High Energy physics \n", "import mplhep as hep\n", "plt.style.use(hep.style.CMS)"]}, {"cell_type": "markdown", "id": "8cb1515b", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Setting Default Figure Parameters</h3>\n", "\n", "The following code cell sets default values for figure parameters."]}, {"cell_type": "code", "execution_count": null, "id": "27cba66a", "metadata": {"tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: PROJ2.0-runcell02\n", "\n", "#set plot resolution\n", "%config InlineBackend.figure_format = 'retina'\n", "\n", "#set default figure parameters\n", "plt.rcParams['figure.figsize'] = (6,6)\n", "\n", "medium_size = 12\n", "large_size = 15\n", "\n", "plt.rc('font', size=medium_size)          # default text sizes\n", "plt.rc('xtick', labelsize=medium_size)    # xtick labels\n", "plt.rc('ytick', labelsize=medium_size)    # ytick labels\n", "plt.rc('legend', fontsize=medium_size)    # legend\n", "plt.rc('axes', titlesize=large_size)      # axes title\n", "plt.rc('axes', labelsize=large_size)      # x and y labels\n", "plt.rc('figure', titlesize=large_size)    # figure title\n"]}, {"cell_type": "markdown", "id": "5c0aa877", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_2_1'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">PROJ2.1 Introduction and Data Exploration</h2>    \n", "\n", "| [Top](#section_2_0) | [Previous Section](#section_2_0) | [Checkpoints](#problems_2_1) | [Next Section](#section_2_2) |\n"]}, {"cell_type": "markdown", "id": "1e85e7fd", "metadata": {"tags": ["learner", "catsoop_01", "md"]}, "source": ["<h3>W to QQ</h3>\n", "\n", "Let's first consider the process that we would like to look for. The production of W bosons in proton collisions. Here is a Feynman diagram of the process. \n", "\n", "<!--<img src=\"images/Wqq.png\" width=\"300\"/>-->\n", "<p align=\"center\">\n", "<img alt=\"Wqq\" src=\"https://raw.githubusercontent.com/mitx-8s50/images/main/PROJ2/Wqq.png\" width=\"300\"/>\n", "</p>\n", "\n", ">source: https://arxiv.org/abs/1807.07454<br>\n", ">attribution: arXiv:1807.07454, Marat Freytsis, Philip Harris, Andreas Hinzmann, Ian Moult, Nhan Tran, Caterina Vernieri\n", "\n", "The left part of the diagram represents the production of the W boson via some initial quark interaction (quarks and anti-quarks are present when two protons collide). At the right you have a gluon (bottom) that is produced in association with the W boson (top). At the top right, the W boson is decaying. It can decay to many things. The full list of W boson decays is <a href=\"http://pdg.lbl.gov/2012/listings/rpp2012-list-w-boson.pdf\" target=\"_blank\">here</a>, in the W branching ratios section. The quark label generically means that the W boson decays to two quarks. In the reference document this is equivalent to a decay to hadrons. \n", "\n", "Both quarks and gluons will decay into objects that we refer to as *jets*. A jet is collection of particles coming from an original quark or gluon. I will not explain the details of a jet, instead I will point you to summer school lectures I gave on this <a href=\"https://indico.fnal.gov/event/11505/session/30/?slotId=0#20160820\" target=\"_blank\">here</a> (start from slide 36). "]}, {"cell_type": "markdown", "id": "a8e65066", "metadata": {"tags": ["learner", "catsoop_01", "md"]}, "source": ["<h3>Lorentz boost</h3>\n", "    \n", "This problem becomes interesting when the W boson has a high energy, or in other words it is boosted. In this case, the decays of the W are restricted to within a cone. A simple calculation of special relativity <font color='red'> that you should do </font> will give you that the maximum angular separation $\\Delta \\theta$ between the two decay products, can be described by:\n", "\n", "$$\n", "\\begin{eqnarray}\n", "\\Delta \\theta & < & \\frac{2m}{p}.\n", "\\end{eqnarray}\n", "$$\n", "\n", "Where, $m$ is the mass and $p$ is the momentum of the resonance decaying, in this case the W boson. Thus, by taking $p$ to be sufficiently high, the angle $\\theta$ is sufficiently small that we can resolve the two quark decays as one single jet cone instead of two separate jets. We will still take a large cone with $\\Delta \\theta_{max} = 0.8$. \n", "This means that our final state will look like this in the detector:\n", "\n", "<!--<img src=\"images/wjet.png\" width=\"300\"/>-->\n", "<p align=\"center\">\n", "<img alt=\"W jet\" src=\"https://raw.githubusercontent.com/mitx-8s50/images/main/PROJ2/wjet.png\" width=\"300\"/>\n", "</p>\n", "\n", "**Note:** A small point about collisions at the LHC is that in place of momentum we often use a variable called transverse momentum or $\\vec{p}_{T}$. This is the projection of $\\vec{p}$ onto the plane perpendicular to the collision. This plane is particularly well understood since, by transverse momentum conservation, all collisions need to have $\\sum_{i} \\vec{p_{T}}^{i}=0$ for all resulting particles in the collision. For this analysis, you can effectively interchange $p$ and $\\vec{p}_{T}$.\n", "\n", "Some elementary introduction about detector's geometry can be found <a href=\"https://www.lhc-closer.es/taking_a_closer_look_at_lhc/0.momentum\" target=\"_blank\">here</a>"]}, {"cell_type": "markdown", "id": "5744ec85", "metadata": {"tags": ["learner", "catsoop_01", "md"]}, "source": ["<h3>Backgrounds</h3>\n", "\n", "Once we have a cone sufficient to detect both decay products we need to find a jet with two quarks. To find a jet with two quarks we need to remove our background processes, which are events that \"look like\" the interaction we want to find, from our events. Our main background process consists of the diagrams below:\n", "\n", "<!--<img src=\"images/dijet.png\" width=\"600\"/>-->\n", "<p align=\"center\">\n", "<img alt=\"Dijet background\" src=\"https://raw.githubusercontent.com/mitx-8s50/images/main/PROJ2/dijet.png\" width=\"600\"/>\n", "</p>\n", "\n", "\n", "where quarks and gluons are produced by the strong force and manifest in the detector as jets. We call this background *multijet* or *QCD background*, which stands for Quantum Chromodynamics. Other sub-dominant background processes are the production of top quark pairs $t\\bar{t}$, refered to as the *top quark background*, or the production of a pair of W or Z bosons, which we refer to as the *diboson background*."]}, {"cell_type": "markdown", "id": "66e6ba56", "metadata": {"tags": ["learner", "catsoop_01", "md"]}, "source": ["<h3>Other processes</h3>\n", "\n", "There are additional processes that will produce a jet and gluon in the final state as well, which we might be interested in looking for as well. These are the production of Z bosons and Higgs bosons H. They are also resonances that may decay into a pair of quarks and can get reconstructed in one single jet cone - *but what is interesting is that they most often decay into a pair of b-quarks*. \n", "\n", "Besides its decay, one can use the mass of the resonance to distinguish between these signatures: the Z boson has a mass of $\\sim 90~$GeV, the Higgs boson has a mass of $\\sim 125~$GeV, while the W boson has a mass of $\\sim 80.4~$GeV.\n", "\n", "As a summary, our main background consists of either a quark or a gluon and our signal is a boson with 2 quarks inside. **So the challenge is to *construct an identification algorithm of a jet that looks like it originated from two quarks*.**"]}, {"cell_type": "markdown", "id": "a2e0e045", "metadata": {"tags": ["learner", "catsoop_01", "md", "learner_chopped"]}, "source": ["<h3> Loading data & Auxiliary functions</h3>\n", "\n", "Before we start, let's define collider coordinates centered around the collision point. We tend to write our momentum 4-vector as $\\vec{p}=(p_{T},\\eta,\\phi,m)$ in place of $\\vec{p}=(p,\\theta,\\phi,m)$. You can read more in this short link <a href=\"https://www.lhc-closer.es/taking_a_closer_look_at_lhc/0.momentum\" target=\"_blank\">here</a>.\n", "\n", "Now is a good time to look at the data. Let's take a look at the different samples we have. You should have run the first cell in Section 0 to download the data, but you can also download from these links:\n", "- <a href=\"https://www.dropbox.com/s/bcyab2lljie72aj/data.tgz?dl=0\" target=\"_blank\">3 GB file</a>\n", "- <a href=\"https://www.dropbox.com/s/p756oa4mfw17lfw/data.zip?dl=0\" target=\"_blank\">130 MB file</a>\n", "\n", "These files should be in a directory called `data`.\n"]}, {"cell_type": "markdown", "id": "3051b558", "metadata": {"tags": ["learner", "catsoop_01", "md"]}, "source": ["Here are the different datasets:\n", "\n", "* **Data**: *data/JetHT_s.root*. The 8 TeV JetHT dataset. This means that the data passed an online selection (trigger) that required the event to have jets. More on triggers below.\n", "\n", "* **W(qq) simulation**: Here we have different options for a simulated qq=>W=>qq dataset\n", "    * *data/WQQ_s.root*: 8 TeV collision energy (low number of events)\n", "    * *data/skimh/WQQ_sh.root*: 13 TeV collision energy (high number of events but different collision energy). Can use these to train NNs and make nice plots.\n", "    * *data/WQQ_new.root*: 8 TeV collision energy (newer dataset with not so high number of events)* \n", "\n", "* **Z(qq) simulation**: Again, we have different options for a simulated qq=>Z=>qq dataset\n", "    * *data/ZQQ_s.root*: 8 TeV collision energy (low number of events)\n", "    * *data/skimh/ZQQ_sh.root*: 13 TeV collision energy (high number of events but higher collision energy). Can use these to train NNs and make nice plots.\n", "    * *data/ZQQ_new.root*: 8 TeV collision energy (newer dataset with not so high number of events)\n", "\n", "* **H(bb) simulation**: *data/ggH.root*: This is a small simulated gg=>H=>bb dataset at 8 TeV collision energy. (we might need this in the future).\n", "\n", "* **Multijet production or QCD background simulation**: *data/QCD_s.root*. This is our main background. And our worst modeled. We just call these backgrounds QCD because they are produced with Quantum chromodynamics.\n", "\n", "* **Top quark pair production simulation**: *data/TT.root*. This is a background sample with top quark decays\n", "\n", "* **Diboson simulation**: *data/WW.root,data/WZ.root,data/ZZ.root*. These are three rarer double W, W+Z and Z+Z diboson samples where we have two bosons instead of one."]}, {"cell_type": "code", "execution_count": null, "id": "697018b9", "metadata": {"tags": ["learner", "py", "catsoop_01", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: PROJ2.1-runcell01\n", "\n", "# Now let's open the data. \n", "\n", "# DATA\n", "#-------------------------------------------------------------------------------\n", "# Our data sample is the JetHT dataset. \n", "# What that means is the data passed triggers that have a jet in one of the triggers (discussed below).\n", "data   = uproot.open(\"data/JetHT_s.root\")[\"Tree\"]\n", "\n", "\n", "# SIMULATION\n", "#-------------------------------------------------------------------------------\n", "# In addition to above we have Monte Carlo Simulation of many processes\n", "# Some of these process are well modelled in simulation and some of them are not\n", "\n", "#the process qq=>W=>qq and qq=>Z=>qq processes at 8TeV collision energy\n", "wqq    = uproot.open(\"data/WQQ_s.root\")[\"Tree\"] \n", "zqq    = uproot.open(\"data/ZQQ_s.root\")[\"Tree\"]\n", "\n", "# To train NNs and make nice plots we will use larger samples produced at a different collision energy\n", "# qq=>W=>qq and qq=>Z=>qq at 13TeV collision energy\n", "wqq13  = uproot.open(\"data/skimh/WQQ_sh.root\")[\"Tree\"]\n", "zqq13  = uproot.open(\"data/skimh/ZQQ_sh.root\")[\"Tree\"]\n", "\n", "# qq=>W=>qq and qq=>Z=>qq at 8TeV collision energy\n", "wqq_n  = uproot.open(\"data/WQQ_8TeV_Jan11_r.root\")[\"Tree\"]\n", "zqq_n  = uproot.open(\"data/ZQQ_8TeV_Jan11_r.root\")[\"Tree\"]\n", "\n", "# Now we have our worst modeled background this is also our main background. \n", "# This is is our di-jet quark and gluon background. \n", "# We just call these backgrounds QCD because they are produced with Quantum Chromo Dynamics.\n", "qcd    = uproot.open(\"data/QCD_s.root\")[\"Tree\"]\n", "\n", "# Now we have the Higgs boson sample (we might need this in the future)\n", "ggh    = uproot.open(\"data/ggH.root\")[\"Tree\"]\n", "\n", "# And top-quark pair production background. \n", "tt     = uproot.open(\"data/TT.root\")[\"Tree\"]\n", "\n", "# Finally we have the rarer double W, W+Z and Z+Z diboson samples where we have two bosons instead of one\n", "ww     = uproot.open(\"data/WW.root\")[\"Tree\"]\n", "wz     = uproot.open(\"data/WZ.root\")[\"Tree\"]\n", "zz     = uproot.open(\"data/ZZ.root\")[\"Tree\"]\n", "\n", "dataDict = {'qcd': qcd,\n", "            'tt': tt,\n", "            'data': data,\n", "            'wqq': wqq,\n", "            'zqq': zqq,\n", "            'wqq13': wqq13,\n", "            'zqq13': zqq13,\n", "            'wqq_n': wqq_n,\n", "            'zqq_n': zqq_n,\n", "            'ww': ww,\n", "            'zz': zz,\n", "            'wz': wz,\n", "            'ggh': ggh\n", "            }\n", "from collections import OrderedDict \n", "\n", "order_of_keys = ['data','qcd','tt','ww','zz','wz','wqq','wqq13','wqq_n','zqq','zqq13','zqq_n','ggh']\n", "list_of_tuples = [(key, dataDict[key]) for key in order_of_keys]\n", "OrdDataDict = OrderedDict(list_of_tuples)"]}, {"cell_type": "markdown", "id": "d5db1164", "metadata": {"tags": ["learner", "catsoop_01", "md"]}, "source": ["<h3>Exploring data</h3>\n", "\n", "Now, let's explore the data. There are a lot of different variables, but most of them will not used for this study. \n", "However, you should feel free to explore the different variables. For completeness, I will write a table of all of the different variables below: \n", "\n", "| sample   | book keeping variable | \n", "|------|------|\n", "| run      | LHC run  period           |\n", "| lumi     | LHC run period sub section | \n", "| event    | LHC collision id |\n", "| trigger  | Bitmask of triggers that have been passed |  \n", "| hltmatch | ??? (unused I think) | \n", "| puweight | Weight to match the beam inensity (so called Pileup) |\n", "| npu      | For simulation the number of simulated pileup collisions | \n", "| npuPlusOne      | \"\" |\n", "| npuMinusOne     | \"\" |\n", "| nvtx            | Number of reconstructed vertices (a proxy for the total number of collisions) | \n", "| metFiltersWord  | Bitmask of whether event had anamalous detector features | \n", "| scale1fb        | The expected number of events per 1/fb of data | \n", "| rho             | energy density | \n", "| metRaw          | Raw Missing Transverse Energy (this is a proxy of the direction of invisible particles in the transverse plane | \n", "| metRawPhi       | Raw Missing Transverse Energy direction in transverse plane | \n", "| met             | Corrected metRaw | \n", "| metphi          | Correct metRawPhi | \n", "| tkmet           | charged metRaw | \n", "| tkmetphi        | charged metRawPhi | \n", "| mvamet          | ML  corrected metRaw |  \n", "| mvametphi       | ML corrected metRawPhi | \n", "| puppet          | PUPPI corrected metRaw |\n", "| puppetphi       | PUPPI corrected metRawPhi | \n", "| mt              | relativistic mass of (met+leading jet) in the transverse plane | \n", "| rawmt           | relativistic mass of (metRaw+leading jet) in the transverse plane | \n", "| tkmt            | relativistic mass of (tkmet+leading jet) in the transverse plane | \n", "| mvamt           | relativistic mass of (mvamet+leading jet) in the transverse plane | \n", "| puppetmt        | relativistic mass of (puppet+leading jet) in the transverse plane | \n", "| metSig          | probalbilistic measure missing transverse Energy is from 0 in sigma | \n", "| mvaMetSig       | probalbilistic mvamet is from 0 in sigma | \n", "| njets           | Number of jets with pt > 30 GeV | \n", "| nbtags          | Number of b-jets with pt > 30 GeV | \n", "| nfwd            | Number of jets with pt > 30 GeV and abs(eta) > 2.5 | \n", "| mindphi         | minimum direction in transverse plane of all jets and met | \n", "| j0_pt           | leading small jet pt | \n", "| j0_eta          | leading small jet $\\eta$ | \n", "| j0_phi          | leading small jet $\\phi$ | \n", "| j1_pt           | sub leading small jet pt | \n", "| j1_eta          | sub leading small jet $\\eta$ | \n", "| j1_phi          | sub leading small jet $\\phi$ | \n", "| j2_pt           | third highest leading small jet pt | \n", "| j2_eta          | third highest leading small jet $\\eta$ | \n", "| j2_phi          | third leading small jet $\\phi$ | \n", "| j0_mass         | leading jet mass | \n", "| j0_csv          | leading jet ML b-quark likelihood ML discriminator |\n", "| j0_qgid         | leading jet quark vs gluon discrminator | \n", "| j0_chf          | leading jet charged particle fraction  |\n", "| j0_nhf          | leading jet neutral (on photon) particle fraction  |\n", "| j0_emf          | leading jet photon particle fraction  |\n", "| j0_dphi         | delta $\\phi$ w. respect to the sub-leading jet | \n", "| j1_mass         | subleading jet mass | \n", "| j1_csv          | subleading jet ML b-quark likelihood ML discriminator |\n", "| j1_qgid         | subleading jet quark vs gluon discrminator | \n", "| j1_chf          | subleading jet charged particle fraction  |\n", "| j1_nhf          | subleading jet neutral (on photon) particle fraction  |\n", "| j1_emf          | subleading jet photon particle fraction  |\n", "| j1_dphi         | delta $\\phi$ w. respect to the leading jet  | \n", "| j2_mass         | third highest jet mass | \n", "| j2_csv          | third highest ML b-quark likelihood ML discriminator |\n", "| j2_qgid         | third highest jet quark vs gluon discrminator | \n", "| j2_chf          | third highest jet charged particle fraction  |\n", "| j2_nhf          | third highest jet neutral (on photon) particle fraction  |\n", "| j2_emf          | third highest jet photon particle fraction  |\n", "| j2_dphi         | delta $\\phi$ w. respect to the closest jet  | \n", "| dj0_pt          | Lead and sub leading jets combined 4-vector pt  | \n", "| dj0_mass        | Lead and sub leading jets combined 4-vector mass | \n", "| dj0_phi         | Lead and sub leading jets combined 4-vector $phi$ | \n", "| dj0_y           | Lead and sub leading jets combined 4-vector rapidity | \n", "| dj0_qgid        | Lead and sub leading jets combined quark gluon| \n", "| dj0_csv         | Lead and sub leading jets combined b-quark discriminator | \n", "| dj0_jdphi       | Lead and sub leading jets difference in transverse plane | \n", "| nvjet           | number of fat jets | \n", "| vjet0_pt        | fat jet pt | \n", "| vjet0_eta       | fat jet $\\eta$ | \n", "| vjet0_phi       | fat jet $\\phi$ | \n", "| vjet0_mass      | fat jet mass | \n", "| vjet0_csv       | fat jet b-tag probability | \n", "| vjet0_flavor    | fat jet flavor id (if simulation) | \n", "| vjet0_t1        | fat jet $\\tau_{1}$ | \n", "| vjet0_t2        | fat jet $\\tau_{2}$ | \n", "| vjet0_t3        | fat jet $\\tau_{3}$ | \n", "| vjet0_msd0      | fat jet soft drop mass $\\beta=0$ |\n", "| vjet0_msd1      | fat jet soft drop mass $\\beta=1$ |\n", "| vjet0_mprune    | fat jet pruned mass |\n", "| vjet0_mtrim     | fat jet pruned mass |\n", "| vjet0_pullAngle | fat jet color flow variable between quarks |\n", "| vjet0_sj1_csv   | fat jet highest momentum subjet b-tag ML discriminator | \n", "| vjet0_sj2_csv   | fat jet subleading momentum subjet b-tag ML discriminator | \n", "| vjet0_sj1_qgid  | fat jet highest momentum subjet quark gluon likelihood | \n", "| vjet0_sj2_qgid  | fat jet subleading momentum subjet quark gluon likelihood | \n", "| vjet0_sj1_q     | fat jet highest momentum subjet charge  | \n", "| vjet0_sj2_q     | fat jet highest momentum subjet charge  | \n", "| vjet0_sj1_z     | fat jet highest momentum subjet energy relative to fat jet  | \n", "| vjet0_sj2_z     | fat jet subleading momentum subjet energy relative to fat jet  | \n", "| vjet0_iso15     | fat jet isolation with 1.5 cone | \n", "| vjet0_c2b0      | fat jet  $C_{2}^{\\beta=0}$ correlation function for two likelihood |  \n", "| vjet0_c2b0P2    | fat jet $C_{2}^{\\beta=0.2}$ correlation function for two likelihood |  \n", "| vjet0_c2b0P5    | fat jet $C_{2}^{\\beta=0.5}$ correlation function for two likelihood |  \n", "| vjet0_c2b1P0    | fat jet $C_{2}^{\\beta=1.0}$ correlation function for two likelihood |  \n", "| vjet0_c2b2P0    | fat jet $C_{2}^{\\beta=2.0}$ correlation function for two likelihood |  \n", "| vjet0_qjet      | fat jet quantum jet volatility |  \n", "| vjet0_trig      | fat jet trigger matched | \n", "| vjet0_genm      | fat jet simulated mass (if matched to a jet) |\n", "| vjet0_genV      | ??? |\n", "| nmuons          | number of muons | \n", "| mu0_pt          | Leading Muon  pt | \n", "| mu0_eta         | Leading Muon $\\eta$ | \n", "| mu0_phi         | Leading Muon $\\phi$ | \n", "| dm0_pt          | dimuon combined 4-vector pt | \n", "| dm0_mass        | dimuon combined 4-vector relativistic mass | \n", "| dm0_phi         | dimuon combined 4-vector $\\phi$ | \n", "| dm0_y           | dimuon combined 4-vector rapidity | \n", "| nelectrons      | number of electrons | \n", "| e0_pt           | leading electron pt | \n", "| e0_eta          | leading electron $\\eta$ |  \n", "| e0_phi          | leading electron $\\phi$ |\n", "| ntaus           | number of hadronic $\\tau_{h}$ | \n", "| tau0_pt         | leading $\\tau_{h}$ $p_{T}$ |\n", "| tau0_eta        | leading $\\tau_{h}$ $\\eta$ |\n", "| tau0_phi        | leading $\\tau_{h}$ $\\phi$ |\n", "| nphotons        | number of additional photons | \n", "| pho0_pt         | leading photon $p_{T}$ |\n", "| pho0_eta        | leading photon $\\eta$ |\n", "| pho0_phi        | leading photon $\\phi$ |\n", "\n", "Now all of these variables are not needed. In this selection we will focus on what we call \"fat jets\". The labels there given by `vjet_`. Fat jets are large cone jets that are reconstructed with a large radius $\\Delta\\theta$ to ensure that both quarks are in the cone. To isolate a single collision we also are applying the PUPPI algorithm (you can ask me about this). You should focus on the `vjet` variables for this project. "]}, {"cell_type": "code", "execution_count": null, "id": "6dedeedc", "metadata": {"tags": ["learner", "py", "catsoop_01", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: PROJ2.1-runcell02\n", "\n", "#these are the datasets that we are working with\n", "#keys: ['data','qcd','tt','ww','zz','wz','wqq','wqq13','wqq_n','zqq','zqq13','zqq_n','ggh']\n", "\n", "# You can view all of these variables within each dataset using the `keys` option\n", "print('wqq keys')\n", "print(wqq.keys())\n", "print()\n", "print('data keys')\n", "print(data.keys())\n", "#note the bdt varaibles at the end  of the `data` can be ignored,\n", "#this is an old deep learning training that we will not use for this study\n"]}, {"cell_type": "markdown", "id": "84e4883a", "metadata": {"tags": ["learner", "catsoop_01", "md"]}, "source": ["<h3>Weights of the simulated data</h3>\n", "\n", "Before we look at the simulated data, we need to understand how we weight our simulation. \n", "\n", "The weights can be written out as \n", "\\begin{eqnarray}\n", "w_{tot} & = & \\rm{total data} \\times \\frac{\\sigma}{N_{\\rm{events}}} \\times w_{PU}\n", "\\end{eqnarray}\n", "\n", "We apply three weights:\n", "\n", "- The total data is defined as the total amount data in our sample. To compute this we quote our data in units of $fb^{-1}$. This is a \"femto-barn\" where a barn is $10^{-28}m^{2}$, a volume of area (that rumour has it Enrico Fermi claimed was as big as a barn).  Note, the total luminosity collected for 8 TeV data was 18.3 $fb^{-1}$.** This translated to 18300 $pb^{-1}$ (picobarns).\n", "\n", "- Our next weight is the cross section, $\\sigma$ divided over the number of generated events. $\\sigma$ is the interaction cross section of the process. We save this ratio in units of $fb$ so that it cancels with our total data \"luminosity\". In our data files this weight is saved as `scale1fb`. *Note: In most of our data files this variable is saved in units of $pb$ so we need to multiply by an extra factor of 1000. This extra factor of 1000 does not apply for the new wqq_n and zqq_n samples.*\n", "\n", "- Lastly, we apply a pileup weight, $w_{PU}$ to match the simulated beam intensity. Pileup stands for the additional interactions between protons when two proton bunches collide at the LHC. This has an effect at modifying the simulation in a certain way. We account for this with the variable ` puweight` saved in our data files.\n", "\n", "Therefore we expect a list of weights such as the following:\n", "```weights=[1000*18300,\"puweight\",\"scale1fb\"]```\n", "where the first element of the list is a *fixed scaling number* and the *last two are variable weights* saved in our files."]}, {"cell_type": "code", "execution_count": null, "id": "92e21ff3", "metadata": {"tags": ["learner", "py", "catsoop_01", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: PROJ2.1-runcell03\n", "\n", "# these are the standard weights\n", "weights=[1000*18300,\"puweight\",\"scale1fb\"]\n", "\n", "def get_weights(weights,mask,key):\n", "    # the first element of the list is the scaling weight\n", "    weight = weights[0]\n", "    # this needs to be divided by 1000 if the sample is wqq_n or zqq_n\n", "    if key=='wqq_n' or key=='zqq_n': \n", "        print('divide weight by 1000.')\n", "        weight /= 1000.\n", "    if key=='ggh': weight /= 1000. #maybe ggh too?\n", "    # now let's loop over the following weights\n", "    for i in range(1,len(weights)):\n", "        weight *= OrdDataDict[key].arrays(weights[i], library=\"np\")[weights[i]][mask]\n", "    return weight\n", "\n", "# For our samples with different collision energy (13 TeV) we need to perform a little hack on the cross section weight\n", "# so we normalize them to the number of events of the 8 TeV collision energy samples after a simple mask\n", "\n", "#This computes the integral of weighted events assuming a basic mask (see below details of this basic selection)\n", "def integral(iData,iWeights,iKey):\n", "    def selection(iData):\n", "        trigger = (iData.arrays('trigger', library=\"np\")[\"trigger\"].flatten() > 0) # trigger selection\n", "        jetpt   = (iData.arrays('vjet0_pt', library=\"np\")[\"vjet0_pt\"].flatten() > 400) # require jet pT above certain threshold\n", "        allcuts = np.logical_and.reduce([trigger,jetpt]) # apply both masks at the same time\n", "        return allcuts\n", "    mask_sel=selection(iData)\n", "    # get weights and take the integral and return it\n", "    weight = get_weights(iWeights,mask_sel,iKey)\n", "    return np.sum(weight)\n", "\n", "def scale(iData8TeV,iData13TeV,iWeights,iKey8TeV,iKey13TeV):\n", "    int_8TeV  = integral(iData8TeV,iWeights,iKey8TeV)\n", "    int_13TeV = integral(iData13TeV,iWeights,iKey13TeV)\n", "    print(\"Scale %s:\"%iKey13TeV,'ratio: ',int_8TeV/int_13TeV,' 8 TeV integral: ',int_8TeV,' 13 TeV integral: ',int_13TeV)\n", "    return int_8TeV/int_13TeV\n", "\n", "# we define this extra scaling number as:\n", "wscale=scale(wqq,wqq13,weights,'wqq','wqq13')\n", "zscale=scale(zqq,zqq13,weights,'zqq','zqq13')\n", "\n", "#w_nscale=scale(wqq,wqq_n,[18300,\"puweight\",\"scale1fb\"],'wqq','wqq_n')\n", "#z_nscale=scale(zqq,zqq_n,[18300,\"puweight\",\"scale1fb\"],'zqq','zqq_n')\n", "\n", "# Note: you could apply this weight function such as\n", "# qcd: get_weights(weights,qcd_mask,'qcd')\n", "# wqq_13: get_weights(weights,w_mask,'wqq_13')*wscale"]}, {"cell_type": "markdown", "id": "877f8300", "metadata": {"tags": ["learner", "catsoop_01", "md"]}, "source": ["Finally, we will make some quick plotting functions, which will be used in the next section. See the comments on how it works, but it should be pretty straightforward. "]}, {"cell_type": "code", "execution_count": null, "id": "24be8cb5", "metadata": {"tags": ["learner", "py", "catsoop_01", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: PROJ2.1-runcell04\n", "\n", "# define some labels and colors\n", "labels = {'qcd': 'QCD',\n", "          'wqq': 'W',\n", "          'zqq': 'Z',\n", "          'wqq13': 'W (13 to 8 TeV)',\n", "          'zqq13': 'Z (13 to 8 TeV)',\n", "          'wqq_n': 'W new',\n", "          'zqq_n': 'Z new',\n", "          'tt': 'tt',\n", "          'ggh': 'H',\n", "          'zz': 'ZZ',\n", "          'ww': 'WW',\n", "          'wz': 'WZ',\n", "          'data': 'Data',\n", "         }\n", "colors = {'qcd': 'orange',\n", "          'wqq': 'royalblue',\n", "          'zqq': 'r',\n", "          'wqq13': 'cornflowerblue',\n", "          'zqq13': 'salmon',\n", "          'wqq_n': 'lightsteelblue',\n", "          'zqq_n': 'lightcoral',\n", "          'tt': 'green',\n", "          'ggh': 'cyan',\n", "          'zz': 'purple',\n", "          'ww': 'brown',\n", "          'wz': 'crimson',\n", "          'data': 'black',\n", "         }\n", "\n", "# build a plot to compare/stack histograms\n", "def histErr(iVar,iLabel,iBins,iMin,iMax,iSims,iMasks,iData=None,iMaskData=None,\n", "            iLabels=None,iColors=None,\n", "            iDensity=True,iStack=False,iWeights=None):\n", "    \n", "    fig, ax = plt.subplots(1,1,figsize=(6,6),dpi=80)\n", "\n", "    # first plot the simulated data - build arrays\n", "    if isinstance(iSims,dict): # if iSims is a dict\n", "        simhists = [x.arrays(iVar, library=\"np\")[iVar][iMasks[key]] for key,x in iSims.items()] \n", "    else: # if it's a list\n", "        simhists = [iSims[i].arrays(iVar, library=\"np\")[iVar][iMasks[i]] for i in range(0,len(iSims))]\n", "        \n", "    # define labels\n", "    plot_labels = iLabels\n", "    if iLabels is None:\n", "        plot_labels = [labels[lk] for lk in list(iSims.keys())] #labels\n", "    plot_colors = iColors\n", "    if iColors is None:\n", "        plot_colors = [colors[lk] for lk in list(iSims.keys())] # colors\n", "    \n", "    # build the histogram weights\n", "    hist_weights = None\n", "    if iWeights:\n", "        hist_weights = [get_weights(weights,iMasks[key],key) for key in iSims.keys()]\n", "        if 'wqq13' in key:\n", "            hist_weights *= wscale\n", "        if 'zqq13' in key:\n", "            hist_weights *= zscale\n", "        \n", "    htype = 'bar'\n", "    if not iStack: htype='step'\n", "        \n", "    _,bins,_ = plt.hist(simhists,\n", "                        color=plot_colors, label=plot_labels, weights=hist_weights,\n", "                        range=(iMin,iMax), bins=iBins, alpha=.6, histtype=htype, \n", "                        density=iDensity,stacked=iStack)\n", "    \n", "    # now include the data points (if any)\n", "    if iData:\n", "        data = iData.arrays(iVar, library=\"np\")[iVar][iMaskData]\n", "        counts, binEdges = np.histogram(data,bins=iBins,range=(iMin,iMax),density=iDensity)\n", "        yerr = np.sqrt(counts) # let's apply Poisson uncertainties\n", "        if iDensity: yerr /= np.sqrt(sum(iMaskData)*(binEdges[1]-binEdges[0]))\n", "        binCenters = (binEdges[1:]+binEdges[:-1])*.5\n", "        plt.errorbar(binCenters, counts, yerr=yerr,fmt=\"o\",c=\"k\",label=\"Data\", ms=3)\n", "    \n", "    #if iDensity:\n", "    #   plt.ylim(0,0.015)\n", "    \n", "    #plt.legend(prop={'size': 10})\n", "    plt.legend(loc=1)\n", "    plt.xlabel(iLabel)\n", "    if iDensity: plt.ylabel(\"Normalized Counts\") \n", "    else: plt.ylabel(\"Counts\")\n", "    plt.show()"]}, {"cell_type": "markdown", "id": "5dab0be6", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='problems_2_1'></a>     \n", "\n", "| [Top](#section_2_0) | [Restart Section](#section_2_1) | [Next Section](#section_2_2) |\n"]}, {"cell_type": "markdown", "id": "e97fae6c", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Checkpoint 2.1.1</span>\n", "\n", "Let's consider the objectives of this project. What are we doing and why are we looking for jets? Select all options below that are relevant to understanding the objectives of this project:\n", "\n", "A) The goal of this project is to find signatures of the Higgs boson.\\\n", "B) The goal of this project is to find W and/or Z bosons that decay into quarks.\\\n", "C) Quarks leave showers of particles that we reconstruct as jets.\\\n", "D) When the momentum of a W or Z boson is high enough, quarks will decay into a single jet cone.\\\n", "E) Studying jets allows us to probe the strong interaction and investigate the properties of quarks and gluons.\\\n", "F) The study of jets helps us to search for new physics phenomena, such as the production of exotic particles or particles beyond the Standard Model."]}, {"cell_type": "markdown", "id": "f93918a6", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Checkpoint 2.1.2</span>\n", "\n", "Do all the data sets have the same keys? For instance, is there a difference between the keys in the `data` file compared to the simulation files? Explore the data (optionally complete the code below to return the difference between lists of keys).\n", "\n", "Choose the correct option:\n", "\n", "A) The data sets have the same keys.\\\n", "B) The `data` files contain the same keys as the simulation files, but also some additional information.\\\n", "C) The simulation files contain the same keys as the `data` file, but also some additional information.\\\n", "D) The data sets all contain different keys and, therefore, different types of information."]}, {"cell_type": "code", "execution_count": null, "id": "0d286dd4", "metadata": {"tags": ["py", "learner_chopped", "draft"]}, "outputs": [], "source": ["#>>>PROBLEM: PROJ2.1.2\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "#these are the datasets that we are working with\n", "#keys: ['data','qcd','tt','ww','zz','wz','wqq','wqq13','wqq_n','zqq','zqq13','zqq_n','ggh']\n", "\n", "#find difference between lists:\n", "def diff_lists(list1, list2):\n", "    return #YOUR CODE HERE\n", "\n", "print(diff_lists(data.keys(),zqq.keys()))"]}, {"cell_type": "markdown", "id": "eb467d08", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Checkpoint 2.1.3</span>\n", "\n", "Let's familiarize ourselves with the data a little more and practice extracting some features. Use `np.mean()` to find the average number of jets (`njets`) in the `wqq` dataset. Also, find the average number of b-tags (`nbtags`) detected in the `ggh` dataset.\n", "\n", "Report your answer as a list of two numbers with precision 1e-2: `[avg njets in wqq, avg nbtags in ggh]`\n"]}, {"cell_type": "code", "execution_count": null, "id": "bab2247a", "metadata": {"tags": ["py", "draft", "learner_chopped"]}, "outputs": [], "source": ["#>>>PROBLEM: PROJ2.1.3\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "print('Avg number of jets in wqq:', #YOUR CODE HERE)\n", "print('Avg number of b-tags in ggh:', #YOUR CODE HERE)"]}, {"cell_type": "markdown", "id": "2806cdc7", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Checkpoint 2.1.4</span>\n", "\n", "Let's look more closely at what the `get_weights` and `integral` functions are doing.\n", "\n", "It is important to understand that weights are how we estimate the expected number of events. For the weights that we use in this sample, we have 3 numbers:\n", "\n", "- the total luminosity of the data `18300*1000` in units of $\\mathrm{fb}^{-1}$\n", "- the weight to adjust for the beam intensity known as the pileup weight, `puweight`\n", "- the production cross section of the sample in units of fb, `scale1fb`\n", "\n", "Note that the cross section for events in samples can be different due to the way samples were produced.  To select events, we apply a mask. Effectively this is just a cut requiring a certain element of the dataset to behave a certain way. \n", "\n", "Given the information above, show that by using the `get_weights` command, we can get the same value as the `integral` function once we have applied the right mask. Complete the code below, then enter your answer as a list of two numbers with precision 1e-2: `[sum of weights, integral of weighted events]`\n"]}, {"cell_type": "code", "execution_count": null, "id": "14cb4235", "metadata": {"tags": ["draft", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>PROBLEM: PROJ2.1.4\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "#build a mask to select everything for a sample\n", "sample='qcd'\n", "test_mask      = (dataDict[sample].arrays('trigger', library=\"np\")[\"trigger\"].flatten() >= 0)\n", "test_jet       = (dataDict[sample].arrays('vjet0_pt', library=\"np\")[\"vjet0_pt\"].flatten() > 400) # require jet pT above certain threshold\n", "test_comb      = np.logical_and.reduce([test_mask,test_jet]) # apply both masks at the same time\n", "\n", "#The function get_weights() returns the events with the proper weights, after the masks are applied\n", "#print(get_weights(weights,test_comb,sample))\n", "\n", "print('Sum of weights:', #YOUR CODE HERE)\n", "print('Integrated result:', #YOUR CODE HERE) #hint: use the integral() function"]}, {"cell_type": "markdown", "id": "bb7b8b02", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_2_2'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">PROJ2.2 Event Selection and Background Mitigation</h2>    \n", "\n", "| [Top](#section_2_0) | [Previous Section](#section_2_1) | [Checkpoints](#problems_2_2) | [Next Section](#section_2_3) |\n"]}, {"cell_type": "markdown", "id": "123bd052", "metadata": {"tags": ["learner", "md", "catsoop_02"]}, "source": ["<h3>Event Selection</h3>\n", "\n", "Let's talk about to perform the selection of events. \n", "\n", "The dataflow at the LHC is complicated, but we can simplify it to the following diagram. Here I show it for ATLAS, but for CMS its basically the same. \n", "\n", "<!--<img src=\"images/atlas-data-flow.png\" width=\"600\"/>-->\n", "<p align=\"center\">\n", "<img alt=\"atlas-data-flow\" src=\"https://raw.githubusercontent.com/mitx-8s50/images/main/PROJ2/atlas-data-flow.png\" width=\"600\"/>\n", "</p>\n", "\n", ">source: https://rreece.github.io/research/<br>\n", ">attribution: (c) Ryan Reece\n", "\n", "We will work with the right part of the plot (*ntuples*). The samples that we are using are called *ntuples*, since they have *n* variables. \n", "\n", "The top part of this diagram tells us how the data initially comes from the detector. We usually call it \"trigger and DAQ\" (DAQ = Data Acquisition). The trigger look at the features of the event **in a fast way** to see if the event is interesting. If the event is interesting we keep it. If it is not, we throw it away. \n", "\n", "*Triggers* can be quite complicated because they have to process a lot of data really fast. The first layer of the trigger takes in data at a rate of 20 MHz (With 13 TeV collisions this increased to 40 MHz). This translates to about 50 terabytes/s, which is the most amount of data in any single system. To process the data quickly, we use specialized (*FPGAs*)(Field-programmable gate arrays) to look at the data quickly and determine if it is interesting. Because we can only take a cursory look at the data, we sometimes make a mistake. This means that with the final reconstructed parameters the trigger will change. To understand how the trigger works let's plot some data.\n", "\n", "In our dataset, we have saved different trigger selections that require at least one *fat jet* with *different energy or transverse momentum*. So let's explore these trigger selections."]}, {"cell_type": "code", "execution_count": null, "id": "7c513840", "metadata": {"tags": ["learner", "py", "catsoop_02", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: PROJ2.2-runcell01\n", "\n", "# Let's select some data (note that trigger can only be > 0)\n", "\n", "# First let's build masks on our data - these will be boolean arrays\n", "alldata      = (dataDict['data'].arrays('trigger', library=\"np\")[\"trigger\"].flatten() >= -1000000)\n", "triggerdata1 = (dataDict['data'].arrays('trigger', library=\"np\")[\"trigger\"].flatten() % 2 > 0) #let's require the lowest trigger jet pT > 320\n", "triggerdata2 = (dataDict['data'].arrays('trigger', library=\"np\")[\"trigger\"].flatten() % 4 > 1) #let's require one of our standard triggers (jet pT > 370 )\n", "\n", "# Now let's make a plot of the fat jet pt  \n", "# normalized\n", "histErr('vjet0_pt','Fat jet $p_T$ [GeV]',50,300,1e3,\n", "        [dataDict['data'],dataDict['data'],dataDict['data']],\n", "        [alldata,triggerdata1,triggerdata2],\n", "        iLabels=['all','$p_T$>320','$p_T$>370'],\n", "        iColors=['black','red','blue'],\n", "        iDensity=True,iStack=False,iWeights=None)\n", "\n", "# and without density\n", "histErr('vjet0_pt','Fat jet $p_T$ [GeV]',50,300,1e3,\n", "        [dataDict['data'],dataDict['data'],dataDict['data']],\n", "        [alldata,triggerdata1,triggerdata2],\n", "        iLabels=['all','$p_T$>320','$p_T$>370'],\n", "        iColors=['black','red','blue'],\n", "        iDensity=False,iStack=False,iWeights=None)\n", "\n", "#So you can see as you cut tighter, you get much less jets, but the data will be cleaner (I suggest triggerdata1)"]}, {"cell_type": "markdown", "id": "e7aac0e8", "metadata": {"tags": ["learner", "md", "catsoop_02"]}, "source": ["<h3>Mitigating background: Looking at Jet Substructure</h3>\n", "\n", "Now, we want to know how to separate our two prong signal jets from one prong background jets. There are some variables in our data that can be used to distinguish these. To make this simple, we are just going to go over the most basic ones. That way you can get a feel for how to identify two prong and one prong jets. Your challenge will be to explore how to do this. \n", "\n", "\n", "<h4>Groomed Mass</h4>\n", "\n", "Jet Grooming is a very powerful tool to clean up the resolution of the mass of jet. The idea is just like how you would groom a bush. The strategy is to take a jet and remove radiative gluons off of quarks. This spurious, soft (small energy), and wide-angle radiation can effectively broaden the mass of a jet. \n", "\n", "The way that grooming is done is by iterating down and remove clusters of quarks and gluons that have low energy and are far away from the central axes of the quark/gluon. Practially speaking this removes radiation away from the original quark and gluon direction. The details of how this works has deep physical meaning, which I will not go through here. What you should take away is that this is an iterative algorithm that is approximate, not perfect, but helps. \n", "\n", "There are many grooming algorithms. The main ones that we use are trimming, pruning, filtering, and soft drop (with various beta parameters). Typically at the LHC we use soft drop with $\\beta=0$. Let's look at how it affects our background (QCD) and our W to quarks signal. "]}, {"cell_type": "markdown", "id": "d8c4c7ab", "metadata": {"tags": ["learner", "md", "catsoop_02"]}, "source": ["<h3>Basic Selection</h3>\n", "\n", "Let's go ahead and define a basic selection of events, and plot the core variables we just discussed. "]}, {"cell_type": "code", "execution_count": null, "id": "41c7b2ea", "metadata": {"tags": ["learner", "py", "catsoop_02", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: PROJ2.2-runcell02\n", "\n", "# First let's define a quick selection (a simple pT cut of 400 GeV and a 320 GeV trigger)\n", "def selection(iData):\n", "    #lets apply a trigger selection\n", "    trigger = (iData.arrays('trigger', library=\"np\")[\"trigger\"].flatten() > 0)\n", "    #Now lets require the jet pt to be above a threshold\n", "    jetpt   = (iData.arrays('vjet0_pt', library=\"np\")[\"vjet0_pt\"].flatten() > 400)\n", "    standard_trig = (iData.arrays('trigger', library=\"np\")[\"trigger\"].flatten() % 2 > 0) #lets require one of our standard triggers (jet pT > 320 )\n", "    # standard_trig = (iData.arrays('trigger', library=\"np\")[\"trigger\"].flatten() % 4 > 1) #lets require one of our standard triggers (jet pT > 370 )\n", "    allcuts = np.logical_and.reduce([trigger,jetpt])\n", "    return allcuts\n", "\n", "#print(wqq.arrays())\n", "# Let's look at all the data files (except the 8 TeV W and Z samples - let's work with the 13 TeV ones)\n", "myDataDict = OrdDataDict.copy()\n", "del myDataDict['wqq_n']\n", "del myDataDict['zqq_n']\n", "del myDataDict['data']\n", "\n", "# Get masks for the selection defined above (both for simulated datasets and data)\n", "masks = {}\n", "for key in myDataDict: masks[key] = selection(myDataDict[key])\n", "maskData = selection(dataDict['data'])\n", "\n", "# Now let's plot the mass and the groomed mass (msd0) for the QCD background\n", "fig, ax = plt.subplots(1,1,figsize=(6,6),dpi=80)\n", "plt.title(\"QCD Background\")\n", "plt.hist(qcd.arrays('vjet0_mass', library=\"np\")[\"vjet0_mass\"][masks['qcd']],weights=get_weights(weights,masks['qcd'],'qcd'),\n", "         bins=50,range=(0,300), color='salmon',label=\"groomed mass\", alpha=.6)\n", "plt.hist(qcd.arrays('vjet0_msd0', library=\"np\")[\"vjet0_msd0\"][masks['qcd']], weights=get_weights(weights,masks['qcd'],'qcd'),\n", "         bins=50,range=(0,300), color='red',label=\"mass\", alpha=.6)\n", "plt.legend()\n", "plt.xlabel(\"QCD Jet mass [GeV]\")\n", "plt.ylabel(\"Counts\")\n", "plt.show()\n", "\n", "# Let's look at the W/Z samples now (8 TeV collision energy)\n", "fig, ax = plt.subplots(1,1,figsize=(6,6),dpi=80)\n", "plt.title(\"8 TeV Collision Energy\")\n", "plt.hist(myDataDict['wqq'].arrays('vjet0_mass', library=\"np\")[\"vjet0_mass\"][masks['wqq']],weights=get_weights(weights,masks['wqq'],'wqq'),\n", "         bins=50,range=(0,300), color='salmon',label=\"W mass\", alpha=.6)\n", "plt.hist(myDataDict['wqq'].arrays('vjet0_msd0', library=\"np\")[\"vjet0_msd0\"][masks['wqq']], weights=get_weights(weights,masks['wqq'],'wqq'),\n", "         bins=50,range=(0,300), color='red',label=\"W groomed mass\", alpha=.6)\n", "plt.hist(myDataDict['zqq'].arrays('vjet0_mass', library=\"np\")[\"vjet0_mass\"][masks['zqq']],weights=get_weights(weights,masks['zqq'],'zqq'),\n", "         bins=50,range=(0,300), color='pink',label=\"Z mass\", alpha=.6)\n", "plt.hist(myDataDict['zqq'].arrays('vjet0_msd0', library=\"np\")[\"vjet0_msd0\"][masks['zqq']], weights=get_weights(weights,masks['zqq'],'zqq'),\n", "         bins=50,range=(0,300), color='hotpink',label=\"Z groomed mass\", alpha=.6)\n", "plt.legend()\n", "plt.xlabel(\"Signal Jet mass [GeV]\")\n", "plt.ylabel(\"Counts\")\n", "plt.show()\n", "\n", "# Let's look at the W/Z samples now (13 TeV collision energy)\n", "# Note that in the weights we need to multiply by wscale\n", "fig, ax = plt.subplots(1,1,figsize=(6,6),dpi=80)\n", "plt.title(\"13 TeV Collision Energy\")\n", "plt.hist(myDataDict['wqq13'].arrays('vjet0_mass', library=\"np\")[\"vjet0_mass\"][masks['wqq13']],weights=get_weights(weights,masks['wqq13'],'wqq13')*wscale,\n", "         bins=50,range=(0,300), color='salmon',label=\"W  mass\", alpha=.6)\n", "plt.hist(myDataDict['wqq13'].arrays('vjet0_msd0', library=\"np\")[\"vjet0_msd0\"][masks['wqq13']], weights=get_weights(weights,masks['wqq13'],'wqq13')*wscale,\n", "         bins=50,range=(0,300), color='red',label=\"W groomed mass\", alpha=.6)\n", "plt.hist(myDataDict['zqq13'].arrays('vjet0_mass', library=\"np\")[\"vjet0_mass\"][masks['zqq13']],weights=get_weights(weights,masks['zqq13'],'zqq13')*wscale,\n", "         bins=50,range=(0,300), color='pink',label=\"Z mass\", alpha=.6)\n", "plt.hist(myDataDict['zqq13'].arrays('vjet0_msd0', library=\"np\")[\"vjet0_msd0\"][masks['zqq13']], weights=get_weights(weights,masks['zqq13'],'zqq13')*wscale,\n", "         bins=50,range=(0,300), color='hotpink',label=\"Z groomed mass\", alpha=.6)\n", "plt.legend()\n", "plt.xlabel(\"Signal Jet mass [GeV]\")\n", "plt.ylabel(\"Counts\")\n", "plt.show()"]}, {"cell_type": "markdown", "id": "c307c623", "metadata": {"tags": ["learner", "md", "catsoop_02"]}, "source": ["What you observe is that the mass for our qcd background goes down to much lower values, and the mass for W boson gets more narrow and approaches the mass of the W boson (80.4 GeV).  This is a great way to reduce the background and improve the sensitivity of the signal. "]}, {"cell_type": "markdown", "id": "e0f7a2a7", "metadata": {"tags": ["learner", "md", "catsoop_02"]}, "source": ["<h4>N-Subjettiness</h4>\n", "\n", "Now lets look at another class of variables. These variables are the n-subjettiness variables. These variables were developed at MIT by Prof. Thaler and a UROP. The original paper is <a href=\"https://arxiv.org/abs/1011.2268\" target=\"_blank\">here</a>. Each of these variables compute the likelihood of a certain number of prongs, or the likelihood that a certain number of sub-jets exist in the shower. \n", "\n", "We write these variables as $\\tau_{i}$, with $\\tau_{1}$ being the likelihood for 1 pronged jet, $\\tau_{2}$ a two pronged and so on. To test these variables we use the ratios as a way to measure the likelihood of $N$ prongs vs $M$ prongs. To look for W and Z bosons, we look for the ratio of 2 prongs with respect to one. Hence, we consider the variable $\\tau_{2}/\\tau_{1}$. \n", "\n", "Let's now look at how this variable behaves between our signal simulation and our background. "]}, {"cell_type": "code", "execution_count": null, "id": "e506f4b7", "metadata": {"tags": ["learner", "py", "catsoop_02", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: PROJ2.2-runcell03\n", "\n", "# Compute the t21 ratio\n", "# let's use the same selection set above\n", "# note that here we are going to use our 13 TeV signal samples\n", "#print(len(masks[\"qcd\"]))\n", "\n", "fig, ax = plt.subplots(1,1,figsize=(6,6),dpi=80)\n", "qcdt21 = (qcd.arrays('vjet0_t2', library=\"np\")[\"vjet0_t2\"][masks['qcd']]/\n", "          qcd.arrays('vjet0_t1', library=\"np\")[\"vjet0_t1\"][masks['qcd']])\n", "wt21 = (wqq13.arrays('vjet0_t2', library=\"np\")[\"vjet0_t2\"][masks['wqq13']]/\n", "          wqq13.arrays('vjet0_t1', library=\"np\")[\"vjet0_t1\"][masks['wqq13']])\n", "\n", "plt.hist(qcdt21, weights=get_weights(weights,masks['qcd'],'qcd'),\n", "         bins=50, color='red',label=\"QCD\", alpha=.6, density=True)\n", "plt.hist(wt21, weights=get_weights(weights,masks['wqq13'],'wqq13')*wscale,\n", "         bins=50, color='black',label=\"W\", alpha=.6, density=True)\n", "plt.legend()\n", "plt.xlabel(r\"$\\tau_{21}$\")\n", "plt.ylabel(\"Normalized Counts\")\n", "plt.show()"]}, {"cell_type": "markdown", "id": "ae5a91bd", "metadata": {"tags": ["learner", "md", "catsoop_02"]}, "source": ["What you can see is that our two pronged signal has a lower $\\tau_{2}/\\tau_{1}$, so the chance of the background is low. So by requiring $\\tau_{2}/\\tau_{1} < X$ we can isolate two pronged signals over the QCD background. "]}, {"cell_type": "markdown", "id": "53e2374c", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='problems_2_2'></a>     \n", "\n", "| [Top](#section_2_0) | [Restart Section](#section_2_2) | [Next Section](#section_2_3) |\n"]}, {"cell_type": "markdown", "id": "a64cd39a", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Checkpoint 2.2.1</span>\n", "\n", "Let's understand what the trigger is doing in code cell `PROJ2.2-runcell01`. We have defined two critical triggers that we care about. The first is whether an event has a transverse momentum pT > 320 GeV, and the second is whether an event has a transverse momentum of pT > 370 GeV.\n", "\n", "To characterize the trigger, the first bit is 1 if pT > 320 GeV and 0 if it's not. The second bit is 1 if pT > 370 GeV and 0 otherwise. We can write the bit value as: `trigger = 2*(pT > 370) + (pT > 320)`. Consider the following possible scenarios:\n", "\n", "- if we have an event with pT < 320, the value of triggger is 0\n", "- if we have an event with pT > 320 but less than 370, the value of the trigger is 1\n", "- if we have an event with pT > 370, the value of trigger is 3\n", "\n", "These are the only possible values of trigger. So, we can define the criteria for selecting events with pT > 320 GeV as `trigger % 2 > 0` (i.e., trigger mod 2 = 1).\n", "\n", "What is the criteria for selecting events with pT > 370? Complete the code below."]}, {"cell_type": "code", "execution_count": null, "id": "25983ffa", "metadata": {"tags": ["draft", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>PROBLEM: PROJ2.2.1\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "#this function is defined for you\n", "def pass_320(trigger):\n", "    #return 1 for events with pT > 320\n", "    if trigger % 2 > 0:\n", "        return 1\n", "    else:\n", "        return 0\n", "    \n", "#this function you must complete\n", "def pass_370(trigger):\n", "    #return 1 for events with pT > 370\n", "    if #YOUR CODE HERE:\n", "        return 1\n", "    else:\n", "        return 0"]}, {"cell_type": "markdown", "id": "622efd46", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Checkpoint 2.2.2</span>\n", "\n", "In this project, we are using simulated data for both 8 TeV and 13 TeV energies. It turns out that our data at 8 TeV provides the most accurate predictions, however, the 13 TeV distributions appear much smoother in the plots because they have more events. Smooth shapes, particularly of invariant quanities like mass, make it it easier to plot and interpolate.\n", "\n", "How can we effectively use both simulation data sets in our analysis?\n", "\n", "A) We can use the shape of the 13 TeV distributions, but scale the normalization to 8TeV distributions. This is an approximation, but it  gets the best features of both.\\\n", "B) We can't use 13 TeV at all, just 8 TeV for 8TeV data\\\n", "C) We can separately analyze the 8 TeV and 13 TeV datasets and compare the obtained results.\\\n"]}, {"cell_type": "markdown", "id": "763e9f37", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Checkpoint 2.2.3</span>\n", "\n", "Below what value of `t2/t1` are W events dominant? Enter your answer as number with precision 1e-1."]}, {"cell_type": "markdown", "id": "2c8023d4", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Checkpoint 2.2.4</span>\n", "\n", "Which of the following statements describes what will happen if we change our cut by increasing the `t2/t1` threshold? Select all that apply.\n", "\n", "A) We will get more W events compared to background.\\\n", "B) We will be able to better distinguish W signal from background.\\\n", "C) We will no longer be able to distinguish W signal from background.\\\n", "D) Nothing because the signal is independent of `t2/t1`.\n"]}, {"cell_type": "markdown", "id": "3c4d118d", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_2_3'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">PROJ2.3 Beginning to Look for the W Signal in the Data</h2>    \n", "\n", "| [Top](#section_2_0) | [Previous Section](#section_2_2) | [Checkpoints](#problems_2_3) | [Next Section](#section_2_4) |\n"]}, {"cell_type": "markdown", "id": "e4d6ad12", "metadata": {"tags": ["learner", "catsoop_03", "md"]}, "source": ["<h3>Overview</h3>\n", "\n", "Now, lets try to find the $W\\rightarrow qq$ and $Z\\rightarrow qq$ peak in the **data.** This is a difficult problem and you will have to use the above ideas plus a few others. To give you a hint you should read <a href=\"https://arxiv.org/abs/1603.00027\" target=\"_blank\">this paper</a>. Also, you should consider all of the other physics papers based on this strategy. That includes our <a href=\"https://arxiv.org/abs/1705.10532\" target=\"_blank\">original paper</a> and two follow-up papers <a href=\"https://arxiv.org/abs/1710.00159\" target=\"_blank\">here</a> and  <a href=\"https://arxiv.org/abs/1909.04114\" target=\"_blank\">here</a>. These later papers use more technology developed along the same lines, but the original paper should have all you need to get a resonance. \n", "\n", "To put it all together, we want to make a data vs simulation  plot. For this will take all of our simulations and add them together. Let's make a simple plotting example. We will plot this in two ways. First, we will just show how the normalized distributions look like so we can compare the shapes. Secondly, we will make the stacked histogram plot, so we can see how the data compares to our prediction. \n"]}, {"cell_type": "code", "execution_count": null, "id": "230af238", "metadata": {"tags": ["learner", "py", "catsoop_03", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: PROJ2.3-runcell01\n", "\n", "try:\n", "    del myDataDict['wqq'] #let's omit 8 TeV samples from here\n", "    del myDataDict['zqq']\n", "except:\n", "    print('samples already deleted')\n", "    \n", "# let's compare shapes\n", "histErr('vjet0_msd0','Fat jet $m_{SD}$ [GeV]',50,40,200,\n", "        myDataDict,masks,\n", "        dataDict['data'],maskData,\n", "        iDensity=True,iStack=False,iWeights=True)\n", "\n", "# Let's do a stacked plot  of all simulation and data\n", "histErr('vjet0_msd0','Fat jet $m_{SD}$ [GeV]',50,40,200,\n", "        myDataDict,masks,\n", "        dataDict['data'],maskData,\n", "        iDensity=False,iStack=True,iWeights=True)"]}, {"cell_type": "markdown", "id": "1219215a", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Checkpoint 2.3.1</span>\n", "\n", "When we look at the above distributions, we see that the W, Z and other channels yield resonant bumps at various masses. However, in the bottom plot, we don't see these bumps in the data or MC simulation. Why do we not see them? \n", "\n", "A) The bumps are not visible in the bottom plot because the detector effects smear out the resonant structures.\\\n", "B) The bumps are not observed in the data or MC simulation in the bottom plot due to limitations in the modeling of certain physical processes.\\\n", "C) The bumps seen in the W, Z, and other channels might be due to statistical fluctuations or specific experimental conditions, which are not replicated in the bottom plot.\\\n", "D) The bumps are there in the plot on the bottom, but the QCD background is just so much larger than the W, Z samples and others that we just can't see them."]}, {"cell_type": "markdown", "id": "05e720b6", "metadata": {"tags": ["learner", "md", "catsoop_03"]}, "source": ["<h3>Starting Our Selection Campaign</h3>\n", "\n", "Lets start fresh by defining our samples, selection, and all the tools that we have above. This is just a refresh, so we can start fresh and begin to perform our selection, below."]}, {"cell_type": "code", "execution_count": null, "id": "615a6b58", "metadata": {"tags": ["learner", "py", "learner_chopped", "catsoop_03"]}, "outputs": [], "source": ["#>>>RUN: PROJ2.3-runcell02\n", "\n", "#Load the data, if you have not done so in Section 1\n", "\n", "wqq    = uproot.open(\"data/WQQ_s.root\")[\"Tree\"]\n", "zqq    = uproot.open(\"data/ZQQ_s.root\")[\"Tree\"]\n", "wqq13  = uproot.open(\"data/skimh/WQQ_sh.root\")[\"Tree\"]\n", "zqq13  = uproot.open(\"data/skimh/ZQQ_sh.root\")[\"Tree\"]\n", "wqq_n  = uproot.open(\"data/WQQ_8TeV_Jan11_r.root\")[\"Tree\"]\n", "zqq_n  = uproot.open(\"data/ZQQ_8TeV_Jan11_r.root\")[\"Tree\"]\n", "qcd    = uproot.open(\"data/QCD_s.root\")[\"Tree\"]\n", "tt     = uproot.open(\"data/TT.root\")[\"Tree\"]\n", "ww     = uproot.open(\"data/WW.root\")[\"Tree\"]\n", "wz     = uproot.open(\"data/WZ.root\")[\"Tree\"]\n", "zz     = uproot.open(\"data/ZZ.root\")[\"Tree\"]\n", "ggh    = uproot.open(\"data/ggH.root\")[\"Tree\"]\n", "data   = uproot.open(\"data/JetHT_s.root\")[\"Tree\"]"]}, {"cell_type": "markdown", "id": "f7f7473c", "metadata": {"tags": ["learner", "md", "catsoop_03"]}, "source": ["After loading the data, you are provided with some simple helper functions that have already been used in earlier sections (perhaps slightly differently). These are used for pre-selection (standard cuts that physicsists usually apply before making measurements) and computing the scaling factor of datasets."]}, {"cell_type": "code", "execution_count": null, "id": "c2319e9a", "metadata": {"tags": ["learner", "py", "learner_chopped", "catsoop_03"]}, "outputs": [], "source": ["#>>>RUN: PROJ2.3-runcell03\n", "\n", "def selection(iData):\n", "    '''\n", "    Standard pre-selection\n", "    '''\n", "    #lets apply a trigger selection\n", "    trigger = (iData.arrays('trigger', library=\"np\")[\"trigger\"].flatten() > 0)\n", "\n", "    #Now lets require the jet pt to be above a threshold (400 TODO: ASK about units)\n", "    jetpt   = (iData.arrays('vjet0_pt', library=\"np\")[\"vjet0_pt\"].flatten() > 400)\n", "\n", "    #Lets apply both jetpt and trigger at the same time\n", "    #standard_trig = (iData.arrays('trigger', library=\"np\")[\"trigger\"].flatten() % 4 > 1) #lets require one of our standard triggers (jet pT > 370 )\n", "    allcuts = np.logical_and.reduce([trigger,jetpt])\n", "\n", "    return allcuts\n", "    \n", "def get_weights(iData,weights,sel):\n", "    \n", "    weight = weights[0]\n", "    \n", "    for i in range(1,len(weights)):\n", "        weight *= iData.arrays(weights[i],library=\"np\")[weights[i]][sel]\n", "        \n", "    return weight\n", "\n", "def integral(iData,iWeights):\n", "    '''\n", "    This computs the integral of weighted events \n", "    assuming a selection given by the function selection (see below)\n", "    '''\n", "    \n", "    #perform a selection on the data (\n", "    mask_sel=selection(iData)\n", "    \n", "    #now iterate over the weights not the weights are in the format of [number,variable name 1, variable name 2,...]\n", "    weight  =iWeights[0]\n", "    \n", "    for i0 in range(1,len(iWeights)):\n", "        weightarr = iData.arrays(iWeights[i0], library=\"np\")[iWeights[i0]][mask_sel].flatten()\n", "        \n", "        #multiply the weights\n", "        weight    = weight*weightarr\n", "    \n", "    #now take the integral and return it\n", "    return np.sum(weight)\n", "\n", "\n", "def scale(iData8TeV,iData13TeV,iWeights):\n", "    '''\n", "    This computes the integral of two selections for two datasets labelled 8TeV and 13TeV,\n", "    but really can be 1 and 2. Then it returns the ratio of the integrals\n", "    '''\n", "    \n", "    int_8TeV  = integral(iData8TeV,iWeights)\n", "    int_13TeV = integral(iData13TeV,iWeights)\n", "    \n", "    return int_8TeV/int_13TeV"]}, {"cell_type": "markdown", "id": "91cabbd6", "metadata": {"tags": ["learner", "md", "catsoop_03"]}, "source": ["<h3>Find the W Peak</h3>\n", "\n", "Now, let's define a new function to produce a similar plot that was shown earlier (`PROJ2.3-runcell01`). We will make a plot of the data vs. jet mass and $\\tau_2$, first without any cuts. "]}, {"cell_type": "code", "execution_count": null, "id": "751587d0", "metadata": {"tags": ["learner", "py", "learner_chopped", "catsoop_03"]}, "outputs": [], "source": ["#>>>RUN: PROJ2.3-runcell04\n", "\n", "def plotDataSim(iVar, iSelection, iVarName, iRange):\n", "    \n", "    #Lets Look at the mass\n", "    weights = [1000*18300, \"puweight\", \"scale1fb\"]\n", "    mrange = iRange #range for mass histogram [GeV]\n", "    bins=40            #bins for mass histogram\n", "    density = False     #to plot the histograms as a density (integral=1)\n", "\n", "    qcdsel      = iSelection(qcd)\n", "    wsel        = iSelection(wqq13)\n", "    zsel        = iSelection(zqq13)\n", "    datasel     = iSelection(data)\n", "    ttsel       = iSelection(tt)\n", "    wwsel       = iSelection(ww)\n", "    wzsel       = iSelection(wz)\n", "    zzsel       = iSelection(zz)\n", "    gghsel      = iSelection(ggh)\n", "\n", "    wscale=scale(wqq,wqq13,weights)\n", "    zscale=scale(zqq,zqq13,weights)\n", "\n", "    # Getting the masses of selected events\n", "    dataW = data.arrays(iVar, library=\"np\") [iVar][datasel]\n", "    qcdW  = qcd.arrays(iVar, library=\"np\")  [iVar][qcdsel]\n", "    wW    = wqq13.arrays(iVar, library=\"np\")[iVar][wsel]\n", "    zW    = zqq13.arrays(iVar, library=\"np\")[iVar][zsel]\n", "    zzW   = zz   .arrays(iVar, library=\"np\")[iVar][zzsel]\n", "    wzW   = wz   .arrays(iVar, library=\"np\")[iVar][wzsel]\n", "    wwW   = ww   .arrays(iVar, library=\"np\")[iVar][wwsel]\n", "    ttW   = tt   .arrays(iVar, library=\"np\")[iVar][ttsel]\n", "    gghW  = ggh  .arrays(iVar, library=\"np\")[iVar][gghsel]\n", "\n", "    #Define the weights for the histograms\n", "    hist_weights = [get_weights(qcd,weights,qcdsel),\n", "                    get_weights(wqq13,weights,wsel)*wscale,\n", "                    get_weights(zqq13,weights,zsel)*zscale,\n", "                    get_weights(zz,weights,zzsel),\n", "                    get_weights(wz,weights,wzsel),\n", "                    get_weights(ww,weights,wwsel),\n", "                    get_weights(tt,weights,ttsel),\n", "                   ]\n", "\n", "    #Hint: Provide a list of selected data\n", "    plt.hist([qcdW,wW, zW, zzW, wzW, wwW, ttW],\n", "             color=[\"royalblue\",\"r\", \"orange\",\"g\", \"b\", \"purple\", \"cyan\",], \n", "             label=[\"QCD\", \"W\", \"Z\", \"ZZ\", \"WZ\", \"WW\", \"tt\",], \n", "             weights=hist_weights,\n", "             range=mrange, bins=50, alpha=.6, density=density,stacked=True)\n", "\n", "    #Other configurations for the histogram\n", "    counts, bins = np.histogram(dataW, bins=bins, range=mrange, density=density)\n", "    yerr = np.sqrt(counts) / np.sqrt(len(dataW)*np.diff(bins))\n", "    binCenters = (bins[1:]+bins[:-1])*.5\n", "    plt.errorbar(binCenters, counts, yerr=yerr,fmt=\"o\",c=\"k\",label=\"data\")\n", "    plt.legend()\n", "    plt.xlabel(iVarName)\n", "    plt.ylabel(\"Counts\")\n", "    plt.show()\n", "\n", "plotDataSim(\"vjet0_msd0\", selection, \"Jet Mass\", [40,200])\n", "plotDataSim(\"vjet0_t2\", selection, r\"$\\tau_2$\", [0,0.5]) \n", "#Add some code here to compare variables"]}, {"cell_type": "markdown", "id": "d787b97d", "metadata": {"tags": ["learner", "md", "catsoop_03"]}, "source": ["<h3>The first cut</h3>\n", "\n", "Now we want to plot the jet mass, first making a cut on the value of $\\tau_2/\\tau_1$ that best discriminates background from signal. Refer to the plot from code cell `PROJ2.2-runcell03` and the results from `Checkpoint 2.2.3`.\n", "\n", "**Complete the code below by entering a value for the `t21` threshold. The examine the plot.**"]}, {"cell_type": "code", "execution_count": null, "id": "a2756213", "metadata": {"tags": ["learner", "py", "learner_chopped", "catsoop_03"]}, "outputs": [], "source": ["#>>>RUN: PROJ2.3-runcell06\n", "\n", "#Define tau2/tau1\n", "def t21_func(itau1,itau2):\n", "    return itau2/itau1\n", "\n", "\n", "def selectionW_firstcut(iData):\n", "    '''\n", "    This is the specific selection for selecting out events with W signal for our analysis\n", "    '''\n", "    \n", "    #Pre-selection citeria\n", "    trigger = (iData.arrays('trigger', library=\"np\")[\"trigger\"].flatten() >= 0)\n", "    jetpt   = (iData.arrays('vjet0_pt', library=\"np\")[\"vjet0_pt\"].flatten() >= 400)\n", "    \n", "    #Select the jets to compute tau2/tau1\n", "    jett2   = (iData.arrays('vjet0_t2', library=\"np\")[\"vjet0_t2\"].flatten())\n", "    jett1   = (iData.arrays('vjet0_t1', library=\"np\")[\"vjet0_t1\"].flatten())\n", "        \n", "    t21 = t21_func(jett1,jett2)\n", "                                \n", "    #And then perform the cut\n", "    #Hint: You could determine the threshold of the cut by plotting the distribution of \n", "    #t21ddt scores for W and background and then determine a ball park threshold\n", "    #where you think the W signal would be best selected\n", "    #Or more simply you could look at the given plot and determine the appropriate threshold.\n", "    \n", "    t21cut   = t21 < #YOUR CODE HERE (enter t21 threshold)\n", "    \n", "    allcuts = np.logical_and.reduce([trigger, jetpt, t21cut])\n", "    \n", "    return allcuts\n", "\n", "plotDataSim(\"vjet0_msd0\", selectionW_firstcut, \"Jet Mass\",[40,200])"]}, {"cell_type": "markdown", "id": "d239bc6d", "metadata": {"tags": ["learner", "md", "catsoop_03"]}, "source": ["Looking at this plot, we can see that the W peak is not at all obvious to find! This is why we need to employ additional techniques in order to clearly identify the W peak, which is what you'll have the chance to do in the next section!"]}, {"cell_type": "markdown", "id": "a7f2fe5a", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='problems_2_3'></a>   \n", "\n", "| [Top](#section_2_0) | [Restart Section](#section_2_3) |\n"]}, {"cell_type": "markdown", "id": "a409dd23", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Checkpoint 2.3.2</span>\n", "\n", "In the above plot, there is the simulation prediction (colored histograms), and then the data (black points). You can see the data has a shape that looks like there are two bumps on each other. However, the bumps are merged and peak roughly at the same spot. Why can we not just use the simulation to extract the W and Z bumps? Select all that apply:\n", "\n", "A) We can! The best way to analyze the properties of a signal is to see where is matches simulation exactly.\\\n", "B) We cannot because our simulations use assumptions that would bias our measurement. This is effectively a kind of circular analysis.\\\n", "C) While the simulation provides a useful reference, it is essential to account for potential discrepancies between the simulation and data due to uncertainties in the theoretical models, calibration of the detectors, or unknown physics phenomena. \n"]}, {"cell_type": "markdown", "id": "4a1cbf4c", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_2_4'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">PROJ2.4 Refining our Selection to Look for the W Signal in the Data</h2>   \n", "\n", "| [Top](#section_2_0) | [Previous Section](#section_2_3) | [Checkpoints](#problems_2_4) | [Next Section](#section_2_5) |\n"]}, {"cell_type": "markdown", "id": "2a51d72d", "metadata": {"tags": ["learner", "md", "catsoop_04"]}, "source": ["<h3>Overview of W Signal</h3>\n", "\n", "Now, lets do the lab. **Your first challenge is to make a mass plot and perform fitting for W signal.**\n", "\n", "A hint is that the plots should come out similarly to this (it doesn't have to be exactly the same), where on the left we show the soft-drop mass/groomed mass ($m_{SD}$) distribution for different processes in the Monte Carlo simulation along with the real data. The plot on the right shows the fit:\n", "\n", "<!--<img src=\"images/S50_WFit.png\" width='900'>-->\n", "<p align=\"center\">\n", "<img alt=\"Fit for W Peak\" src=\"https://raw.githubusercontent.com/mitx-8s50/images/main/PROJ2/S50_WFit.png\" width=\"900\"/>\n", "</p>\n"]}, {"cell_type": "markdown", "id": "a504fb6f", "metadata": {"tags": ["learner", "md", "learner_chopped", "catsoop_04"]}, "source": ["<h3>Objective 1: Develop a procedure to select data and make a W boson mass plot</h3>\n", "\n", "Since finding W peak is hard, we need to use another parameter, $\\rho$, which is a scaling variable for QCD jets. This parameter adds another channels of mass and $p_T$ to our selection, helping us to refine our W peak. The parameter $\\rho$ is defined in <a href=\"https://arxiv.org/pdf/1603.00027.pdf\" target=\"_blank\">this paper.</a>\n", "\n", "Your first goal is to figure out how $\\rho$ is defined by quoting the paper, and then figure out the best selections based on a combination of $\\rho$ and $\\tau_2/\\tau_1$. The final cut is based on a parameter defined as *DT* (Deccorelated Taggers) score:\n", "\n", "$$(\\tau_2/\\tau_1)_{dt} = \\tau_2/\\tau_1 - (\\text{your correlation})*\\rho$$\n", "\n", "Where the correlation `(your correlation)` is the correlation coefficient between $\\tau_2/\\tau_1$ and $\\rho$. \n", "\n", "To figure out the correlation, let's plot $\\tau_2/\\tau_1$ and $\\rho$ in the data first!"]}, {"cell_type": "markdown", "id": "72df7b03", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Checkpoint 2.4.1</span>\n", "\n", "Complete the code below to plot $\\tau_2/\\tau_1$ vs. $\\rho$. Specifically, we will check the function `rho_func` in the answer-checker, then you should use your result within the funciton `plot_taus_and_rho` to create a plot."]}, {"cell_type": "code", "execution_count": null, "id": "844184eb", "metadata": {"tags": ["py", "learner_chopped", "draft"]}, "outputs": [], "source": ["#>>>PROBLEM: PROJ2.4.1\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "def rho_func(imass,ipt,mu=1.):\n", "    return #YOUR CODE HERE\n", "    \n", "\n", "def plot_taus_and_rho(iData):\n", "    \n", "    jetptnocut = (iData.arrays('vjet0_pt', library=\"np\")[\"vjet0_pt\"].flatten())\n", "    jetmass = (iData.arrays('vjet0_msd0', library=\"np\")[\"vjet0_msd0\"].flatten())\n", "    jett2   = (iData.arrays('vjet0_t2', library=\"np\")[\"vjet0_t2\"].flatten())\n", "    jett1   = (iData.arrays('vjet0_t1', library=\"np\")[\"vjet0_t1\"].flatten())\n", "    \n", "    #Define rho according to the paper (eqn. 3.2)\n", "    #Really this is rho_prime as defined in the paper, with mu=1 in these units\n", "    rho = #YOUR CODE HERE\n", "    \n", "    #Define tau2/tau1\n", "    t21 = t21_func(jett1,jett2)\n", "    \n", "    plt.hist2d(rho, t21, bins = 40)\n", "    \n", "    plt.xlabel(r\"$\\rho$\")\n", "    plt.ylabel(r\"$\\tau_2/\\tau_1$\")\n", "    plt.show()\n", "    \n", "plot_taus_and_rho(qcd)"]}, {"cell_type": "markdown", "id": "510ce34b", "metadata": {"tags": ["learner", "md", "catsoop_04"]}, "source": ["Great! Now we can fit a line on the 2D histogram to determine the correlation! Here we give you the fitting code. The codes fit by putting a threshold on the 2D histogram to selectively fit on the most relevant data points, your task for this is to play around with the threshold to determine the best fit!"]}, {"cell_type": "code", "execution_count": null, "id": "1807e3dd", "metadata": {"tags": ["learner", "py", "learner_chopped", "catsoop_04"]}, "outputs": [], "source": ["#>>>RUN: PROJ2.4-runcell02\n", "\n", "def fit_correlation(iData):\n", "    \n", "    jetptnocut = (iData.arrays('vjet0_pt', library=\"np\")[\"vjet0_pt\"].flatten())\n", "    jetmass = (iData.arrays('vjet0_msd0', library=\"np\")[\"vjet0_msd0\"].flatten())\n", "    jett2   = (iData.arrays('vjet0_t2', library=\"np\")[\"vjet0_t2\"].flatten())\n", "    jett1   = (iData.arrays('vjet0_t1', library=\"np\")[\"vjet0_t1\"].flatten())\n", "    \n", "    #Define rho according to the paper\n", "    rho = rho_func(jetmass,jetptnocut)\n", "    \n", "    #Define tau2/tau1\n", "    t21 = t21_func(jett1,jett2)\n", "    \n", "    plt.hist2d(rho, t21, bins = 40)\n", "    plt.xlabel(r\"$\\rho$\")\n", "    plt.ylabel(r\"$\\tau_2/\\tau_1$\")\n", "    \n", "    #Fit the line\n", "    #Produce 2D histogram\n", "    H,xedges,yedges = np.histogram2d(rho,t21, bins=40,density = True)\n", "    \n", "    bin_centers_x = (xedges[:-1]+xedges[1:])/2.0\n", "    bin_centers_y = (yedges[:-1]+yedges[1:])/2.0\n", "    \n", "    #Find the non-zero indicies\n", "    non_zero_idx = np.argwhere(H > 0.6) #You can play around with this!\n", "    x_idx = non_zero_idx[:,0]\n", "    y_idx = non_zero_idx[:,1]\n", "    \n", "    x_coord = [bin_centers_x[x_idx[i]] for i in range(0,len(x_idx))]\n", "    y_coord = [bin_centers_y[y_idx[i]] for i in range(0,len(y_idx))]\n", "    \n", "    #Fit a linear model on the points plotted\n", "    def func(x, a, b):\n", "        return a * x + b\n", "    plt.scatter(x_coord, y_coord)\n", "    \n", "    popt, pcov = curve_fit(func, x_coord, y_coord)\n", "    plt.plot(bin_centers_x, func(bin_centers_x, *popt), 'r-',\n", "             label='fit: a=%5.3f, b=%5.3f' % tuple(popt))\n", "    \n", "    #Show the fit result\n", "    legend = plt.legend()\n", "    plt.setp(legend.get_texts(), color='w')\n", "    plt.show()\n", "\n", "from scipy.optimize import curve_fit\n", "fit_correlation(qcd)"]}, {"cell_type": "markdown", "id": "b1ae5470", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Checkpoint 2.4.2</span>\n", "\n", "Now use the correlation from the plot to define $(\\tau_2/\\tau_1)_{dt}$ and plot it with $\\rho$ to verify that we have successfully decorrelated the tagger. If you do it correctly, you can see that the decorrelated scores are now independent of $\\rho$ (you should see a straight-line distribution in the histogram)!\n", "\n", "Specifically, complete the function `t21ddt_func`, which should decorrelate the `t21` value as a function of `rho`. Set the default value of `iMcorr` based on your fit above."]}, {"cell_type": "code", "execution_count": null, "id": "743daa54", "metadata": {"tags": ["py", "learner_chopped", "draft"]}, "outputs": [], "source": ["#>>>PROBLEM: PROJ2.4.2\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "def t21ddt_func(it21,irho,iMcorr=#YOUR CODE HERE):\n", "    #iMcorr is the correlation coefficient\n", "    return #YOUR CODE HERE\n", "\n", "\n", "def plot_tausdt_and_rho(iData):\n", "    \n", "    jetptnocut = (iData.arrays('vjet0_pt', library=\"np\")[\"vjet0_pt\"].flatten())\n", "    jetmass = (iData.arrays('vjet0_msd0', library=\"np\")[\"vjet0_msd0\"].flatten())\n", "    jett2   = (iData.arrays('vjet0_t2', library=\"np\")[\"vjet0_t2\"].flatten())\n", "    jett1   = (iData.arrays('vjet0_t1', library=\"np\")[\"vjet0_t1\"].flatten())\n", "    \n", "    #Define rho according to the paper\n", "    rho = rho_func(jetmass,jetptnocut)\n", "    \n", "    #Define tau2/tau1\n", "    t21 = t21_func(jett1,jett2)\n", "    \n", "    #decorrelated tagger score\n", "    t21ddt = t21ddt_func(t21,rho)\n", "    \n", "    plt.hist2d(rho, t21ddt, bins = 40)\n", "    \n", "    plt.xlabel(r\"$\\rho$\")\n", "    plt.ylabel(r\"$\\tau_2/\\tau_1$_dt\")\n", "                \n", "    #Fit the line\n", "    #Produce 2D histogram\n", "    H,xedges,yedges = np.histogram2d(rho,t21ddt, bins=40,density = True)\n", "    \n", "    bin_centers_x = (xedges[:-1]+xedges[1:])/2.0\n", "    bin_centers_y = (yedges[:-1]+yedges[1:])/2.0\n", "    \n", "    #Find the non-zero indicies\n", "    non_zero_idx = np.argwhere(H > 0.6) #You can play around with this!\n", "    x_idx = non_zero_idx[:,0]\n", "    y_idx = non_zero_idx[:,1]\n", "    x_coord = [bin_centers_x[x_idx[i]] for i in range(0,len(x_idx))]\n", "    y_coord = [bin_centers_y[y_idx[i]] for i in range(0,len(y_idx))]\n", "    \n", "    #Fit a linear model on the points plotted\n", "    def func(x, a, b):\n", "        return a * x + b\n", "    plt.scatter(x_coord, y_coord)\n", "    \n", "    popt, pcov = curve_fit(func, x_coord, y_coord)\n", "    plt.plot(bin_centers_x, func(bin_centers_x, *popt), 'r-',\n", "             label='fit: a=%5.3f, b=%5.3f' % tuple(popt))\n", "    \n", "    #Show the fit result\n", "    legend = plt.legend()\n", "    plt.setp(legend.get_texts(), color='w')\n", "    plt.show()\n", "    \n", "plot_tausdt_and_rho(qcd)"]}, {"cell_type": "markdown", "id": "b18695e4", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Checkpoint 2.4.3</span>\n", "\n", "Since you figured out your decorrelation, determine the best cut for the decorrelated taggers score (Refer to the plot from code cell `PROJ2.2-runcell03` and the results from `Checkpoint 2.2.3`) and use it in your selection function!\n", "\n", "Complete the function `get_t21cut_W`, which should return values of `t21ddt` below the threshold that you define."]}, {"cell_type": "code", "execution_count": null, "id": "2bf017cf", "metadata": {"tags": ["py", "learner_chopped", "draft"]}, "outputs": [], "source": ["#>>>PROBLEM: PROJ2.4.3\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "def get_t21cut_W(t21ddt,t21_thresh=#YOUR CODE HERE):\n", "    #return values of t21ddt that occur below the threshold\n", "    #based on previous analysis of t21 vs. rho\n", "    ### YOUR CODE HERE ### \n", "    return\n", "\n", "    \n", "def selectionW(iData):\n", "    '''\n", "    This is the specific selection for selecting out events with W signal for our analysis\n", "    '''\n", "    \n", "    #Pre-selection citeria\n", "    trigger = (iData.arrays('trigger', library=\"np\")[\"trigger\"].flatten() >= 0)\n", "    jetpt   = (iData.arrays('vjet0_pt', library=\"np\")[\"vjet0_pt\"].flatten() >= 400)\n", "    jetptnocut = (iData.arrays('vjet0_pt', library=\"np\")[\"vjet0_pt\"].flatten())\n", "    jetmass = (iData.arrays('vjet0_msd0', library=\"np\")[\"vjet0_msd0\"].flatten())\n", "    jett2   = (iData.arrays('vjet0_t2', library=\"np\")[\"vjet0_t2\"].flatten())\n", "    jett1   = (iData.arrays('vjet0_t1', library=\"np\")[\"vjet0_t1\"].flatten())\n", "    \n", "    \n", "    #Define the parameters rho, tau2/tau1\n", "    rho = rho_func(jetmass,jetptnocut)\n", "    t21 = t21_func(jett1,jett2)\n", "    \n", "    #Define the decorrelated tagger\n", "    Mcorr = #YOUR CODE HERE\n", "    t21ddt = t21ddt_func(t21,rho,Mcorr) \n", "                            \n", "    #And then perform the cut\n", "    #Hint: You could determine the threshold of the cut by plotting the distribution of \n", "    #t21ddt scores for W and background and then determine a ball park threshold\n", "    #where you think the W signal would be best selected\n", "    t21cut   = ### YOUR CODE HERE ### \n", "    \n", "    allcuts = np.logical_and.reduce([trigger, jetpt, t21cut])\n", "    \n", "    return allcuts"]}, {"cell_type": "markdown", "id": "10bfdc27", "metadata": {"tags": ["learner", "md", "catsoop_04"]}, "source": ["Now that we have our selection function, let's try to make the mass plot!"]}, {"cell_type": "code", "execution_count": null, "id": "9b8e37c1", "metadata": {"tags": ["learner", "py", "learner_chopped", "catsoop_04"]}, "outputs": [], "source": ["#>>>RUN: PROJ2.4-runcell05\n", "\n", "#Lets Look at the mass\n", "weights = [1000*18300, \"puweight\", \"scale1fb\"]\n", "mrange = (45,200)  #range for mass histogram [GeV]\n", "bins=40            #bins for mass histogram\n", "density = True     #to plot the histograms as a density (integral=1)\n", "\n", "qcdsel      = selectionW(qcd)\n", "wsel        = selectionW(wqq13)\n", "zsel        = selectionW(zqq13)\n", "datasel     = selectionW(data)\n", "ttsel       = selectionW(tt)\n", "wwsel       = selectionW(ww)\n", "wzsel       = selectionW(wz)\n", "zzsel       = selectionW(zz)\n", "gghsel      = selectionW(ggh)\n", "wscale=scale(wqq,wqq13,weights)\n", "zscale=scale(zqq,zqq13,weights)\n", "\n", "dataW = data.arrays('vjet0_msd0', library=\"np\")[\"vjet0_msd0\"][datasel]\n", "qcdW = qcd.arrays('vjet0_msd0', library=\"np\")[\"vjet0_msd0\"][qcdsel]\n", "wW = wqq13.arrays('vjet0_msd0', library=\"np\")[\"vjet0_msd0\"][wsel]\n", "zW = zqq13.arrays('vjet0_msd0', library=\"np\")[\"vjet0_msd0\"][zsel]\n", "zzW = zz.arrays('vjet0_msd0', library=\"np\")[\"vjet0_msd0\"][zzsel]\n", "wzW = wz.arrays('vjet0_msd0', library=\"np\")[\"vjet0_msd0\"][wzsel]\n", "wwW = ww.arrays('vjet0_msd0', library=\"np\")[\"vjet0_msd0\"][wwsel]\n", "ttW = tt.arrays('vjet0_msd0', library=\"np\")[\"vjet0_msd0\"][ttsel]\n", "gghW = ggh.arrays('vjet0_msd0', library=\"np\")[\"vjet0_msd0\"][gghsel]\n", "\n", "hist_weights = [get_weights(qcd,weights,qcdsel),\n", "                get_weights(wqq13,weights,wsel)*wscale,\n", "                get_weights(zqq13,weights,zsel)*zscale,\n", "                get_weights(zz,weights,zzsel),\n", "                get_weights(wz,weights,wzsel),\n", "                get_weights(ww,weights,wwsel),\n", "                get_weights(tt,weights,ttsel),\n", "               ]\n", "\n", "plt.hist([qcdW,wW, zW, zzW, wzW, wwW, ttW], \n", "         color=[\"royalblue\",\"r\", \"orange\",\"g\", \"b\", \"purple\", \"cyan\",], \n", "         label=[\"QCD\", \"W\", \"Z\", \"ZZ\", \"WZ\", \"WW\", \"tt\",], \n", "         weights=hist_weights,\n", "         range=mrange, bins=50, alpha=.6, density=density,stacked=True)\n", "\n", "counts, bins = np.histogram(dataW, bins=bins, range=mrange, density=density)\n", "yerr = np.sqrt(counts) / np.sqrt(len(dataW)*np.diff(bins))\n", "binCenters = (bins[1:]+bins[:-1])*.5\n", "plt.errorbar(binCenters, counts, yerr=yerr,fmt=\"o\",c=\"k\",label=\"data\")\n", "plt.legend()\n", "plt.xlabel(r\"Mass [GeV]\")\n", "plt.ylabel(\"Normalized Counts\")\n", "plt.show()"]}, {"cell_type": "markdown", "id": "21e06438", "metadata": {"tags": ["learner", "md", "catsoop_04"]}, "source": ["Remember to compare your mass plot with the one shown at the start of this section."]}, {"cell_type": "markdown", "id": "4163ab64", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_2_5'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">PROJ2.5 Fit for W Peak</h2>   \n", "\n", "| [Top](#section_2_0) | [Previous Section](#section_2_4) | [Checkpoints](#problems_2_5) | [Next Section](#section_2_6) |\n"]}, {"cell_type": "markdown", "id": "3765cbff", "metadata": {"tags": ["learner", "md", "catsoop_05", "learner_chopped"]}, "source": ["<h3>Objective 2: Fit the W mass peak with the appropriate function</h3>\n", "\n", "Now you will perform the fit on W signal. It involves a few steps:\n", "    \n", "<h4>1. Defining a model</h4>\n", "\n", "First you need to define a fit model of your own. In this case we would use some functions (gaussian, exponential) in conjuntion with a 6th order polynomial. You could see more on how the order of the polynomials are determined here: https://en.wikipedia.org/wiki/Chow_test. The concepts were also covered in previous Lessons, if you want to review them. Adding a chow test will likely allow you to improve the measurement by lowering the polynomial. Whilte its not needed here, we strongly encourage this investigation. \n", "\n", "In the extended projec you will have the chance to determine the order of the polynomial for the Z fit. You might see that we might not necessarily need a 6th order polynomial for the Z fit. The main reason for this is that we have much more data in W sample than the Z sample."]}, {"cell_type": "markdown", "id": "c05dcf99", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Checkpoint 2.5.1</span>\n", "\n", "Define a fit function `fitW()` that combines a Gaussian of the form $a\\exp{-(x-\\mu)^2/(2\\sigma^2)}$ with a 6th order polynomial. The parameters `a`, `mu`, and `sigma`, the parameters of the polynomial are left as fit parameters. "]}, {"cell_type": "code", "execution_count": null, "id": "6e3d38b3", "metadata": {"tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>PROBLEM: PROJ2.5.1\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "def fitW(x, p0, p1, p2, p3, p4, p5, a, mu, sigma):\n", "    #Our model is a gaussian on top of 6th order polynomial.\n", "    \n", "    #Define the polynomial\n", "    poly  = #YOUR CODE HERE\n", "    \n", "    #Define the gaussian\n", "    gauss = #YOUR CODE HERE\n", "    \n", "    #Stick them together\n", "    y =  poly + a*gauss\n", "    \n", "    return y"]}, {"cell_type": "markdown", "id": "636009a3", "metadata": {"tags": ["learner", "md", "catsoop_05"]}, "source": ["<h4>2. Performing a fit</h4>\n", "\n", "After defining your model, you need to get the data histogram and perform the fit:"]}, {"cell_type": "markdown", "id": "8f8cbdb7", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Checkpoint 2.5.2</span>\n", "\n", "Edit the fit model `p` to set the initial conditions for the fit parameters. Run the fit and see how it looks!\n", "\n", "What is the reduced chi-squared value? Is it good? Report your answer as a number with precision 1e-1."]}, {"cell_type": "code", "execution_count": null, "id": "9f72b700", "metadata": {"tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>PROBLEM: PROJ2.5.2\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "# Now we get the data histogram so we can fit it\n", "bins = 50\n", "mrange=[40,140]\n", "counts, bins = np.histogram(dataW,bins=bins,range=mrange,density=False)\n", "\n", "w = (1/ #poisson unc) #Poisson uncertainty here\n", "binCenters = (bins[1:]+bins[:-1])*.5\n", "x,y = binCenters.astype(\"float32\"), counts.astype(\"float32\")\n", "\n", "#Perform the fit \n", "model = lm.Model(fitW)\n", "     \n", "#Set initial conditions for your fit\n", "#You could experiment with zeros or your intuition first.\n", "#For better fit I suggest adding restrictions to the fit.\n", "p = model.make_params(#YOUR CODE HERE) \n", "\n", "\n", "result_W = model.fit(data=y,\n", "                   params=p,\n", "                   x=x,\n", "                   weights=w)\n", "\n", "#Plot the result\n", "plt.figure()\n", "result_W.plot()\n", "plt.xlabel(\"mass[GeV]\",position=(0.92,0.1))\n", "plt.ylabel(\"Entries/bin\",position=(0.1,0.84))\n", "\n", "#Print the fit summary\n", "print(result_W.fit_report())\n", "result_W.chisqr"]}, {"cell_type": "markdown", "id": "cfb52e44", "metadata": {"tags": ["learner", "md", "catsoop_05"]}, "source": ["<h4>3. Extracting the Mass</h4>\n", "\n", "Remember to compare your fit plot! Now we need to extract the W mass and the error in the measurement!"]}, {"cell_type": "markdown", "id": "2e5a183d", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Checkpoint 2.5.3</span>\n", "\n", "Finally, get the mass and standard error from your fit results (corresponding to the specific choice of fit function we defined in `Checkpoint 2.5.2`). This should be the parameter `mu`, corresponding to the Gaussian fit. Report the mass with precision 1e-1."]}, {"cell_type": "code", "execution_count": null, "id": "680b6bf9", "metadata": {"tags": ["py", "learner_chopped", "learner", "draft"]}, "outputs": [], "source": ["#>>>PROBLEM: PROJ2.5.3\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "mW = #YOUR CODE HERE\n", "mWerr = #YOUR CODE HERE\n", "\n", "print(mW, \"+/-\", mWerr)"]}], "metadata": {"celltoolbar": "Tags", "kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}